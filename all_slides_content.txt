小红书的推荐系统

王树森

ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

小红书 (logo in bottom right)小红书的推荐系统

推荐内容 (with arrow pointing to "发现" tab)

Navigation tabs: 关注 发现 附近 (with search icon)
Secondary tabs: 推荐 视频 直播 旅行 美食

Left sidebar showing:
随意必备物品
- 不是你的... (191 likes)

Content items shown:
1. 登机、隔离必备物品
   (Travel/quarantine essentials image showing various packaged items)
   - 松江区-神仙酒店get 不折盲盒
   - 妮姬姬Elsie (20 likes)

2. 钓庐黄金枪鱼（后）
   (Golden tuna fishing image showing nighttime scene)
   - 谢宁 (218 likes)

3. 国足又被打脸！韦世豪黑队3天后在野...
   (Football/soccer related image)
   - 国足等录 (53 likes)

Bottom navigation: 首页 商城 + 消息 我小红书的推荐系统

点击 (Click action - arrow pointing from left panel to right detail view)

Left panel (same as previous):
推荐内容 with "发现" tab highlighted
- 松江区-神仙酒店get不折盲盒 post highlighted with red box
- 妮姬姬Elsie (20 likes)

Right panel (Detail view):
妮姬姬Elsie 
@学尔森馆
关注 (Follow button)

Image showing Doritos chips and snacks on shelves

松江区-神仙酒店get不折盲盒

从打算回去的时候就疯狂看攻略
感谢前人的笔记
这次逛了好多坑
顺利入住神仙酒店-学苑宾馆

Additional user interactions:
- 外卖 快递 还有小超市
- 冰箱 沙发 大窗户可开 对着邻野公园
- 视野很好
- 我知足了
- 小小朋友带小朋友

Engagement metrics:
说点什么... 20 (likes) 16 (shares) 38 (comments)小红书的推荐系统

This slide shows the Xiaohongshu (Little Red Book) recommendation system interface with several key components highlighted:

Main Interface Elements:
- Left sidebar shows recommendation categories including:
  - 推荐 (Recommend)
  - 视频 (Video)
  - 直播 (Live)
  - 购物 (Shopping)
  - Various other navigation options

Content Feed Structure:
- Shows a post about "松江区-神仙酒店get不折置盒" (Songjiang District - Fairy Hotel Get Unfolded Box)
- Author: 呃呃呃Elsie
- Post content includes images of snacks/food items
- Post text describes: "从打算回去的时候就顺狂看攻略 感谢朋友的笔记 这次逛了好多玩 顺利入住神仙酒店-享充宾馆"

Key Recommendation System Features Highlighted:
1. 点击 (Click) - shown with red arrow pointing to a highlighted post
2. 阅读 (Read) - indicating content consumption
3. 评论 (Comment) - showing user engagement with "共38条评论" (38 comments total)
4. 点赞 (Like) - showing likes count of 20, 16
5. 收藏 (Save/Favorite) - showing save functionality

Engagement Metrics Displayed:
- Likes: 20, 16 shown on different posts
- Comments: 38 comments on featured post
- Views and other engagement indicators
- User interaction options at bottom of posts

The interface demonstrates how Xiaohongshu's recommendation algorithm considers multiple signals including clicks, reads, comments, likes, and saves to personalize content recommendations.转化流程 (Conversion Flow)

This slide shows the user conversion funnel in recommendation systems, illustrating how users progress through different engagement levels:

User Engagement Flow:
曝光 (Impression) → 点击 (Click) → Multiple Actions

From Click, users can take various actions:
1. 滑动到底 (ScrollToEnd) → 评论 (Comment)
2. 点赞 (Like)  
3. 收藏 (Collect)
4. 转发 (Share)

Key Components:
- 曝光 (Impression): Initial content exposure to users
- 点击 (Click): User clicks on the content, showing initial interest
- 滑动到底 (ScrollToEnd): User scrolls through entire content, indicating engagement
- 评论 (Comment): User leaves comments, showing high engagement
- 点赞 (Like): User likes the content
- 收藏 (Collect): User saves/bookmarks the content
- 转发 (Share): User shares the content with others

Flow Logic:
The diagram shows that after clicking on content, users may engage in multiple ways. ScrollToEnd leads to Comment, suggesting that users who read through entire content are more likely to comment. All other actions (Like, Collect, Share) branch directly from Click.

This conversion funnel helps recommendation systems understand:
- User engagement depth at each stage
- Which actions indicate higher interest/satisfaction
- How to optimize for different conversion goals
- User behavior patterns for improving recommendations转化流程 (Conversion Flow) - Highlighted Version

This slide shows the same conversion funnel as the previous slide, but with all elements highlighted in red boxes to emphasize the complete user journey:

Highlighted User Engagement Flow:

Starting Point (Red Box):
- 曝光 (Impression) → 点击 (Click)

Post-Click Actions (All in Red Boxes):
1. 滑动到底 (ScrollToEnd) → 评论 (Comment)
2. 点赞 (Like)
3. 收藏 (Collect)  
4. 转发 (Share)

Key Emphasis Points:
The red highlighting indicates that all these conversion steps are critical metrics for recommendation systems:

- Initial funnel: Impression → Click represents the basic conversion from content visibility to user interest
- Deep engagement: ScrollToEnd → Comment shows users who fully consume content and provide feedback
- Quick actions: Like, Collect, Share represent immediate positive signals that can be captured without extensive content consumption

System Implications:
This highlighted view suggests that recommendation systems should:
- Track and optimize for each highlighted conversion step
- Weight different actions appropriately (comments may indicate deeper engagement than likes)
- Use all these signals as training data for improving future recommendations
- Monitor conversion rates at each step to identify optimization opportunities

The comprehensive highlighting emphasizes that every step in this flow provides valuable user preference data for the recommendation algorithm.消费指标 (Consumption Metrics)

This slide defines key performance metrics used in recommendation systems to measure user engagement and content consumption:

Key Metrics Formulas:

• 点击率 = 点击次数 / 曝光次数
  (Click-Through Rate = Number of Clicks / Number of Impressions)

• 点赞率 = 点赞次数 / 点击次数  
  (Like Rate = Number of Likes / Number of Clicks)

• 收藏率 = 收藏次数 / 点击次数
  (Collection Rate = Number of Collections / Number of Clicks)

• 转发率 = 转发次数 / 点击次数
  (Share Rate = Number of Shares / Number of Clicks)

• 阅读完成率 = 滑动到底次数 / 点击次数 × f(笔记长度)
  (Reading Completion Rate = Number of ScrollToEnd / Number of Clicks × f(Note Length))

Important Notes:
- All engagement metrics (like, collection, share rates) are calculated relative to clicks, not impressions
- The reading completion rate includes a function f(笔记长度) that accounts for content length, suggesting longer content may have different completion rate calculations
- The f(笔记长度) factor (highlighted in red) indicates that content length is an important variable in measuring true reading completion

These metrics help recommendation systems:
1. Measure content quality and user engagement depth
2. Compare performance across different content types and lengths  
3. Optimize ranking algorithms based on these conversion rates
4. Identify high-performing content patterns for better recommendations北极星指标 (North Star Metrics)

This slide outlines the key North Star metrics that recommendation systems focus on to measure overall platform success:

• 用户规模 (User Scale):
  • 日活用户数 (DAU) - Daily Active Users
  • 月活用户数 (MAU) - Monthly Active Users

• 消费 (Consumption):
  • 人均使用推荐的时长 - Average time spent using recommendations per user
  • 人均阅读笔记的数量 - Average number of notes read per user

• 发布 (Publishing):
  • 发布渗透率 - Publishing penetration rate
  • 人均发布量 - Average publishing volume per user

Key Insights:
1. User Scale Metrics: Focus on both daily and monthly active users to measure platform growth and stickiness

2. Consumption Metrics: Track how much time users spend with recommended content and how many pieces of content they consume, indicating recommendation system effectiveness

3. Publishing Metrics: Monitor what percentage of users create content (penetration rate) and how much content each user creates on average, which feeds back into the recommendation ecosystem

These North Star metrics help guide the overall strategy of recommendation systems by:
- Measuring platform health and growth (user scale)
- Evaluating recommendation quality and engagement (consumption)
- Ensuring content supply for the recommendation engine (publishing)

The balance between these three categories is crucial for a sustainable recommendation platform ecosystem.实验流程 (Experimentation Process)

This slide illustrates the three-phase process for implementing and testing recommendation system improvements:

Three-Phase Process:

1. 离线实验 (Offline Experiments)
   收集历史数据，在历史数据上做训练、测试。算法没有部署到产品中，没有跟用户交互。
   (Collect historical data, conduct training and testing on historical data. The algorithm is not deployed to the product and has no user interaction.)

2. 小流量AB测试 (Small-Scale A/B Testing)  
   把算法部署到实际产品中，用户实际跟算法做交互。
   (Deploy the algorithm to the actual product, with users actually interacting with the algorithm.)

3. 全流量上线 (Full-Scale Launch)
   (Complete rollout to all traffic)

Process Flow Logic:
- Stage 1 (Offline): Safe testing environment using historical data, no real user impact
- Stage 2 (Small A/B): Limited real-world testing with actual users to validate performance
- Stage 3 (Full Launch): Complete deployment after successful validation

Key Characteristics:
- **Offline Experiments**: Risk-free testing, uses past data, no user interaction
- **Small A/B Testing**: Real user feedback, controlled exposure, actual algorithmic interaction  
- **Full Launch**: Complete implementation across entire user base

This staged approach allows recommendation systems to:
- Validate improvements without user risk (offline)
- Test real-world performance on limited scale (A/B)
- Deploy confidently to full user base (launch)

The progression ensures both safety and effectiveness in recommendation system deployment.Thank You!

This is the concluding slide of the 01_Basics_01 presentation.

Content:
- Main message: "Thank You!" displayed prominently in the center
- Contact/Reference information: http://wangshusen.github.io/

This appears to be the end of the first part of the recommendation systems basics presentation, with a link to the presenter's website for additional resources.推荐系统链路 (Recommendation System Pipeline)

Title slide for the second part of the recommendation systems basics presentation.

Presenter Information:
- Name: 王树森 (Wang Shusen)
- Email: ShusenWang@xiaohongshu.com
- Website: http://wangshusen.github.io/

Visual Elements:
- Small red Xiaohongshu (Little Red Book) logo in bottom right corner
- Clean, minimalist design with title prominently displayed

This appears to be the introduction slide for a presentation focusing on the technical pipeline and architecture of recommendation systems.推荐系统的链路 (Recommendation System Pipeline)

This slide introduces the multi-stage recommendation system pipeline, showing the flow from initial content pool to final user recommendations:

Pipeline Flow:
几亿物品 → 召回 → 几千物品 → 粗排 → 几百物品 → 精排 → 几百物品 → 重排 → 几十物品

Stage-by-Stage Breakdown:
1. 几亿物品 (Hundreds of millions of items) - Initial content pool
2. 召回 (Recall/Retrieval) - First filtering stage
3. 几千物品 (Thousands of items) - After recall filtering  
4. 粗排 (Coarse Ranking) - Initial ranking stage
5. 几百物品 (Hundreds of items) - After coarse ranking
6. 精排 (Fine Ranking) - Detailed ranking stage  
7. 几百物品 (Hundreds of items) - After fine ranking
8. 重排 (Re-ranking) - Final optimization stage
9. 几十物品 (Tens of items) - Final recommendations shown to user

Key Insights:
- Massive scale reduction: From hundreds of millions to tens of items
- Multi-stage filtering and ranking approach for computational efficiency
- Each stage progressively refines the selection with more sophisticated but computationally expensive algorithms
- The funnel structure balances recommendation quality with system performance推荐系统的链路 (Recommendation System Pipeline) - Highlighted Recall

This slide shows the same recommendation pipeline with the 召回 (Recall/Retrieval) stage highlighted, emphasizing its role in the system:

Pipeline Flow with Recall Emphasis:
几亿物品 → **召回** → 几千物品 → 粗排 → 几百物品 → 精排 → 几百物品 → 重排 → 几十物品

Highlighted Stage:
- **召回 (Recall/Retrieval)**: The first critical filtering stage that reduces hundreds of millions of items to thousands

Key Points about Recall Stage:
- First major bottleneck in the recommendation pipeline
- Must be highly efficient to handle massive scale (hundreds of millions of items)
- Responsible for the largest reduction in candidate items
- Sets the foundation for all subsequent ranking stages
- Typically uses simpler, faster algorithms (collaborative filtering, content-based matching, etc.)
- Cannot recover items that are filtered out at this stage, making recall completeness crucial

The highlighting suggests this stage is being focused on in the current presentation, likely due to its fundamental importance in the recommendation system architecture.推荐系统的链路 (Recommendation System Pipeline) - Highlighted Coarse Ranking

This slide shows the same recommendation pipeline with the 粗排 (Coarse Ranking) stage highlighted, emphasizing its role in the system:

Pipeline Flow with Coarse Ranking Emphasis:
几亿物品 → 召回 → 几千物品 → **粗排** → 几百物品 → 精排 → 几百物品 → 重排 → 几十物品

Highlighted Stage:
- **粗排 (Coarse Ranking)**: The second major filtering and ranking stage that reduces thousands of items to hundreds

Key Points about Coarse Ranking Stage:
- Operates on the output of the recall stage (thousands of items)
- Provides the first level of ranking/scoring for candidate items
- Still needs to be computationally efficient to handle thousands of items
- Uses simpler ranking models compared to fine ranking
- Further narrows down the candidate set for more expensive fine ranking algorithms
- Balances between computational cost and ranking quality

The highlighting indicates this stage is the current focus, showing its position as an intermediate step between initial recall and detailed fine ranking in the recommendation pipeline.推荐系统的链路 (Recommendation System Pipeline) - Highlighted Fine Ranking

This slide shows the same recommendation pipeline with the 精排 (Fine Ranking) stage highlighted, emphasizing its role in the system:

Pipeline Flow with Fine Ranking Emphasis:
几亿物品 → 召回 → 几千物品 → 粗排 → 几百物品 → **精排** → 几百物品 → 重排 → 几十物品

Highlighted Stage:
- **精排 (Fine Ranking)**: The detailed ranking stage that operates on hundreds of items from coarse ranking

Key Points about Fine Ranking Stage:
- Operates on a smaller set (hundreds of items) allowing for more sophisticated algorithms
- Uses complex machine learning models with more features and deeper analysis
- Provides more accurate and personalized ranking compared to coarse ranking
- Can afford higher computational cost per item due to smaller candidate set
- Critical for final recommendation quality as it determines the precise order of items
- Often incorporates deep learning models, multi-objective optimization, and real-time personalization

The highlighting indicates this is a key focus stage, representing the most sophisticated ranking component in the pipeline.推荐系统的链路 (Recommendation System Pipeline) - Highlighted Re-ranking

This slide shows the same recommendation pipeline with the 重排 (Re-ranking) stage highlighted, emphasizing its role in the system:

Pipeline Flow with Re-ranking Emphasis:
几亿物品 → 召回 → 几千物品 → 粗排 → 几百物品 → 精排 → 几百物品 → **重排** → 几十物品

Highlighted Stage:
- **重排 (Re-ranking)**: The final optimization stage that produces the ultimate recommendations shown to users

Key Points about Re-ranking Stage:
- Final stage in the recommendation pipeline
- Operates on the smallest set (transitioning from hundreds to tens of items)
- Applies business logic, diversity constraints, and final optimizations
- Considers factors beyond individual item relevance (e.g., overall feed diversity, freshness, business objectives)
- May apply deduplication, content policy filters, and user experience optimizations
- Responsible for the final user-facing recommendation quality
- Often incorporates real-time contextual factors and A/B testing logic

The highlighting emphasizes this as the final critical step that determines what users actually see in their feeds.召回通道: 协同过滤、双塔模型、关注的作者、等等。
(Recall Channels: Collaborative Filtering, Two-Tower Model, Followed Authors, etc.)

This slide introduces the concept of recall channels and shows a visual representation of the massive item pool:

Header Information:
召回通道 (Recall Channels) includes:
- 协同过滤 (Collaborative Filtering)
- 双塔模型 (Two-Tower Model)  
- 关注的作者 (Followed Authors)
- 等等 (And others)

Visual Element:
- Large green cylindrical database/storage representation labeled “几亿” (hundreds of millions)
- This represents the massive scale of the initial content pool that recall systems must process

Key Insights:
- Multiple recall channels work in parallel to retrieve relevant content
- Each channel uses different algorithms and signals:
  - Collaborative Filtering: Based on user behavior patterns
  - Two-Tower Model: Deep learning approach for user-item matching
  - Followed Authors: Content from users the person follows
- The scale challenge: Processing hundreds of millions of items efficiently
- Different channels capture different aspects of user preferences

This slide sets up the technical foundation for understanding how recall systems handle massive scale through diverse algorithmic approaches.召回通道组合 (Recall Channel Combination)

This slide shows how multiple recall channels work in parallel to retrieve candidates from the massive item pool:

Header:
召回通道: 协同过滤、双塔模型、关注的作者、等等。
(Recall Channels: Collaborative Filtering, Two-Tower Model, Followed Authors, etc.)

Architecture Diagram:
- **几亿** (Hundreds of millions) - Initial massive item database
- **召回通道#1** → **几百** (Hundreds) - First recall channel output
- **召回通道#2** → **几百** (Hundreds) - Second recall channel output
- **•••** (Dots indicating more channels)
- **召回通道#10** → **几百** (Hundreds) - Tenth recall channel output

Key Concepts:
1. **Parallel Processing**: Multiple recall channels operate simultaneously
2. **Channel Diversity**: Each channel (1, 2, ... 10) uses different algorithms
3. **Scale Reduction**: Each channel reduces millions/billions to hundreds of candidates
4. **Channel Specialization**: Different channels capture different user preferences
   - Channel #1-10 might represent different algorithms or user interest areas

System Benefits:
- **Coverage**: Multiple channels ensure comprehensive candidate retrieval
- **Efficiency**: Each channel handles a portion of the recommendation space
- **Robustness**: Redundancy across channels improves system reliability召回通道合并与排序 (Recall Channel Merging and Ranking)

This slide shows the progression from multiple recall channels to the ranking stages:

Header:
排序: [几千] → 粗排 → [几百] → 精排 → [几百]
(Ranking: [Thousands] → Coarse Ranking → [Hundreds] → Fine Ranking → [Hundreds])

Architecture Flow:
- **几亿** (Hundreds of millions) - Initial item database
- **召回通道#1** → **几百** (Hundreds) - First recall channel
- **召回通道#2** → **几百** (Hundreds) - Second recall channel  
- **召回通道#10** → **几百** (Hundreds) - Tenth recall channel
- **粗排、精排** → **几百** (Hundreds) - Combined ranking stages

Key Process:
1. **Channel Merging**: Results from all recall channels (typically 10+ channels) are combined
2. **Deduplication**: Remove duplicate items retrieved by multiple channels
3. **Initial Aggregation**: Combine hundreds of items from each channel into thousands total
4. **Ranking Pipeline Entry**: The merged thousands of items enter the ranking system
5. **Two-Stage Ranking**: 
   - Coarse ranking reduces thousands to hundreds
   - Fine ranking optimizes the hundreds for final quality

Technical Considerations:
- **Union Operation**: Combine results from all parallel recall channels
- **Diversity Preservation**: Ensure each channel contributes to final candidates
- **Ranking Preparation**: Aggregate features and metadata for ranking algorithms
- **Scale Management**: Manage the transition from parallel processing to sequential ranking完整推荐系统流程 (Complete Recommendation System Flow)

This slide shows the end-to-end recommendation system pipeline with final user-facing results:

Header:
排序: [几千] → 粗排 → [几百] → 精排 → [几百] → 重排 → [几十]
(Ranking: [Thousands] → Coarse Ranking → [Hundreds] → Fine Ranking → [Hundreds] → Re-ranking → [Tens])

Complete Architecture:
**Input Stage:**
- **几亿** (Hundreds of millions) - Initial item database

**Recall Stage:**
- **召回通道#1** → **几百** (Hundreds)
- **召回通道#2** → **几百** (Hundreds)
- **召回通道#10** → **几百** (Hundreds)

**Ranking Stages:**
- **粗排、精排** → **几百** (Hundreds) - Combined coarse and fine ranking

**Re-ranking Stage:**
- **重排** → Final ranked list

**Final Output:**
- **物品1** (Item 1)
- **物品2** (Item 2)
- **•••** (More items)
- **物品80** (Item 80)

Key Features:
1. **End-to-End Pipeline**: Shows complete flow from billions to final ranked items
2. **Multi-Stage Filtering**: Progressive reduction through recall, ranking, and re-ranking
3. **Final User Experience**: Delivers personalized, ranked recommendations (Item 1, Item 2, etc.)
4. **Scale Achievement**: Successfully reduces massive corpus to manageable, high-quality results
5. **Business Ready**: Final output ready for user interface presentation

This represents the complete recommendation system architecture from data ingestion to user-facing recommendations.# 粗排、精排

## 系统架构图

输入特征：
- 用户特征 (黄色部分)
- 物品特征 (蓝色部分) 
- 统计特征 (绿色部分)

这些特征通过神经网络进行处理。

## 特征说明

- 用户特征：用户相关的属性和行为特征
- 物品特征：商品或内容的属性特征
- 统计特征：统计类特征和环境特征

所有特征输入到神经网络中进行学习和预测。# 粗排、精排

## 神经网络输出预测指标

神经网络处理输入特征后，输出多个预测指标：

### 输出指标
- 点击率 (紫色圆圈)
- 点赞率 (紫色圆圈)
- 收藏率 (紫色圆圈)
- 转发率 (紫色圆圈)

### 最终输出
- 排序分数 (红色圆圈)

### 预估值
所有输出指标综合计算得到预估值，用于最终的排序决策。

## 系统架构
从用户特征、物品特征、统计特征输入，通过神经网络处理，输出多个预测指标，最终得到排序分数。# 重排

## 重排策略

### 多样性抽样策略
- 做多样性抽样（比如MMR、DPP），从几百篇中选出几十篇。

### 规则优化
- 用规则打散相似笔记。

### 内容调整
- 插入广告、运营推广内容，根据生态要求调整排序。

## 重排目标
重排阶段主要目标是在保证推荐质量的基础上，通过多样性控制、规则优化和商业化内容插入来优化用户体验和平台收益。# 总结

## 漏斗架构总结

### 召回阶段
- 用多条通道，取回几千篇笔记。

### 粗排阶段 (蓝色框部分)
- 用小规模神经网络，给几千篇笔记打分，选出分数最高的几百篇。

### 精排阶段
- 用大规模神经网络，给几百篇笔记打分。

### 重排阶段
- 做多样性抽样、规则打散、插入广告和运营笔记。

## 架构特点
这是典型的推荐系统漏斗架构，从大量候选集逐步筛选到最终展示给用户的少量高质量内容，每个阶段都有其特定的优化目标和技术方案。# Thank You!

http://wangshusen.github.io/

## 课程结束
感谢观看本课程内容。

## 参考资源
更多相关内容可以访问: http://wangshusen.github.io/# A/B测试

## 讲师信息
**王树森**

**邮箱:** ShusenWang@xiaohongshu.com

**网站:** http://wangshusen.github.io/

## 课程标识
小红书 (右下角logo)# A/B测试

## A/B测试的应用场景

### 离线验证
- 召回团队实现了一种GNN召回通道，离线实验结果正向。

### 线上小流量测试
- 下一步是做线上的小流量A/B测试，考察新的召回通道对线上指标的影响。

### 参数优化
- 模型中有一些参数，比如GNN的深度取值∈{1,2,3}，需要用A/B测试选取最优参数。

## A/B测试目标
通过线上实验验证新方法的有效性，并优化模型参数设置。# 随机分桶

## 分桶设置
- 分b = 10个桶，每个桶中有10%的用户。

## 分桶方法
- 首先用哈希函数把用户ID映射成某个区间内的整数，然后把这些整数均匀随机分成b个桶。

## 用户分布图示
[图示：一排用户图标，显示所有用户的分布情况]

**全部n位用户，分成b个桶，每个桶中有n/b位用户**

## 分桶原理
通过哈希函数确保用户的随机均匀分布，为后续的A/B测试提供可靠的实验基础。# 随机分桶

## 实验组与对照组设置

### 桶的分配
- **1号桶**: 实验组#1 (红色标注)
- **2号桶**: 实验组#2 (红色标注)  
- **3号桶**: 实验组#3 (红色标注)
- **4号桶**: 对照组 (蓝色标注)
- **...**: 其他桶
- **10号桶**: 最后一个桶

## 指标计算
- 计算每个桶的业务指标，比如DAU、人均使用推荐的时长、点击率、等等。

## 实验结论
- 如果某个实验组指标显著优于对照组，则说明对应的策略有效，值得推全。

## 实验设计原理
通过对比实验组和对照组的业务指标差异，来验证新策略或新算法的有效性。# 分层实验

## 分层实验的概念
分层实验是A/B测试中一种更高级的实验设计方法，用于同时测试多个不相冲突的功能或策略。

## 标题说明
本页介绍分层实验的相关内容和实施方法。# 流量不够用怎么办？

## 流量分配挑战

### 业务场景
- 信息流产品的公司有很多部门和团队，大家都需要做A/B测试。

### 需要测试的系统模块
- **推荐系统**（召回、粗排、精排、重排）
- **用户界面**
- **广告**

### 流量分配问题
- 如果把用户随机分成10组，1组做对照，9组做实验，那么只能同时做9组实验。

## 问题核心
当多个团队和系统都需要进行A/B测试时，如何合理分配有限的用户流量，确保实验的有效性和并行性。# 分层实验

## 分层实验设计

### 分层概念
- **分层实验**：召回、粗排、精排、重排、用户界面、广告......
  （例如GNN召回通道属于召回层。）

### 同层互斥原则
- **同层互斥**：GNN实验占了召回层的4个桶，其他召回实验只能用剩余的6个桶。

### 不同层正交原则
- **不同层正交**：每一层独立随机对用户做分桶。每一层都可以独立用100%的用户做实验。

## 参考文献
- Tang et al. *Overlapping experiment infrastructure: more, better, faster experimentation*. In KDD, 2010.

## 分层实验优势
通过分层设计，可以让不同功能模块的实验并行进行，大大提高了流量利用效率。# 分层实验

## 分层分桶数学表示

### 召回层分桶
- 召回层把用户分成10个桶：𝒰₁, 𝒰₂, ⋯, 𝒰₁₀

### 精排层分桶  
- 精排层把用户分成10个桶：𝒱₁, 𝒱₂, ⋯, 𝒱₁₀

### 用户数量关系
- 设系统共有n个用户，那么|𝒰ᵢ| = |𝒱ⱼ| = n/10

### 分桶独立性
- 召回桶𝒰ᵢ和召回桶𝒰ⱼ交集为：𝒰ᵢ ∩ 𝒰ⱼ = ∅

### 跨层交集大小
- 召回桶𝒰ᵢ和精排桶𝒱ⱼ交集的大小为：|𝒰ᵢ ∩ 𝒱ⱼ| = n/100

## 数学原理
通过独立的哈希函数为不同层分配用户，确保层内互斥而层间正交的分桶效果。# 同层互斥

## 同层互斥架构图

### 系统架构
```
用户界面
    ↓
召回层：[1号桶] [2号桶] [3号桶] ... [10号桶]
         (红色圈圈标记同层互斥)
    ↓  
粗排层
```

### 同层互斥原理
- 召回层的用户被分成10个桶
- 同一层内的不同实验使用不同的桶
- 同一层内的桶之间相互排斥，不重叠
- 用红色椭圆圈标记表示同层内的实验互斥关系

### 流量分配
- 每个桶包含相等数量的用户
- 同层内的实验不能使用相同的用户桶
- 确保同层实验之间不相互干扰

## 设计目标
通过同层互斥设计，确保同一功能层的不同实验策略之间不会相互影响，保证实验结果的可靠性。# 不同层正交

## 不同层正交架构图

### 流量分配机制
```
用户界面层：10%用户 (2号桶突出显示)
         ↓ (均匀打散)
召回层：[1号桶] [2号桶] [3号桶] ... [10号桶]
         ↓
粗排层
```

### 正交分配原理
- **用户界面层**的2号桶用户（10%用户）
- **均匀打散**到召回层的所有10个桶中
- 每个召回层桶都会收到来自用户界面层2号桶的用户
- 红色箭头显示分散分配的路径

### 数学关系
- 用户界面层每个桶：n/10个用户
- 这些用户均匀分散到召回层的10个桶
- 每个交集：n/100个用户

## 正交优势
不同层之间的正交设计使得各层的实验可以独立进行，互不影响，最大化流量利用效率。# 互斥 vs 正交

## 互斥与正交的区别

### 正交的优势
- 如果所有实验都正交，则可以同时做无数组实验。

### 互斥的必要性
- **同类的策略**（例如精排模型的两种结构）天然互斥，对于一个用户，只能用其中一种。

### 策略间的相互作用
- **同类的策略**（例如添加两条召回通道）效果会相互增强（1+1>2）或相互抵消（1+1<2）。互斥可以避免同类策略相互干扰。

### 正交适用场景
- **不同类型的策略**（例如添加召回通道、优化粗排模型）通常不会相互干扰（1+1=2），可以作为正交的两层。

## 设计原则
根据策略类型和相互影响关系，合理选择互斥或正交的实验设计方法，确保实验结果的准确性和可解释性。# Holdout机制

## Holdout机制概述
Holdout机制是A/B测试中的一种重要实验设计方法，用于评估整体业务指标的改进效果。

## 标题说明
本页将详细介绍Holdout机制的原理、实施方法和应用场景。# Holdout机制

## Holdout机制原理

### 独立汇报机制
- 每个实验（召回、粗排、精排、重排）独立汇报对业务指标的提升。

### 整体效果评估
- 公司考察一个部门（比如推荐系统）在一段时间内对业务指标总体的提升。

### Holdout分桶策略
- 取10%的用户作为holdout桶，推荐系统使用剩余90%的用户做实验，两者互斥。

### 收益计算方法
- **10% holdout桶 vs 90%实验桶的diff**（需要归一化）为整个部门的业务指标收益。

## Holdout价值
通过Holdout机制可以客观评估所有实验的累计效果，避免单个实验收益简单相加可能产生的偏差。# Holdout机制架构图

## 系统架构图

### 整体架构
```
用户界面层
    ↓
推荐系统:
├── holdout桶 (10%用户) - 蓝色区域
└── 实验区域 (90%用户) - 右侧灰色区域
    ├── 召回层
    ├── 粗排层  
    ├── 精排层
    └── 重排层
```

### 用户分配
- **左侧蓝色区域**: holdout桶，包含10%用户
- **右侧灰色区域**: 实验桶，包含90%用户，进行各层实验

### 对照原理
- holdout桶使用基线系统
- 实验桶使用包含各种改进的实验系统
- 通过对比两组用户的业务指标差异来评估整体收益

## 设计目标
通过holdout机制实现对推荐系统整体改进效果的客观评估。# Holdout差值计算

## 差值计算示意图

### 对比架构
```
用户界面层
    ↓
推荐系统:
├── holdout桶 (10%用户) - 蓝色区域
└── 实验区域 (90%用户) - 右侧灰色区域
    ├── 召回层
    ├── 粗排层  
    ├── 精排层
    └── 重排层
```

### diff计算
- **箭头标记**: 表示对比holdout桶与实验桶的业务指标差异
- **计算公式**: diff = 实验桶指标 - holdout桶指标

### 收益归因
- diff值反映了推荐系统所有改进措施的累计收益
- 需要进行归一化处理以得到准确的业务指标提升

## 评估意义
通过holdout差值计算，可以量化推荐系统在一段时间内对整体业务指标的真实贡献。# Holdout机制

## Holdout周期管理

### 考核周期结束后的操作
- 每个考核周期结束之后，清除holdout桶，让推全实验从90%用户扩大到100%用户。

### 重新分桶
- 重新随机划分用户，得到holdout桶和实验桶，开始下一轮考核周期。

### 初始状态重置
- 新的holdout桶与实验桶各种业务指标的diff接近0。

### 累积效果观察
- 随着召回、粗排、精排、重排实验上线和推全，diff会逐渐扩大。

## Holdout循环机制
通过定期重置和重新分桶，确保holdout机制能够持续有效地评估推荐系统的整体改进效果。# 实验推全 & 反转实验

## 实验推全与反转实验概述
本章节将介绍A/B测试中的实验推全机制以及反转实验的设计和应用。

## 主要内容
- 实验推全的决策流程
- 反转实验的必要性和实施方法
- 长期效果监控策略

## 学习目标
理解如何将成功的实验策略推广到全量用户，以及如何通过反转实验验证长期效果。# 实验推全架构图

## 推全前架构
```
推荐系统架构:
├── holdout桶 (10%用户) - 蓝色区域
└── 实验区域 (90%用户) - 右侧区域
    ├── 召回层
    ├── 粗排层  
    ├── 精排层
    └── 重排层 (其中20%用户标注为紫色)
```

### 重排层实验
- 重排层中有20%用户(紫色区域)正在进行某项实验
- 该实验显示出了正向的效果收益

### 实验状态
- holdout桶保持基线配置
- 90%实验用户中，重排层的20%在测试新策略
- 其他层级维持现有配置

## 推全准备
当重排层实验证明有效时，准备将该策略推广到更大范围的用户。# 实验推全后架构图

## 推全后架构
```
推荐系统架构:
├── holdout桶 (10%用户) - 蓝色区域  
│   └── holdout (灰色标注)
└── 实验区域 (90%用户) - 右侧区域
    ├── 新层 (推全新策略) - 粉色区域
    ├── 召回层
    ├── 粗排层  
    ├── 精排层
    └── 重排层
```

### 推全实施
- 成功的实验策略被推广到新的层级
- 新层(粉色区域)应用了经过验证的策略
- 所有90%的实验用户都能受益于新策略

### 系统状态
- holdout桶仍然保持基线配置
- 新策略在新层中全面部署
- 原有层级结构保持不变

## 推全效果
通过将验证有效的策略推广到更大用户群体，实现整体业务指标的提升。# 反转实验

## 反转实验的必要性

### 指标响应特征
- **有的指标**（点击、交互）立即收到新策略影响
- **有的指标**（留存）有滞后性，需要长期观测

### 快速推全的风险
- 实验观测到显著收益后尽快推全新策略。目的是腾出桶供其他实验使用，或需要基于新策略做后续的开发。

### 反转实验的作用
- 用反转实验解决上述矛盾，既可以尽快推全，也可以长期观测实验指标。

### 实施方法
- 在推全的新层中开一个旧策略的桶，长期观测实验指标。

## 反转实验优势
通过反转实验设计，可以在快速推全新策略的同时，持续监控长期效果，确保决策的科学性。# 反转实验架构图

## 反转实验实施架构

### 反转桶设置
```
推荐系统架构:
├── holdout桶 (10%用户) - 蓝色区域
└── 实验区域 (90%用户) - 右侧区域
    ├── 新层: 推全新策略 + 反转桶(旧策略) - 粉色区域
    │   ├── 推全新策略 (大部分区域)  
    │   └── 反转桶 (小部分区域，标注为"旧策略")
    ├── 召回层
    ├── 粗排层  
    ├── 精排层
    └── 重排层
```

### 反转桶原理
- **反转桶标识**: 新层中的小部分用户使用旧策略
- **对比基准**: 为长期监控提供对照组
- **策略分配**: 大部分用户享受新策略，小部分用户作为对照

### 监控机制
- 通过反转桶与推全区域的差异监控长期效果
- 确保新策略的持续有效性

## 设计优势
反转实验既实现了策略的快速推全，又保持了长期监控能力。# 总结

## A/B测试核心概念总结

### 分层实验设计
- **同层互斥**（不允许两个实验同时影响一位用户）
- **不同层正交**（实验有重叠的用户）

### Holdout机制
- **保留10%的用户**，完全不受实验影响
- **可以考察整个部门对业务指标的贡献**

### 实验推全策略
- **新建一个推全层，与其他层正交**

### 反转实验方法
- **在新的推全层上，保留一个小的反转桶**
- **使用旧策略，长期观测新旧策略的diff**

## 方法论价值
通过系统化的A/B测试框架，确保推荐系统改进的科学性和可持续性。# Thank You!

http://wangshusen.github.io/

## 课程结束
感谢观看A/B测试相关内容。

## 参考资源
更多相关内容和资料可以访问: http://wangshusen.github.io/

## 学习成果
通过本课程，您已经了解了：
- A/B测试的基本原理
- 随机分桶方法
- 分层实验设计
- Holdout机制
- 实验推全与反转实验

这些知识将帮助您在推荐系统优化中进行科学的实验设计和效果评估。基于物品的协同过滤 (ItemCF)

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

小红书 (Red logo in bottom right corner)ItemCF的原理

我喜欢看《笑傲江湖》

《笑傲江湖》与《鹿鼎记》相似 → 给我推荐《鹿鼎记》

我没看过《鹿鼎记》

[Visual elements: Text boxes showing the logic flow with an arrow indicating the recommendation process]ItemCF的原理

我喜欢看《笑傲江湖》

《笑傲江湖》与《鹿鼎记》相似 → 给我推荐《鹿鼎记》

我没看过《鹿鼎记》

推荐系统如何知道《笑傲江湖》与《鹿鼎记》相似？

• 看过《笑傲江湖》的用户也看过《鹿鼎记》。
• 给《笑傲江湖》好评的用户也给《鹿鼎记》好评。

[Visual elements: The middle section is highlighted in yellow, with text boxes and an arrow showing the recommendation flow. The highlighted question is prominently displayed in a yellow box.]ItemCF的实现用户对物品的兴趣：
like(user, item_i)

[Diagram showing user-item interactions]
- User (represented by user icon with laptop)
- Arrows pointing to different items (video icons) labeled with numbers: 2, 1, 4, 3
- Items are labeled as "用户交互过的物品" (Items the user has interacted with)

[Visual elements: User icon connected to 4 red video/movie icons through numbered arrows, representing user interaction with items]用户对物品的兴趣：
like(user, item_i)

[Diagram showing user-item interactions with a candidate item]
- User (represented by user icon with laptop)
- Arrows pointing to different items (video icons) labeled with numbers: 2, 1, 4, 3
- Items are labeled as "用户交互过的物品" (Items the user has interacted with)
- Additional yellow item icon on the right labeled "Item" (candidate item for recommendation)

[Visual elements: Same as previous slide but with an additional yellow video icon on the right representing a potential item to recommend]用户对物品的兴趣：
like(user, item_i)

物品之间的相似度：
sim(item_j, item)

[Diagram showing user-item interactions with similarity scores]
- User (represented by user icon with laptop)  
- Arrows pointing to different items (video icons) labeled with numbers: 2, 1, 4, 3
- Items are labeled as "用户交互过的物品" (Items the user has interacted with)
- Dashed arrows pointing from the interacted items to a yellow candidate item showing similarity scores: 0.1, 0.4, 0.2, 0.6
- Yellow item labeled "Item" (candidate item for recommendation)

[Visual elements: Addition of dashed arrows with similarity scores connecting user's interacted items to the candidate item]用户对物品的兴趣：
like(user, item_i)

物品之间的相似度：
sim(item_j, item)

[Diagram showing user-item interactions with similarity scores]
- User (represented by user icon with laptop)
- Arrows pointing to different items (video icons) labeled with numbers: 2, 1, 4, 3
- Items are labeled as "用户交互过的物品" (Items the user has interacted with)
- Dashed arrows pointing from the interacted items to a yellow candidate item showing similarity scores: 0.1, 0.4, 0.2, 0.6
- Yellow item labeled "Item" (candidate item for recommendation)

预估用户对候选物品的兴趣：∑_j like(user, item_j) × sim(item_j, item)

[Visual elements: Mathematical formula shown in red box at bottom of slide, representing the calculation for predicting user interest in candidate item]用户对物品的兴趣：
like(user, item_i)

物品之间的相似度：
sim(item_j, item)

[Diagram showing user-item interactions with similarity scores and red highlighting]
- User (represented by user icon with laptop)
- Red arrows pointing to different items (video icons) labeled with numbers: 2, 1, 4, 3
- Items are labeled as "用户交互过的物品" (Items the user has interacted with)
- Red dashed arrows pointing from the interacted items to a yellow candidate item showing similarity scores: 0.1, 0.4, 0.2, 0.6
- Yellow item labeled "Item" (candidate item for recommendation)

预估用户对候选物品的兴趣：2×0.1 + 1×0.4 + 4×0.2 + 3×0.6 = 3.2

[Visual elements: Same diagram as previous slide but with red highlighting of all arrows and the calculation result shown in red box at bottom]物品的相似度物品相似度

• 两个物品的受众重合度越高，两个物品越相似。

• 例如：
  • 喜欢《射雕英雄传》和《神雕侠侣》的读者重合度很高。
  • 可以认为《射雕英雄传》和《神雕侠侣》相似。两个物品不相似

[Visual diagram showing two items with different user groups]
- Left side: Red video icon connected by arrows to 3 users (female user with pink shirt, female user with dark hair, blonde female user)
- Right side: Green video icon connected by arrows to 3 different users (male user with red tie, male user with glasses, male user with blue shirt)

[Visual elements: Diagram illustrates that two items are dissimilar because they have completely different user bases - no overlap between the user groups who interact with each item]两个物品相似

[Visual diagram showing two items with overlapping user groups]
- Left side: Blue video icon connected by arrows to 6 users
- Right side: Green video icon connected by arrows to the same 6 users
- Cross-connecting arrows show that users interact with both items
- Users represented by diverse icons: female user with pink shirt, female user with dark hair, blonde female user, male user with red tie, male user with glasses, male user with blue shirt

[Visual elements: Diagram illustrates that two items are similar because they have overlapping user bases - many users who interact with one item also interact with the other item, shown by the cross-connecting arrows]计算物品相似度

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。计算物品相似度

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。

• 两个物品的相似度：

sim(i₁, i₂) = |V| / √(|W₁| · |W₂|)

注：公式没有考虑喜欢的程度 like(user, item)计算物品相似度

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。

• 两个物品的相似度：

sim(i₁, i₂) = Σ_{v∈V} like(v, i₁) · like(v, i₂) / [√(Σ_{u₁∈W₁} like²(u₁, i₁)) · √(Σ_{u₂∈W₂} like²(u₂, i₂))]

余弦相似度 (cosine similarity)小结

• ItemCF 的基本思想：
  • 如果用户喜欢物品 item₁，而且物品 item₁ 与 item₂ 相似，
  • 那么用户很可能喜欢物品 item₂。小结

• ItemCF 的基本思想：
  • 如果用户喜欢物品 item₁，而且物品 item₁ 与 item₂ 相似，
  • 那么用户很可能喜欢物品 item₂。

• 预估用户对候选物品的兴趣：
    Σⱼ like(user, itemⱼ) × sim(itemⱼ, item).小结

• ItemCF 的基本思想：
  • 如果用户喜欢物品 item₁，而且物品 item₁ 与 item₂ 相似，
  • 那么用户很可能喜欢物品 item₂。

• 预估用户对候选物品的兴趣：
    Σⱼ like(user, itemⱼ) × sim(itemⱼ, item).

• 计算两个物品的相似度：
  • 把每个物品表示为一个稀疏向量，向量每个元素对应一个用户。
  • 相似度 sim 就是两个向量夹角的余弦。ItemCF 召回的完整流程事先做离线计算

建立“用户→物品”的索引

• 记录每个用户最近点击、交互过的物品ID。

• 给定任意用户ID，可以找到他近期感兴趣的物品列表。事先做离线计算

建立“用户→物品”的索引

• 记录每个用户最近点击、交互过的物品ID。

• 给定任意用户ID，可以找到他近期感兴趣的物品列表。

建立“物品→物品”的索引

• 计算物品之间两两相似度。

• 对于每个物品，索引它最相似的 k 个物品。

• 给定任意物品ID，可以快速找到它最相似的 k 个物品。“用户→物品”的索引

用户：“用户→物品”的索引

用户：                (物品ID，兴趣分数) 的列表：

[User icon] → [红色电影图标, 2] [黑色电影图标, 1] [蓝色电影图标, 4] [棕色电影图标, 3] …

[User icon] → [黄绿电影图标, 3] [绿色电影图标, 1] [黑色电影图标, 1] [浅蓝电影图标, 1] …“物品→物品”的索引

物品：“物品→物品”的索引

物品：                最相似的 k 个物品的 （ID，相似度）：

[红色电影图标] → [棕色电影图标, 0.7] [暗红电影图标, 0.6] [粉红电影图标, 0.6] [棕色电影图标, 0.3] [红色电影图标, 0.3]

[棕色电影图标] → [黄绿电影图标, 0.9] [绿色电影图标, 0.6] [黑色电影图标, 0.6] [浅蓝电影图标, 0.5] [灰色电影图标, 0.4]线上做召回

1. 给定用户ID，通过“用户→物品”索引，找到用户近期感兴趣的物品列表（last-n）。

2. 对于last-n列表中每个物品，通过“物品→物品”的索引，找到 top-k 相似物品。线上做召回

1. 给定用户ID，通过“用户→物品”索引，找到用户近期感兴趣的物品列表（last-n）。

2. 对于last-n列表中每个物品，通过“物品→物品”的索引，找到 top-k 相似物品。

3. 对于取回的相似物品（最多有 nk 个），用公式预估用户对物品的兴趣分数。

4. 返回分数最高的100个物品，作为推荐结果。线上做召回

索引的意义在于避免枚举所有的物品。

1. 记录用户最近感兴趣的 n = 200 个物品。

2. 取回每个物品最相似的 k = 10 个物品。

3. 给取回的 nk = 2000 个物品打分 (用户对物品的兴趣)。

4. 返回分数最高的 100 个物品作为 ItemCF 通道的输出。

用索引，离线计算量大，线上计算量小。线上做召回

用户感兴趣的物品（ID，兴趣分数）

[Image shows a user icon with an arrow pointing to a sequence of items represented by video icons with scores: (red video icon, 2), (black video icon, 1), (blue video icon, 4), (orange video icon, 3), ...]线上做召回

用户感兴趣的物品（ID，兴趣分数）

[Image shows user icon with arrow pointing to items: (red video, 2), (black video, 1), (blue video, 4), (orange video, 3), ...

Below the first red video item is a downward arrow leading to "Top-k 相似" showing two similar video items:
- (red video, 0.7)
- (dark red video, 0.6)]线上做召回

用户感兴趣的物品（ID，兴趣分数）

[Image shows user icon with arrow pointing to items: (red video, 2), (black video, 1), (blue video, 4), (orange video, 3), ...

Multiple downward arrows from each item leading to "Top-k 相似" sections showing similar items:
- From red video: (red video, 0.7), (dark red video, 0.6)
- From black video: (gray video, 0.8), (gray video, 0.5)
- From blue video: (olive video, 0.6), (dark video, 0.6)
- From orange video: (orange video, 0.9), (green video, 0.4)
- ...]线上做召回

用户感兴趣的物品（ID，兴趣分数）

[Image shows user icon with arrow pointing to items: (red video, 2), (black video, 1), (blue video, 4), (orange video, 3), ...

The "Top-k 相似" section is highlighted with a red rectangle showing all the similar items retrieved for each user-interested item.]线上做召回

用户感兴趣的物品（ID，兴趣分数）

[Image shows both the user-interested items (highlighted in red rectangle at top) and the retrieved similar items (highlighted in red rectangle at bottom) with the Top-k similarity process connecting them.]总结ItemCF的原理

• 用户喜欢物品 i₁，那么用户喜欢与物品 i₁ 相似的物品 i₂。

• 物品相似度：
  • 如果喜欢 i₁、i₂ 的用户有很大的重叠，那么 i₁ 与 i₂ 相似。
  • 公式：sim(i₁, i₂) = |W₁ ∩ W₂| / √(|W₁| · |W₂|)ItemCF召回通道

• 维护两个索引：
  • 用户→物品列表：用户最近交互过的 n 个物品。
  • 物品→物品列表：相似度最高的 k 个物品。

• 线上做召回：
  • 利用两个索引，每次取回 nk 个物品。
  • 预估用户对每个物品的兴趣分数：
    Σⱼ like(user, itemⱼ) × sim(itemⱼ, item)。
  • 返回分数最高的100个物品，作为召回结果。长期招聘优秀的算法工程师

• 部门：小红书社区技术部。

• 方向：搜索、推荐。

• 职位：校招、社招、实习。

• 地点：上海、北京。

• 联系方式：ShusenWang@xiaohongshu.comThank You!

http://wangshusen.github.io/Swing召回通道

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

小红书 (Red logo in bottom right corner)ItemCF的原理

• 物品相似度：如果喜欢 i₁、i₂ 的用户有很大的重叠，那么 i₁ 与 i₂ 相似。

• 用户喜欢物品 i₁  ⟶  用户很可能喜欢物品 i₂

• 物品 i₁ 与 i₂ 相似ItemCF的物品相似度

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。ItemCF的物品相似度

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。

• 两个物品的相似度：

sim(i₁, i₂) = |V| / √(|W₁| · |W₂|)

[The formula is highlighted with a red rectangle]ItemCF的物品相似度

[Image shows a bipartite graph with a red video item on the left and a green video item on the right, with 6 users in the center connected to both items with lines, representing users who liked both items]ItemCF的物品相似度

[Image shows the same bipartite graph with labels:]
- Left: 蜀川菜 (red video item)
- Right: 支持绿色能源 (green video item)  
- Center: 交集 V = W₁ ∩ W₂ (highlighted users in blue rectangle)

[6 users are connected to both video items, showing they liked both items]假如重合的用户是一个小圈子......

[Image shows the same bipartite graph with labels:]
- Left: 《某网站护肤品打折》 (red video item)
- Right: 《学节裁贞了》 (green video item)
- Center: 某个微信群 (highlighted users in blue rectangle)

[Shows users who liked both items forming a small social circle or WeChat group]Swing模型

• 用户 u₁ 喜欢的物品记作集合 J₁。

• 用户 u₂ 喜欢的物品记作集合 J₂。

• 定义两个用户的重合度：
  overlap(u₁, u₂) = |J₁ ∩ J₂|。

• 用户 u₁ 和 u₂ 的重合度高，则他们可能来自一个小圈子，要降低他们的权重。Swing模型

• 喜欢物品 i₁ 的用户记作集合 W₁。

• 喜欢物品 i₂ 的用户记作集合 W₂。

• 定义交集 V = W₁ ∩ W₂。

• 两个物品的相似度：

sim(i₁, i₂) = Σ Σ [1 / (α + overlap(u₁, u₂))]
              u₁∈V u₂∈V

[The formula is highlighted with a red rectangle]总结

• Swing 与 ItemCF 唯一的区别在于物品相似度。

• ItemCF：两个物品重合的用户比例高，则判定两个物品相似。

• Swing：额外考虑重合的用户是否来自一个小圈子。
  • 同时喜欢两个物品的用户记作集合 V。
  • 对于 V 中的用户 u₁ 和 u₂，重合度记作 overlap(u₁, u₂)。
  • 两个用户重合度大，则可能来自一个小圈子，权重降低。长期招聘优秀的算法工程师

• 部门：小红书社区技术部。

• 方向：搜索、推荐。

• 职位：校招、社招、实习。

• 地点：上海、北京。

• 联系方式：ShusenWang@xiaohongshu.com

[小红书 logo appears in the bottom right corner]Thank You!

http://wangshusen.github.io/基于用户的协同过滤 (UserCF)

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

小红书 (Red logo in bottom right corner)UserCF的原理

有很多跟我兴趣非常相似的网友

其中某个网友对某笔记点赞、转发  ⟶  给我推荐这篇笔记

我没看过这篇笔记UserCF的原理

有很多跟我兴趣非常相似的网友

其中某个网友对某笔记点赞、转发  ⟶  给我推荐这篇笔记

我没看过这篇笔记

推荐系统如何找到跟我兴趣非常相似的网友呢？

• 方法一：点击、点赞、收藏、转发的笔记有很大的重合。
• 方法二：关注的作者有很大的重合。UserCF 的实现用户之间的相似度：
sim(user, userj)

Diagram Description:
The slide shows a user-user similarity calculation illustration. On the left is a "User" icon (person with laptop), and on the right is a column of similar users (兴趣相似的用户). The similarity scores are shown as:
- 0.9 (to first similar user - businessman)
- 0.7 (to second similar user - man with glasses)
- 0.7 (to third similar user - business person)
- 0.4 (to fourth similar user - woman)

All connections are shown with dotted arrows from the main user to the similar users, displaying their respective similarity values.用户之间的相似度：
sim(user, userj)

用户对物品的兴趣：
like(useri, item)

Diagram Description:
This slide builds on the previous one by adding the item preference component. The left side shows the same user-user similarity structure with similarity scores (0.9, 0.7, 0.7, 0.4). The right side introduces an "Item" (movie icon) and shows user preferences:
- Similar users have different preference values for the item: 0, 1, 3, 0
- This demonstrates how similar users may have different levels of interest in specific items

The diagram illustrates the two key components of collaborative filtering: user similarity and item preferences.用户之间的相似度：
sim(user, userj)

用户对物品的兴趣：
like(useri, item)

预估用户对候选物品的兴趣：∑j sim(user, userj) × like(userj, item)

Diagram Description:
This slide completes the collaborative filtering formula by adding the prediction calculation. The bottom shows the mathematical formula highlighted in a red box:
∑j sim(user, userj) × like(userj, item)

This formula represents how to predict a user's interest in an item by:
1. Taking all similar users (j)
2. Multiplying each similar user's preference for the item by their similarity to the target user
3. Summing all these weighted preferences

The red highlighting emphasizes this as the core collaborative filtering prediction formula.用户之间的相似度：
sim(user, userj)

用户对物品的兴趣：
like(useri, item)

预估用户对候选物品的兴趣：0.9×0 + 0.7×1 + 0.7×3 + 0.4×0 = 2.8

Diagram Description:
This slide provides a concrete numerical example of the collaborative filtering calculation. The formula from the previous slide is now calculated with actual values:

0.9×0 + 0.7×1 + 0.7×3 + 0.4×0 = 2.8

This demonstrates:
- User with similarity 0.9 has interest 0 in the item
- User with similarity 0.7 has interest 1 in the item  
- User with similarity 0.7 has interest 3 in the item
- User with similarity 0.4 has interest 0 in the item

The final predicted interest score is 2.8, shown with red underlining to emphasize the result.用户的相似度

Section Title:
This slide presents a simple section heading "用户的相似度" (User Similarity) in large, centered text. This appears to be a section divider introducing the topic of user similarity calculation methods in collaborative filtering systems.两个用户不相似

Diagram Description:
This slide illustrates two users who are not similar to each other. The title "两个用户不相似" (Two users are not similar) is shown at the top, with "不相似" (not similar) highlighted in red.

The diagram shows:
- Left user: A man with glasses and teal sweater
- Right user: A woman with black hair and pink top
- Each user has arrows pointing to different sets of items (movie/book icons in various colors)
- The users have completely different preferences, with no overlap in the items they're interested in
- This demonstrates the concept of dissimilar users who don't share common interests两个用户相似

Diagram Description:
This slide contrasts with the previous one, showing two users who are similar. The title "两个用户相似" (Two users are similar) is displayed at the top, with "相似" (similar) highlighted in red.

The diagram shows:
- Left user: A man in business attire with a light blue shirt
- Right user: A man in business suit with red tie  
- Both users have arrows pointing to the same set of items (movie/book icons in various colors)
- Multiple connecting lines show both users interact with overlapping items
- This demonstrates the concept of similar users who share common interests and preferences

The crossing lines between users and items illustrate shared preferences, which is the foundation for collaborative filtering algorithms.计算用户相似度

• 用户 u1 喜欢的物品记作集合 J1。
• 用户 u2 喜欢的物品记作集合 J2。  
• 定义交集 I = J1 ∩ J2。

Content Description:
This slide introduces the mathematical framework for calculating user similarity. It defines:

1. J1: The set of items that user u1 likes
2. J2: The set of items that user u2 likes
3. I: The intersection set I = J1 ∩ J2 (items liked by both users)

This sets up the foundation for similarity calculation by identifying shared preferences between users through set intersection.计算用户相似度

• 用户 u1 喜欢的物品记作集合 J1。
• 用户 u2 喜欢的物品记作集合 J2。
• 定义交集 I = J1 ∩ J2。
• 两个用户的相似度：

sim(u1, u2) = |I| / √(|J1| · |J2|)

Formula Description:
This slide presents the complete user similarity calculation formula. Building on the previous slide's set definitions, it introduces the similarity metric:

sim(u1, u2) = |I| / √(|J1| · |J2|)

Where:
- |I| is the size of the intersection set (number of common items)  
- |J1| is the size of user u1's preferred items set
- |J2| is the size of user u2's preferred items set

The formula is highlighted in a red box to emphasize its importance. This represents a cosine similarity-based approach to measuring user similarity in collaborative filtering.降低热门物品权重

Diagram Description:
This slide addresses the issue of reducing the weight of popular items in similarity calculations. The title "降低热门物品权重" (Reduce the Weight of Popular Items) indicates this is about handling popularity bias.

The diagram shows:
- Left user: Man with glasses and teal sweater
- Right user: Woman with black hair and pink top  
- Both users have arrows pointing to various book/movie items displayed as covers

The items shown include:
- "Deep Learning for NLP and Speech Recognition" (Springer)
- "Deep Learning" book
- "System Design Interview" book  
- "Harry Potter" book
- Chinese books including "金瓶梅" and "红楼梦"
- "Warren Buffett" biography

This illustrates how popular items (like Harry Potter or classic Chinese literature) might need reduced weighting in similarity calculations to avoid overemphasizing common but less discriminative preferences.降低热门物品权重

• 用户 u1 喜欢的物品记作集合 J1。
• 用户 u2 喜欢的物品记作集合 J2。
• 定义交集 I = J1 ∩ J2。
• 两个用户的相似度：

不论冷门、热门，
物品权重都是 1。

sim(u1, u2) = ∑l∈I 1 / √(|J1| · |J2|) = |I|

Formula Description:
This slide addresses the issue of popular items having the same weight as niche items. The red annotation states "不论冷门、热门，物品权重都是1" (Whether cold or hot items, the weight is all 1).

The current formula treats all items equally:
- ∑l∈I 1 represents summing 1 for each common item
- This simplifies to |I| (the number of common items)
- This approach doesn't distinguish between popular and niche items in similarity calculation降低热门物品权重

• 用户 u1 喜欢的物品记作集合 J1。
• 用户 u2 喜欢的物品记作集合 J2。
• 定义交集 I = J1 ∩ J2。
• 两个用户的相似度：

sim(u1, u2) = ∑l∈I (1/log(1 + nl)) / √(|J1| · |J2|)

nl：喜欢物品 l 的用户数量，反映物品的热门程度

Formula Description:
This slide introduces the improved similarity formula that reduces the weight of popular items:

sim(u1, u2) = ∑l∈I (1/log(1 + nl)) / √(|J1| · |J2|)

Where nl represents the number of users who like item l, reflecting the item's popularity.

Key improvements:
- Uses 1/log(1 + nl) instead of just 1 for each common item
- Popular items (high nl) get lower weights due to the logarithmic penalty
- Niche items (low nl) maintain higher discriminative power
- This helps identify more meaningful similarities between users based on less common preferences小结

• UserCF 的基本思想：
  • 如果用户 user1 跟用户 user2 相似，而且 user2 喜欢某物品，
  • 那么用户 user1 也很可能喜欢该物品。

Summary Content:
This slide presents a summary (小结) of the basic concept of User-based Collaborative Filtering (UserCF):

The fundamental idea of UserCF:
- If user1 is similar to user2, and user2 likes a certain item,
- Then user1 is very likely to also like that item.

This captures the core assumption behind user-based collaborative filtering - that similar users will have similar preferences.小结

• UserCF 的基本思想：
  • 如果用户 user1 跟用户 user2 相似，而且 user2 喜欢某物品，
  • 那么用户 user1 也很可能喜欢该物品。

• 预估用户 user 对候选物品 item 的兴趣：
  ∑j sim(user, userj) × like(userj, item)

Summary Content:
This slide builds on the previous summary by adding the mathematical formulation. It includes:

1. The basic UserCF concept (same as previous slide)

2. The prediction formula for estimating a user's interest in a candidate item:
   ∑j sim(user, userj) × like(userj, item)

The key parts of the formula (user, userj, item) are highlighted with red underlines to emphasize the core components of collaborative filtering prediction.小结

• UserCF 的基本思想：
  • 如果用户 user1 跟用户 user2 相似，而且 user2 喜欢某物品，
  • 那么用户 user1 也很可能喜欢该物品。

• 预估用户 user 对候选物品 item 的兴趣：
  ∑j sim(user, userj) × like(userj, item)

• 计算两个用户的相似度：
  • 把每个用户表示为一个稀疏向量，向量每个元素对应一个物品。
  • 相似度 sim 就是两个向量夹角的余弦。

Summary Content:
This slide completes the UserCF summary by adding the explanation of user similarity calculation:

1. Basic UserCF concept (same as previous)
2. Prediction formula (same as previous, with sim(user, userj) highlighted in red box)
3. User similarity calculation method:
   - Represent each user as a sparse vector, where each element corresponds to an item
   - Similarity sim is the cosine of the angle between two vectors

This provides a complete mathematical framework understanding of user-based collaborative filtering.UserCF 召回的完整流程

Section Title:
This slide presents a section heading "UserCF 召回的完整流程" (Complete Process of UserCF Recall) in large, centered text. This appears to be a section divider introducing the end-to-end process of user-based collaborative filtering for item recall in recommendation systems.事先做离线计算

建立“用户→物品”的索引

• 记录每个用户最近点击、交互过的物品ID。
• 给定任意用户ID，可以找到他近期感兴趣的物品列表。

Content Description:
This slide introduces the first step of offline computation for UserCF: building a "User → Item" index.

Key points:
- Record each user's recently clicked and interacted item IDs
- For any given user ID, can find their recently interested item list

This creates the foundation for understanding user preferences and item interactions.事先做离线计算

建立“用户→物品”的索引

• 记录每个用户最近点击、交互过的物品ID。
• 给定任意用户ID，可以找到他近期感兴趣的物品列表。

建立“用户→用户”的索引

• 对于每个用户，索引他最相似的 k 个用户。
• 给定任意用户ID，可以快速找到他最相似的 k 个用户。

Content Description:
This slide builds on the previous one by adding the second index: "User → User" index.

New addition:
- For each user, index their k most similar users
- For any given user ID, can quickly find their k most similar users

This creates both user-item and user-user mappings needed for collaborative filtering.“用户→物品”的索引

用户：        （物品ID，兴趣分数）的列表：

Diagram Description:
This slide illustrates the "User → Item" index structure with concrete examples:

Two users are shown:
1. First user (person with laptop): Has a list of items with interest scores:
   - Red video item: score 2
   - Black video item: score 1  
   - Blue video item: score 4
   - Orange video item: score 3
   - ... (indicating more items)

2. Second user (woman): Has a different list of items with interest scores:
   - Brown video item: score 3
   - Green video item: score 1
   - Black video item: score 1
   - Light blue video item: score 1
   - ... (indicating more items)

The dots at the bottom indicate there are more users in the system. This demonstrates how each user maps to their personalized list of items with corresponding interest scores.“用户→用户”的索引

用户：

Diagram Description:
This slide introduces the "User → User" index structure. The title shows this will map each user to other similar users.

The slide shows:
- A header indicating this is about user-to-user indexing
- Two user avatars are displayed:
  1. A man with glasses and brown clothing
  2. A woman with red/pink hair
- Dots at the bottom indicate there are more users in the system

This sets up the concept that each user will be mapped to their most similar users, which is essential for collaborative filtering recommendations."用户→用户"的索引

用户：        最相似的 k 个用户的（ID，相似度）：

Diagram Description:
This slide shows the complete "User → User" index structure with concrete examples:

Two users are displayed with their top-k most similar users:

1. First user (man with glasses and brown clothing): Maps to 5 most similar users with similarity scores:
   - Businessman: 0.7
   - Man with glasses and teal sweater: 0.6
   - Business person with tie: 0.6
   - Person in light blue: 0.3
   - Person in dark blue: 0.3

2. Second user (woman with red/pink hair): Maps to 5 most similar users with similarity scores:
   - Woman with brown hair: 0.9
   - Woman with dark hair/headband: 0.6
   - Woman with brown hair: 0.6
   - Person in blue: 0.5
   - Person in teal: 0.4

The dots indicate more users exist in the system. This demonstrates how each user is mapped to their most similar users with quantified similarity scores.线上做召回

1. 给定用户ID，通过"用户→用户"索引，找到 top-k 相似用户。

2. 对于每个 top-k 相似用户，通过"用户→物品"索引，找到用户近期感兴趣的物品列表（last-n）。

Content Description:
This slide outlines the online recall process for UserCF with two key steps:

1. Given a user ID, use the "User → User" index to find the top-k similar users
2. For each of the top-k similar users, use the "User → Item" index to find their recently interested item list (last-n)

This describes the first two steps of the real-time recommendation process.线上做召回

1. 给定用户ID，通过"用户→用户"索引，找到 top-k 相似用户。

2. 对于每个 top-k 相似用户，通过"用户→物品"索引，找到用户近期感兴趣的物品列表（last-n）。

3. 对于取回的 nk 个相似物品，用公式预估用户对每个物品的兴趣分数。

4. 返回分数最高的100个物品，作为召回结果。

Content Description:
This slide presents the complete 4-step online recall process for UserCF:

1. Find top-k similar users using "User → User" index
2. Get recently interested items (last-n) for each similar user using "User → Item" index  
3. For the retrieved nk similar items, use the formula to predict user's interest score for each item
4. Return the top 100 highest-scoring items as the recall result

This provides the complete end-to-end process for generating recommendations in real-time.线上做召回

Top-k 相似的用户（ID，相似度）

Diagram Description:
This slide illustrates step 1 of the online recall process with a concrete example.

The diagram shows:
- A target user (person with laptop on left)
- An arrow pointing to their top-k most similar users
- The top-5 similar users are displayed in a table format with their IDs and similarity scores:
  - Businessman: 0.7
  - Man with glasses and teal sweater: 0.6  
  - Business person with tie: 0.6
  - Person in light blue: 0.3
  - Person in dark blue: 0.3

This demonstrates the first step where the system retrieves the most similar users to the target user for recommendation generation.线上做召回

Top-k 相似的用户（ID，相似度）

用户感兴趣的n个物品

Diagram Description:
This slide builds on the previous one by showing step 2 of the online recall process.

The diagram shows:
- The same target user and their top-k similar users from the previous slide
- An arrow pointing down from the first similar user (businessman with 0.7 similarity)
- A small table showing the user's interested items:
  - Red video item: score 1
  - Brown video item: score 3

This demonstrates step 2 where for each similar user, the system retrieves their recently interested items (last-n) to use for generating recommendations for the target user.
Z

Top-k <(7ID<	

(7tn*i

Diagram Description:
This slide shows the expanded view of step 2 in the online recall process.

The diagram illustrates:
- The target user and their top-5 similar users (same as previous slides)
- Arrows pointing down from each similar user to their interested items
- Each of the 5 similar users has 2 items with interest scores:
  
User 1 (0.7 similarity): Red video (1), Brown video (3)
User 2 (0.6 similarity): Grey video (4), Orange video (1)  
User 3 (0.6 similarity): Yellow/green video (3), Dark video (3)
User 4 (0.3 similarity): Orange video (1), Green video (2)
User 5 (0.3 similarity): Red video (4), Blue video (4)

This demonstrates how the system collects interested items from all similar users to create a candidate pool for recommendation.[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
[Content extracted from slide image - detailed mathematical formulas and diagrams about UserCF recommendation process]
Thank You!

http://wangshusen.github.io/

Content Description:
This is the final slide of the presentation with:
- Large "Thank You!" text in the center
- A website URL "http://wangshusen.github.io/" at the bottom
- Simple, clean layout marking the end of the UserCF (User-based Collaborative Filtering) presentation离散特征处理

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

Title Slide Description:
This is the title slide for a presentation about "离散特征处理" (Discrete Feature Processing) by:
- Instructor: 王树森 (Wang Shusen)
- Email: ShusenWang@xiaohongshu.com  
- Website: http://wangshusen.github.io/
- Small red logo appears in bottom right corner

This presentation focuses on handling discrete/categorical features in machine learning and recommendation systems.离散特征

• 性别：男、女两种类别。
• 国籍：中国、美国、印度等200个国家。
• 英文单词：常见的英文单词有几万个。
• 物品ID：小红书有几亿篇笔记，每篇笔记有一个ID。
• 用户ID：小红书有几亿个用户，每个用户有一个ID。

Content Description:
This slide introduces discrete/categorical features with examples:

• Gender: Male, Female (2 categories)
• Nationality: China, USA, India, etc. (~200 countries)
• English words: Common English words (tens of thousands)
• Item ID: XiaoHongShu has hundreds of millions of posts, each with an ID
• User ID: XiaoHongShu has hundreds of millions of users, each with an ID

This demonstrates the variety and scale of discrete features in recommendation systems, from small categorical variables to massive ID spaces.离散特征处理

1. 建立字典：把类别映射成序号。
   • 中国 → 1
   • 美国 → 2
   • 印度 → 3

2. 向量化：把序号映射成向量。
   • One-hot编码：把序号映射成高维稀疏向量。
   • Embedding：把序号映射成低维稠密向量。

Content Description:
This slide outlines the two-step process for handling discrete features:

1. **Build Dictionary**: Map categories to sequence numbers
   - China → 1
   - USA → 2
   - India → 3

2. **Vectorization**: Map sequence numbers to vectors
   - **One-hot encoding**: Map sequence numbers to high-dimensional sparse vectors
   - **Embedding**: Map sequence numbers to low-dimensional dense vectors

This establishes the foundation for converting categorical data into numerical representations that can be used by machine learning models.One-Hot 编码

Section Title:
This slide introduces the "One-Hot 编码" (One-Hot Encoding) section with large, centered text. This is a section divider introducing the concept of one-hot encoding for discrete feature processing in machine learning and recommendation systems.例1：性别特征

• 性别：男、女两种类别。
• 字典：男 → 1，女 → 2。
• One-hot编码：用 2 维向量表示性别。
  • 未知 → 0 → [0, 0]
  • 男  → 1 → [1, 0]
  • 女  → 2 → [0, 1]

Content Description:
This slide provides Example 1: Gender Feature using one-hot encoding.

The process shown:
1. **Categories**: Male, Female (2 categories)
2. **Dictionary mapping**: Male → 1, Female → 2
3. **One-hot encoding**: Use 2-dimensional vector to represent gender
   - Unknown → 0 → [0, 0] (all zeros for missing values)
   - Male → 1 → [1, 0] (first position is 1)
   - Female → 2 → [0, 1] (second position is 1)

The red highlighting emphasizes the one-hot vectors, showing how categorical data is converted to sparse binary vectors.[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
[Content from slide about discrete feature processing - includes technical details, formulas, and examples]
Thank You!

http://wangshusen.github.io/

Content Description:
This is the final slide of the discrete feature processing presentation with:
- Large "Thank You!" text centered on the slide
- Website URL "http://wangshusen.github.io/" at the bottom
- Clean, simple layout marking the conclusion of the presentation on handling discrete/categorical features in machine learning and recommendation systems
# 矩阵补充 (Matrix Completion)

## 主讲人
王树森

## 网站链接
http://wangshusen.github.io/

## 品牌标识
小红书 (标识在右下角)# Matrix Completion Architecture Diagram

## Components:
1. **内积 (Inner Product)**: (a, b)
   - Position: Top center with purple node

2. **User Side (Left)**:
   - Blue embedding vector **a**
   - Embedding Layer (灰色方框)
   - Blue circle: 用户ID (User ID)

3. **Item Side (Right)**:
   - Red embedding vector **b**
   - Embedding Layer (灰色方框)
   - Red circle: 物品ID (Item ID)

## Process Flow:
- User ID and Item ID are processed through separate embedding layers
- Each produces an embedding vector (a for user, b for item)
- The vectors are combined using inner product to produce the final result (a, b)
- Arrows indicate the flow from IDs through embedding layers to final computation# Matrix Completion with Parameter Sharing

## Architecture:
1. **内积 (Inner Product)**: (a, b)
   - Position: Top center with purple node

2. **User Side (Left)** - Blue Box:
   - Blue embedding vector **a**
   - Embedding Layer (灰色方框)
   - Blue circle: 用户ID (User ID)

3. **Item Side (Right)** - Blue Box:
   - Red embedding vector **b**  
   - Embedding Layer (灰色方框)
   - Red circle: 物品ID (Item ID)

## Key Features:
- Both user and item sides are enclosed in blue boxes
- Same embedding layer structure for both sides
- Separate processing paths that converge at the inner product
- Parameter sharing architecture indicated by the consistent blue framing# Matrix Completion with Shared Parameters

## Architecture:
1. **内积 (Inner Product)**: (a, b)
   - Position: Top center with purple node

2. **User Side (Left)**:
   - Blue embedding vector **a**
   - Embedding Layer (灰色方框)
   - Blue circle: 用户ID (User ID)

3. **Item Side (Right)**:
   - Red embedding vector **b**
   - Embedding Layer (灰色方框)  
   - Red circle: 物品ID (Item ID)

4. **Key Feature**:
   - **不共享参数** (Do Not Share Parameters) - text between the two embedding layers
   - Blue curved line connecting both embedding layers indicating the relationship

## Technical Details:
- Separate embedding layers with independent parameters
- Blue boundary encompasses both sides showing the overall architecture
- Different parameter sets for user and item embeddings# 训练 (Training)

## Section Title
Training phase of the matrix completion model

## Content Summary
This appears to be a section divider slide introducing the training phase of the matrix completion approach.# 基本想法 (Basic Idea)

## User Embedding Parameters:
• 用户 embedding 参数矩阵记作 **A**。第 u 号用户对应矩阵第 u 列，记作向量 **a_u**。

## Item Embedding Parameters:
• 物品 embedding 参数矩阵记作 **B**。第 i 号物品对应矩阵第 i 列，记作向量 **b_i**。

## Visual Representation:
- **Matrix A** (Left): Blue-tinted matrix with highlighted blue column representing a_u
- **Matrix B** (Right): Red-tinted matrix with highlighted red column representing b_i

## Key Points:
- User embeddings stored as columns in matrix A
- Item embeddings stored as columns in matrix B
- Each user u has corresponding embedding vector a_u
- Each item i has corresponding embedding vector b_i# 基本想法 (Basic Idea)

## User Embedding Parameters:
• 用户 embedding 参数矩阵记作 **A**。第 u 号用户对应矩阵第 u 列，记作向量 **a_u**。

## Item Embedding Parameters:
• 物品 embedding 参数矩阵记作 **B**。第 i 号物品对应矩阵第 i 列，记作向量 **b_i**。

## Inner Product Prediction:
• 内积 ⟨**a_u**, **b_i**⟩ 是第 u 号用户对第 i 号物品兴趣的预估值。

## Training Objective:
• 训练模型的目的是学习矩阵 **A** 和 **B**，使得预估值尽可能接近实观测的兴趣分数。

## Key Concepts:
- Inner product ⟨a_u, b_i⟩ predicts user u's interest in item i
- Training aims to learn matrices A and B
- Goal is to make predictions close to observed interest scores# 数据集 (Dataset)

## Dataset Definition:
• 数据集：（用户ID，物品ID，兴趣分数）的集合，记作  
  Ω = {(u, i, y)}

## Interest Score Description:
• 数据集中的兴趣分数是系统记录的，比如：

### Score Examples:
• 曝光但是没有点击 → 0 分
• 点击、点赞、收藏、转发 → 各算 1 分  
• 分数最低是 0，最高是 4。

## Key Points:
- Dataset contains triplets of (user ID, item ID, interest score)
- Interest scores are system-recorded behavioral signals
- Score range: 0 to 4
- Different user actions correspond to different scores
- No interaction (exposure only) = 0 points
- Positive interactions (click, like, save, share) = 1 point each# 训练 (Training)

## ID to Vector Mapping:
• 把用户ID、物品ID映射成向量。

### Mapping Process:
• 第 u 号用户 → 向量 **a_u**。
• 第 i 号物品 → 向量 **b_i**。

## Key Concepts:
- Convert user IDs and item IDs into embedding vectors
- Each user u maps to vector a_u
- Each item i maps to vector b_i
- This is the foundation for the embedding-based approach# 训练 (Training)

## ID to Vector Mapping:
• 把用户ID、物品ID映射成向量。

### Mapping Process:
• 第 u 号用户 → 向量 **a_u**。
• 第 i 号物品 → 向量 **b_i**。

## Optimization Problem:
• 求解优化问题，得到参数 **A** 和 **B**。

### Objective Function:
```
min     Σ        (y - ⟨a_u, b_i⟩)²
A,B  (u,i,y)∈Ω
```

## Key Components:
- Minimize squared error between predicted and actual ratings
- Learn embedding matrices A and B
- Sum over all observed (user, item, rating) triplets in dataset Ω
- Inner product ⟨a_u, b_i⟩ predicts the rating# 矩阵补充 (Matrix Completion)

## Matrix Representation:
A sparse user-item rating matrix with the following characteristics:

### Matrix Structure:
- **每行对应一个用户** (Each row corresponds to one user)
- **每列对应一个物品** (Each column corresponds to one item)

### Rating Values:
The matrix contains various integer ratings from 0 to 4:
- Values include: 0, 1, 2, 3, 4
- Most cells are empty (sparse matrix)
- Green cells contain observed ratings
- Gray cells represent unobserved interactions

### Sample Data Points:
- Multiple entries of 0, 1, 2, 3, 4 distributed across the matrix
- Sparse structure with many missing values
- Each non-empty cell represents a user-item interaction with its corresponding rating# 矩阵补充 (Matrix Completion)

## Matrix Legend:
**绿色位置表示曝光给用户的物品；灰色位置表示没有曝光。**
(Green positions indicate items exposed to users; gray positions indicate no exposure.)

## Matrix Structure:
- **每行对应一个用户** (Each row corresponds to one user)
- **每列对应一个物品** (Each column corresponds to one item)

## Example Highlighted:
**第3号用户对第2号物品的兴趣分数等于4**
(User #3's interest score for Item #2 equals 4)

### Visual Elements:
- Green arrow pointing to a specific cell with value 4
- Matrix shows sparse rating data
- Green cells: observed user-item interactions (with exposure)
- Gray cells: unobserved interactions (no exposure)

### Rating Distribution:
Matrix contains various ratings: 0, 1, 2, 3, 4
- Sparse structure with many missing entries
- Each green cell represents an actual user-item rating# 在实践中效果不好...... (Poor Performance in Practice)

## 缺点1：仅用ID embedding，没利用物品、用户属性。

### Item Attributes:
• 物品属性：类目、关键词、地理位置、作者信息。

### User Attributes:
• 用户属性：性别、年龄、地理定位、感兴趣的类目。

### Model Enhancement:
• 双塔模型可以看做矩阵补充的升级版。

## Key Issues:
- Only uses ID embeddings without leveraging rich item and user features
- Ignores valuable content and demographic information
- Two-tower model can be seen as an upgraded version of matrix completion# 在实践中效果不好...... (Poor Performance in Practice)

## 缺点1：仅用ID embedding，没利用物品、用户属性。

## 缺点2：负样本的选取方式不对。

### Sample Types:
• 样本：用户——物品的二元组，记作 (u, i)。

• 正样本：曝光之后，有点击、交互。（正确的做法）

• 负样本：曝光之后，没有点击、交互。（错误的做法）

## Key Problem:
- Incorrect negative sampling approach
- Should only consider items that were exposed but not interacted with
- Treating all non-exposed items as negative samples is problematic# 在实践中效果不好...... (Poor Performance in Practice)

## 缺点1：仅用ID embedding，没利用物品、用户属性。

## 缺点2：负样本的选取方式不对。

## 缺点3：做训练的方法不好。

### Training Issues:
• 内积 ⟨**a_u**, **b_i**⟩ 不如余弦相似度。

• 用平方损失（回归），不如用交叉熵损失（分类）。

## Alternative Approaches:
- Cosine similarity is better than inner product for similarity measurement
- Classification with cross-entropy loss is better than regression with squared loss
- These represent fundamental improvements to the training methodology# 线上服务 (Online Service)

## Section Title
Introduction to the online serving phase

## Content Summary
This slide serves as a section divider introducing the online service component of the matrix completion system.# 模型存储 (Model Storage)

## Step 1: Train Matrices A and B
1. 训练得到矩阵 **A** 和 **B**。

### Matrix Structure:
• **A** 的每一列对应一个用户。
• **B** 的每一列对应一个物品。

## Key Components:
- Matrix A contains user embeddings (each column represents a user)
- Matrix B contains item embeddings (each column represents an item)
- These matrices are the result of the training process# 模型存储 (Model Storage)

## Step 1: Train Matrices A and B
1. 训练得到矩阵 **A** 和 **B**。

### Matrix Structure:
• **A** 的每一列对应一个用户。
• **B** 的每一列对应一个物品。

## Step 2: Store Matrix A in Key-Value Table
2. 把矩阵 **A** 的列存储到 key-value 表。

### Key-Value Structure:
• key 是用户ID，value 是 **A** 的一列。
• 给定用户ID，返回一个向量（用户的 embedding）。

## Step 3: Matrix B Storage and Indexing
3. 矩阵 **B** 的存储和索引比较复杂。

## Implementation Details:
- Matrix A stored as lookup table for fast user embedding retrieval
- Matrix B requires more complex indexing for efficient item recommendations# 线上服务 (Online Service)

## Step 1: User Embedding Lookup
1. 把用户 ID 作为 key，查询 key-value 表，得到该用户的向量，记作 **a**。

## Process Overview:
- Use user ID as key to query the key-value table
- Retrieve the user's embedding vector a
- This is the first step in the online recommendation process# 线上服务 (Online Service)

## Step 1: User Embedding Lookup
1. 把用户 ID 作为 key，查询 key-value 表，得到该用户的向量，记作 **a**。

## Step 2: Nearest Neighbor Search
2. 最近邻查找：查找用户最有可能感兴趣的 k 个物品，作为召回结果。

### Similarity Calculation:
• 第 i 号物品的 embedding 向量记作 **b_i**。
• 内积 ⟨**a**, **b_i**⟩ 是用户对第 i 号物品兴趣的预估。
• 返回内积最大的 k 个物品。

## Recommendation Process:
- Calculate inner product between user embedding a and each item embedding b_i
- Rank items by similarity score
- Return top k items with highest scores# 线上服务 (Online Service) - Complete Process

## Step 1: User Embedding Lookup
1. 把用户 ID 作为 key，查询 key-value 表，得到该用户的向量，记作 **a**。

## Step 2: Nearest Neighbor Search
2. 最近邻查找：查找用户最有可能感兴趣的 k 个物品，作为召回结果。

### Detailed Similarity Calculation:
• 第 i 号物品的 embedding 向量记作 **b_i**。
• 内积 ⟨**a**, **b_i**⟩ 是用户对第 i 号物品兴趣的预估。
• 返回内积最大的 k 个物品。

## Computational Complexity Issue:
**如果放算所有物品，时间复杂度正比于物品数量。**

### Performance Challenge:
- Brute force search through all items is computationally expensive
- Time complexity scales linearly with number of items
- Need efficient indexing and search strategies for large catalogs# 近似最近邻查找
(Approximate Nearest Neighbor Search)

## Section Title
Introduction to Approximate Nearest Neighbor Search techniques

## Purpose
This section introduces methods to efficiently find approximate nearest neighbors in high-dimensional embedding spaces, addressing the computational complexity issues of exhaustive search.# 支持最近邻查找的系统

## Available Systems:
• 系统：Milvus、Faiss、HnswLib、等等。

## Distance Metrics for Nearest Neighbor:
• 衡量最近邻的标准：

### Available Metrics:
• 欧式距离最小（L2 距离）
• 向量内积最大（内积相似度）
• 向量夹角余弦最大（cosine相似度）

## Key Systems:
- **Milvus**: Distributed vector database
- **Faiss**: Facebook AI Similarity Search library
- **HnswLib**: Hierarchical Navigable Small World graphs
- And many others

## Similarity Measures:
- **L2 Distance**: Euclidean distance minimization
- **Inner Product**: Dot product maximization
- **Cosine Similarity**: Cosine of angle between vectors# Vector Space Visualization

## 2D Embedding Space Scatter Plot

This visualization shows item embeddings distributed in a 2D coordinate space with:

### Visual Elements:
- **X-axis**: First dimension of embedding space
- **Y-axis**: Second dimension of embedding space
- **Colored dots**: Individual item embeddings
- **Color coding**: Different colors represent different item categories or clusters

### Item Distribution:
- Points scattered across four quadrants
- Various colors including:
  - Green spectrum (light to dark green)
  - Blue spectrum (light to dark blue) 
  - Red spectrum (pink to dark red)
  - Orange/yellow spectrum
  - Purple points

### Clustering Patterns:
- Items appear to form natural clusters by color
- Similar items (same color) tend to be grouped together
- Clear separation between different item types

This visualization demonstrates how embedding vectors capture item similarities in the learned vector space, where proximity indicates similarity for recommendation purposes.# User Query Vector in Embedding Space

## Vector Space with User Query

This visualization shows the same 2D embedding space with the addition of:

### New Element:
- **Black star (★a)**: User embedding vector representing the query user's preferences
- Located in the lower-right quadrant of the coordinate system

### Spatial Analysis:
- User vector **a** is positioned among various item embeddings
- Proximity to different colored clusters indicates potential user interests
- Items closer to the user vector **a** are more likely to be relevant recommendations

### Recommendation Strategy:
- Calculate similarity between user vector **a** and all item vectors (colored dots)
- Items with shortest distance or highest similarity to **a** become top recommendations
- The visual proximity in this 2D space represents the mathematical similarity in the full embedding space

### Use Case:
- This represents the first step in the online serving process
- After retrieving user embedding **a**, the system searches for nearest item neighbors
- The spatial arrangement guides the approximate nearest neighbor search# Angular Partitioning for Approximate Search

## Partitioning Strategy

This visualization demonstrates an angular partitioning approach for efficient nearest neighbor search:

### Partitioning Method:
- **Circular division**: The 2D space is divided into angular sectors/wedges
- **Multi-colored regions**: Each sector represents a partition in the search index
- **Radial lines**: Two diagonal arrows (orange and blue) show the partitioning boundaries

### Color-coded Sectors:
- **Yellow/Green sectors**: Upper quadrants
- **Blue/Cyan sectors**: Left quadrants  
- **Pink/Red sectors**: Lower-right quadrants
- **Orange sectors**: Right quadrants

### Search Optimization:
- Items are pre-grouped by angular position
- For a query vector, only relevant sectors need to be searched
- This reduces computational complexity from O(n) to O(n/k) where k is number of partitions

### Technical Implementation:
- Each item embedding is assigned to a sector based on its angular position
- Query processing focuses on the most relevant sectors first
- This is a form of approximate nearest neighbor search used in systems like LSH (Locality Sensitive Hashing)# Multi-directional Angular Partitioning

## Enhanced Partitioning Scheme

This shows a more complex angular partitioning system:

### Directional Vectors:
- **Multiple colored arrows** radiating from the center origin
- **Green arrows**: Upper-left directions (3 arrows)
- **Orange arrows**: Right direction (2 arrows)
- **Blue arrow**: Lower-left direction
- **Purple arrow**: Lower direction
- **Red arrow**: Lower-right direction
- **Cyan arrow**: Left direction

### Advanced Search Strategy:
- **Multi-directional search**: Query can be matched against multiple relevant directions
- **Hierarchical partitioning**: Different angular granularities for different regions
- **Adaptive search**: System can choose optimal directions based on query characteristics

### Implementation Benefits:
- More precise partitioning than simple radial division
- Better handling of query vectors that fall between major partitions
- Improved approximation quality while maintaining efficiency
- Can be adapted for different embedding space densities

### Use in Practice:
- This type of partitioning is used in advanced ANN systems
- Helps balance search accuracy with computational efficiency
- Particularly useful for high-dimensional embedding spaces# Query Vector with Directional Search Paths

## User Query in Multi-directional System

This visualization combines the user query vector with the directional partitioning system:

### Key Elements:
- **Black star (★a)**: User query vector positioned in the lower-right area
- **Colored directional arrows**: Multiple search directions from the origin
- **Spatial relationship**: Query vector **a** is positioned relative to the directional grid

### Search Process:
1. **Query vector identification**: User embedding **a** is located in the space
2. **Direction matching**: System identifies which directional arrows are most relevant to **a**
3. **Focused search**: Items along relevant directions are prioritized for similarity computation

### Relevant Directions for Query **a**:
- **Orange arrows** (right direction): High relevance due to proximity
- **Red arrow** (lower-right): Direct alignment with query position
- **Purple arrow** (lower): Secondary relevance

### Efficiency Gains:
- Instead of computing similarity with all items, focus on items in relevant directions
- Significant reduction in computational load
- Maintains high-quality recommendations by targeting the most promising search directions

### Practical Implementation:
- This represents how modern ANN systems optimize user queries
- Combines spatial indexing with directional search strategies# Focused Search Region

## Angular Sector Search with Query

This demonstrates the focused search approach:

### Key Elements:
- **Light orange/pink sector**: Highlighted search region
- **Orange arrow**: Primary search direction
- **Black star (★a)**: User query vector
- **Colored dots**: Item embeddings within the search sector

### Items in Search Region:
- **Orange dots**: Items closely aligned with search direction
- **Red dots**: Items in the sector but with different characteristics
- **Pink dots**: Items at sector boundaries

### Search Strategy:
1. **Sector identification**: Query vector **a** determines the relevant angular sector
2. **Focused computation**: Only items within the highlighted sector are considered
3. **Efficient processing**: Dramatically reduces computational load compared to exhaustive search

### Performance Benefits:
- **Reduced complexity**: From O(n) to O(n/k) where k is number of sectors
- **Maintained accuracy**: Items in relevant sectors are most likely to be good recommendations
- **Scalable approach**: Works well with large item catalogs

### Practical Implementation:
- This sector-based approach is commonly used in production recommendation systems
- Balances recommendation quality with computational efficiency# Refined Search with Elliptical Region

## Advanced Proximity-Based Search

This shows a more sophisticated search region:

### Key Features:
- **Blue elliptical boundary**: Defines the search region around query vector **a**
- **Orange arrow**: Primary search direction
- **Black star (★a)**: User query vector at the center
- **Light orange sector**: Background angular sector

### Items in Elliptical Region:
- **Orange dots**: High-priority items within the ellipse
- **Red dots**: Medium-priority items at ellipse boundaries
- **Pink dots**: Lower-priority items in the outer region

### Enhanced Search Strategy:
1. **Dual filtering**: Combines angular sector with distance-based elliptical region
2. **Proximity weighting**: Items closer to query vector get higher priority
3. **Shape optimization**: Elliptical region can be tuned for different recommendation scenarios

### Technical Advantages:
- **Better precision**: Elliptical boundaries provide more accurate similarity regions
- **Flexible shape**: Can adapt ellipse orientation and size based on data distribution
- **Multi-criteria filtering**: Combines directional and distance-based filtering

### Real-world Applications:
- Advanced ANN systems use similar elliptical or hyperspherical regions
- Particularly effective for high-dimensional embedding spaces
- Commonly implemented in systems like FAISS with custom index structures# 总结 (Summary)

## Matrix Completion Summary

This section provides the key takeaways from the Matrix Completion approach for recommendation systems.# 矩阵补充 (Matrix Completion) - Summary

## Core Approach:
• 把物品ID、用户ID做 embedding，映射成向量。

## Similarity Calculation:
• 两个向量的内积⟨**a_u**, **b_i**⟩作为用户 u 对物品 i 兴趣的预估。

## Training Objective:
• 让⟨**a_u**, **b_i**⟩拟合真实观测的兴趣分数，学习模型的 embedding 层参数。

## Fundamental Limitations:
• 矩阵补充模型有很多缺点，效果不好。

## Key Issues Identified:
1. **Limited feature usage**: Only uses ID embeddings, ignores rich user and item attributes
2. **Poor negative sampling**: Incorrect handling of negative examples
3. **Suboptimal training methods**: Inner product and squared loss are not ideal choices

## Practical Impact:
- Foundation for modern recommendation systems
- Led to development of more sophisticated approaches like two-tower models
- Important stepping stone but not suitable for production use# 线上召回 (Online Retrieval)

## Query Process:
• 把用户向量 **a** 作为 query，查找使得⟨**a**, **b_i**⟩最大化的物品 i。

## Performance Challenge:
• 暴力析举速度太慢。实践中用近似最近邻查找。

## Technical Solutions:
• Milvus、Faiss、HnswLib 等向量数据库支持近似最近邻查找。

## Key Components:
1. **User vector retrieval**: Get embedding vector **a** for the user
2. **Similarity search**: Find items with maximum inner product ⟨**a**, **b_i**⟩
3. **Efficient indexing**: Use ANN systems to avoid brute force search
4. **Scalable serving**: Handle large item catalogs with sub-linear search complexity

## Production Considerations:
- **Speed vs. Accuracy tradeoff**: ANN provides approximate but fast results
- **Index maintenance**: Regular updates required as item catalog changes
- **Memory management**: Balance between index size and query performance
- **Quality metrics**: Monitor recommendation relevance despite approximations# Thank You!

## Presentation Conclusion

### Course Website:
http://wangshusen.github.io/

## Summary:
This concludes the presentation on Matrix Completion for recommendation systems. The presentation covered:

- **Basic concepts** of matrix completion approach
- **Training methodology** using embedding layers
- **Practical limitations** and why it performs poorly in practice
- **Online serving** challenges and solutions
- **Approximate nearest neighbor search** techniques
- **Vector database systems** for efficient retrieval

### Key Takeaway:
While Matrix Completion provides a foundational understanding of embedding-based recommendation systems, its practical limitations have led to the development of more sophisticated approaches like two-tower models and other advanced architectures.

### Next Steps:
This material serves as preparation for understanding more advanced recommendation system architectures that address the limitations identified in this basic matrix completion approach.# 双塔模型：模型和训练
(Two-Tower Model: Model and Training)

## 主讲人
王树森

## 联系信息
ShusenWang@xiaohongshu.com

## 网站链接
http://wangshusen.github.io/

## 品牌标识
小红书 (Xiaohongshu/Little Red Book logo in bottom right)

## 课程概述
This presentation covers the Two-Tower model architecture for recommendation systems, focusing on both the model structure and training methodologies.# 矩阵补充模型 (Matrix Completion Model)

## 架构图示：
1. **内积 (Inner Product)**: (a, b)
   - Position: Top center with purple node

2. **User Side (Left)**:
   - Blue embedding vector **a**
   - Embedding Layer (灰色方框)
   - Blue circle: 用户ID (User ID)

3. **Item Side (Right)**:
   - Red embedding vector **b**
   - Embedding Layer (灰色方框)
   - Red circle: 物品ID (Item ID)

## 流程说明：
- User ID and Item ID are processed through separate embedding layers
- Each produces an embedding vector (a for user, b for item)
- The vectors are combined using inner product to produce the final result (a, b)
- Arrows indicate the flow from IDs through embedding layers to final computation

This represents the basic matrix completion approach that serves as a foundation for understanding the two-tower model.# 双塔模型 (Two-Tower Model)

## Section Title
Introduction to the Two-Tower Model architecture

## Content Summary
This slide serves as a section divider introducing the Two-Tower model, which is an advanced architecture that builds upon the basic matrix completion approach but incorporates additional features and more sophisticated processing.# Two-Tower Model Architecture - User Tower

## User Tower Components:

### 1. User ID Processing (Left):
- **Input**: 用户ID (User ID) - Blue circle
- **Processing**: Embedding Layer (gray box)
- **Output**: Blue embedding vector

### 2. User Discrete Features (Center):
- **Input**: 用户离散特征 (User Discrete Features) - Blue circle
- **Processing**: Embedding Layers (gray box)
- **Output**: Blue embedding vector

### 3. User Continuous Features (Right):
- **Input**: 用户连续特征 (User Continuous Features) - Blue vector at bottom
- **Processing**: 归一化、分箱等处理 (Normalization, Binning, etc.)
- **Output**: Cyan embedding vector

## Key Improvements over Matrix Completion:
- **Multi-modal inputs**: Handles different types of user features
- **Feature processing**: Specialized handling for discrete vs. continuous features
- **Rich representation**: Combines ID, categorical, and numerical user information

## Processing Pipeline:
User features → Feature-specific processing → Embeddings → User tower representation# Complete User Tower Architecture

## User Feature Processing:

### Feature Concatenation:
- All user feature embeddings are concatenated together
- **Concatenate** operation combines multiple feature vectors

### Neural Network Processing:
- **神经网络 (Neural Network)** processes the concatenated features
- Transforms raw features into meaningful representations

### Final Output:
- **用户的表征 (User Representation)** - Final user embedding vector

## Input Features:
1. **用户ID (User ID)**:
   - Processed through Embedding Layer
   - Generates user-specific representation

2. **用户离散特征 (User Discrete Features)**:
   - Categorical features (e.g., age group, gender, location)
   - Processed through multiple Embedding Layers

3. **用户连续特征 (User Continuous Features)**:
   - Numerical features (e.g., activity scores, engagement metrics)
   - Processed through 归一化、分箱等处理 (Normalization, Binning, etc.)

## Architecture Benefits:
- **Multi-modal fusion**: Combines different types of user information
- **Deep learning**: Neural network learns complex feature interactions
- **Scalable processing**: Handles various user feature types efficiently# Page 6 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_006.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 7 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_007.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 8 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_008.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 9 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_009.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 10 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_010.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 11 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_011.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 12 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_012.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 13 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_013.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 14 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_014.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 15 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_015.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 16 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_016.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 17 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_017.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 18 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_018.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 19 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_019.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 20 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_020.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 21 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_021.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 22 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_022.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 23 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_023.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 24 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_024.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 25 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_025.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 26 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_026.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 27 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_027.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 28 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_028.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 29 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_029.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 30 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_030.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
# Page 31 - Two-Tower Model

## Content
This page contains information about the Two-Tower model architecture for recommendation systems.

## Status
Content extracted from slide 02_Retrieval_06_page_031.png

## Note
This slide is part of the Two-Tower Model presentation covering model architecture, training methodologies, and implementation details.
双塔模型：正负样本

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/正样本

• 正样本：曝光而且有点击的用户-物品二元组。
（用户对物品感兴趣）。

• 问题：少部分物品占据大部分点击，导致正样本
大多是热门物品。

• 解决方案：过采样冷门物品，或降采样热门物品。

• 过采样（up-sampling）：一个样本出现多次。

• 降采样（down-sampling）：一些样本被抛弃。推荐系统的链路

几亿物品 -> 召回 -> 几千物品 -> 粗排、精排 -> 几百物品 -> 重排 -> 物品1, 物品2, ..., 物品80如何选择负样本？

几亿物品 -> 召回 -> 几千物品 -> 粗排、精排 -> 几百物品 -> 重排 -> 物品1, 物品2, ..., 物品80

没有被召回                  被召回，但是                      被曝光，但是
                        没有选中和曝光                     没有被用户点击

                                        负样本简单负样本简单负样本：全体物品

• 未被召回的物品，大概率是用户不感兴趣的。

• 未被召回的物品 ≈ 全体物品

• 从全体物品中做抽样，作为负样本。

• 均匀抽样 or 非均匀抽样？简单负样本：全体物品

均匀抽样：对冷门物品不公平

• 正样本大多是热门物品。

• 如果均匀抽样产生负样本，负样本大多是冷门物品。

非均匀抽样：目的是打压热门物品

• 负样本抽样概率与热门程度（点击次数）正相关。

• 抽样概率 ∝ (点击次数)^0.75。简单负样本：Batch内负样本

用户：           物品：
[用户图标] ——点击——> [视频图标]
[用户图标] ——点击——> [视频图标]
⋮                ⋮
[用户图标] ——点击——> [视频图标]简单负样本：Batch内负样本

用户：           物品：
[用户图标] ——点击——> [视频图标]
[用户图标] ——点击——> [视频图标] ——— 正样本 （被紫色椭圆圈起来）
⋮                ⋮
[用户图标] ——点击——> [视频图标]简单负样本：Batch内负样本

用户：           物品：
[用户图标] ——点击——> [视频图标]

• 一个batch内有n个正样本。

[用户图标] ——点击——> [视频图标]

• 一个用户和n-1个物品组成
  负样本。

⋮                ⋮
                 负样本
                 （绿色区域标注）

• 这个batch内一共有n(n-1)
  个负样本。

[用户图标] ——点击——> [视频图标]

• 都是简单负样本。（因为第一
  个用户不喜欢第二个物品。）简单负样本：Batch内负样本

用户：           物品：
[用户图标] ——点击——> [视频图标]

• 一个物品出现在batch内的概
  率 ∝ 点击次数。

[用户图标] ——点击——> [视频图标]

• 物品成为负样本的概率本该是
  ∝ (点击次数)^0.75，但在这里是
  ∝ 点击次数。

⋮                ⋮

• 热门物品成为负样本的概率过
  大。

[用户图标] ——点击——> [视频图标]

参考文献：
• Xinyang Yi et al. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations. In RecSys, 2019.简单负样本：Batch内负样本

用户：           物品：
[用户图标] ——点击——> [视频图标]

• 物品i被抽样到的概率：
  pi ∝ 点击次数

[用户图标] ——点击——> [视频图标]

• 预估用户对物品i的兴趣：
  cos(a, bi)

⋮                ⋮

• 做训练的时候，调整为：
  cos(a, bi) - log pi

[用户图标] ——点击——> [视频图标]

参考文献：
• Xinyang Yi et al. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations. In RecSys, 2019.困难负样本困难负样本

• 困难负样本：
  • 被粗排淘汰的物品（比较困难）。
  • 精排分数靠后的物品（非常困难）。

• 对正负样本做二元分类：
  • 全体物品（简单）分类准确率高。
  • 被粗排淘汰的物品（比较困难）容易分错。
  • 精排分数靠后的物品（非常困难）更容易分错。训练数据

• 混合几种负样本。

• 50%的负样本是全体物品（简单负样本）。

• 50%的负样本是没通过排序的物品（困难负样本）。常见的错误曝光但是没有点击

用户点击    物品1
         物品2    用户浏览
用户未点击   物品3
负样本？   物品4
         物品5    用户未浏览
         物品6
         物品7
         ⋮
         物品80曝光但是没有点击

用户点击    物品1
         物品2    用户浏览
用户未点击   物品3
负样本？   物品4
         物品5    用户未浏览
         物品6
         物品7
         ⋮
         物品80

训练召回模型不能用这类负样本

训练排序模型会用这类负样本选择负样本的原理

召回的目标：快速找到用户可能感兴趣的物品。

• 全体物品（easy）：绝大多数是用户根本不感兴趣的。

• 被排序淘汰（hard）：用户可能感兴趣，但是不够感兴趣。

• 有曝光没点击（没用）：用户感兴趣，可能碰巧没有点击。

可以作为排序的负样本，
不能作为召回的负样本总结

→ • 正样本：曝光而且有点击。

→ • 简单负样本：
  • 全体物品。
  • batch内负样本。

→ • 困难负样本：被召回，但是被排序淘汰。

→ • 错误：曝光、但是未点击的物品做召回的负样本。Thank You!

http://wangshusen.github.io/长期招聘优秀的算法工程师

• 部门：小红书社区技术部。

• 方向：搜索、推荐。

• 职位：校招、社招、实习。

• 地点：上海、北京。

• 联系方式：ShusenWang@xiaohongshu.com双塔模型：线上召回和更新

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/线上召回离线存储

把〈特征向量b, 物品ID〉
保存到向量数据库

用户ID、离散特征、连续特征 ——> 特征变换 ——> 神经网络 ——> a

物品ID、离散特征、连续特征 ——> 特征变换 ——> 神经网络 ——> b离线存储

用户ID、离散特征、连续特征 ——> 特征变换 ——> 神经网络 ——> a

向量数据库:
ID=1 ——> [红色向量]
ID=2 ——> [红色向量]
ID=3 ——> [红色向量]
ID=4 ——> [红色向量]
⋮
ID=n ——> [红色向量]线上召回

给定用户ID和特征，
在线上计算向量a。

用户ID、离散特征、连续特征 ——> 特征变换 ——> 神经网络 ——> a（用蓝色虚线框标注）

向量数据库:
ID=1 ——> [红色向量]
ID=2 ——> [红色向量]
ID=3 ——> [红色向量]
ID=4 ——> [红色向量]
⋮
ID=n ——> [红色向量]

红色箭头从a指向向量数据库双塔模型的召回

离线存储：把物品向量b存入向量数据库。

1. 完成训练之后，用物品塔计算每个物品的特征向量b。

2. 把几亿个物品向量b存入向量数据库（比如Milvus、Faiss、HnswLib）。

3. 向量数据库建索引，以便加速最近邻查找。双塔模型的召回

离线存储：把物品向量b存入向量数据库。

线上召回：查找用户最感兴趣的k个物品。

1. 给定用户ID和画像，线上用神经网络算用户向量a。

2. 最近邻查找：

• 把向量a作为query，调用向量数据库做最近邻查找。

• 返回余弦相似度最大的k个物品，作为召回结果。双塔模型的召回

事先存储物品向量b，线上现算用户向量a，why？

• 每做一次召回，用到一个用户向量a，几亿物品向量b。
（线上算物品向量的代价过大。）

• 用户兴趣动态变化，而物品特征相对稳定。（可以离线存储用户向量，但不利于推荐效果。）模型更新全量更新 vs 增量更新

全量更新：今天凌晨，用昨天全天的数据训练模型。

• 在昨天模型参数的基础上做训练。（不是随机初始化）

• 用昨天的数据，训练1 epoch，即每条数据只用一遍。

• 发布新的用户塔神经网络和物品向量，供线上召回使用。

• 全量更新对数据流、系统的要求比较低。全量更新 vs 增量更新

增量更新：做online learning更新模型参数。

• 用户兴趣会随时发生变化。

• 实时收集线上数据，做流式处理，生成TFRecord文件。

• 对模型做online learning，增量更新ID Embedding参数。
（不更新神经网络其他部分的参数。）

• 发布用户ID Embedding，供用户塔在线上计算用户向量。全量更新 vs 增量更新

基于前天的全量模型，用
前天的数据，做全量更新。

前天的数据 → 昨天凌晨 → 做增量更新全量更新 vs 增量更新

基于昨天的全量模型，用
昨天的数据，做全量更新。

前天的数据 → 昨天凌晨 → 昨天的数据 → 今天凌晨 → 做增量更新全量更新 vs 增量更新

问题：能否只做增量更新，不做全量更新？

前天的数据 → 昨天凌晨 → 昨天的数据 → 今天凌晨 → 做增量更新
（红色箭头指示连续的增量更新链条）全量更新 vs 增量更新

问题：能否只做增量更新，不做全量更新？

• 小时级数据有偏；分钟级数据偏差更大。

• 全量更新：random shuffle一天的数据，做1 epoch训练。

• 增量更新：按照数据从早到晚的顺序，做1 epoch训练。

• 随机打乱优于按顺序排列数据，全量训练优于增量训练。总结双塔模型

• 用户塔、物品塔各输出一个向量，两个向量的余弦相似度作为兴趣的预估值。

• 三种训练的方式：pointwise、pairwise、listwise。

• 正样本：用户点击过的物品。

• 负样本：全体物品（简单）、被排序淘汰的物品（困难）。召回

• 做完训练，把物品向量存储到向量数据库，供线上最近邻查找。

• 线上召回时，给定用户ID、用户画像，调用用户塔现算用户向量a。

• 把a作为query，查询向量数据库，找到余弦相似度最高的k个物品向量，返回k个物品ID。更新模型

• 全量更新：今天凌晨，用昨天的数据训练整个神经网络，做1 epoch的随机梯度下降。

• 增量更新：用实时数据训练神经网络，只更新ID Embedding，锁住全连接层。

• 实际的系统：
  • 全量更新 & 增量更新相结合。
  • 每隔几十分钟，发布最新的用户ID Embedding，供用户塔在线上计算用户向量。Thank You!

http://wangshusen.github.io/长期招聘优秀的算法工程师

• 部门：小红书社区技术部。

• 方向：搜索、推荐。

• 职位：校招、社招、实习。

• 地点：上海、北京。

• 联系方式：ShusenWang@xiaohongshu.com双塔模型+自监督学习

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/双塔模型

余弦相似度：cos(a, b) = (a, b) / (||a||₂ · ||b||₂)

用户塔        物品塔
用户特征      物品特征双塔模型的问题

• 推荐系统的头部效应严重：
  • 少部分物品占据大部分点击。
  • 大部分物品的点击次数不高。

• 高点击物品的表征学得好，长尾物品的表征学得不好。

• 自监督学习：做 data augmentation，更好地学习长尾物品的向量表征。

参考文献：
• Tiansheng Yoo et al. Self-supervised Learning for Large-scale Item Recommendations. In CIKM, 2021.复习：双塔模型的训练Batch内负样本

用户：          物品：

[图显示用户和物品的对应关系，多个用户点击不同的视频项目]

⋮               ⋮Batch内负样本

用户：          物品：

[图显示用户与物品的关系，其中一个用户-物品对被紫色圆圈标记为"正样本"]

⋮               ⋮Batch内负样本

用户：          物品：

• 一个 batch 内有 n 对正样本。

• 组成 n 个 list，每个 list 中有 1 对正样本和 n - 1 对负样本。

⋮               ⋮

负样本Listwise训练

• 一个 batch 包含 n 对正样本（有点击）：

(a₁, b₁), (a₂, b₂), ⋯ , (aₙ, bₙ).

• 负样本：{(aᵢ, bⱼ)}，对于所有的 i ≠ j。

• 鼓励 cos(aᵢ, bᵢ) 尽量大，cos(aᵢ, bⱼ) 尽量小。损失函数

向量 pᵢ : p_{i,1}, p_{i,2}, ⋯, p_{i,i}, ⋯, p_{i,n}

Softmax激活函数

cos(aᵢ, b₁), cos(aᵢ, b₂), ⋯, cos(aᵢ, bᵢ), ⋯, cos(aᵢ, bₙ)
                                正样本损失函数

向量 yᵢ : 0, 0, ⋯, 1, ⋯, 0

向量 pᵢ : p_{i,1}, p_{i,2}, ⋯, p_{i,i}, ⋯, p_{i,n}

Softmax激活函数

cos(aᵢ, b₁), cos(aᵢ, b₂), ⋯, cos(aᵢ, bᵢ), ⋯, cos(aᵢ, bₙ)
                                正样本损失函数

向量 yᵢ : 0, 0, ⋯, 1, ⋯, 0

CrossEntropyLoss(yᵢ, pᵢ) = -log p_{i,i} = -log(exp(cos(aᵢ,bᵢ))/∑ⁿⱼ₌₁ exp(cos(aᵢ,bⱼ)))

向量 pᵢ : p_{i,1}, p_{i,2}, ⋯, p_{i,i}, ⋯, p_{i,n}

Softmax激活函数

cos(aᵢ, b₁), cos(aᵢ, b₂), ⋯, cos(aᵢ, bᵢ), ⋯, cos(aᵢ, bₙ)
                                正样本纠偏

• 物品 j 被抽样到的概率：
  
  pⱼ ∝ 点击次数

• 预估用户 i 对物品 j 的兴趣：cos(aᵢ, bⱼ)

• 做训练的时候，把 cos(aᵢ, bⱼ) 替换为：
  
  cos(aᵢ, bⱼ) - log pⱼ

参考文献：
• Xinyang Yi et al. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations. In RecSys, 2019.训练双塔模型

• 从点击数据中随机抽取 n 个用户—物品二元组，组成一个 batch。

• 双塔模型的损失函数：

  L_main[i] = -log(exp(cos(aᵢ,bᵢ)-log pᵢ)/∑ⁿⱼ₌₁ exp(cos(aᵢ,bⱼ)-log pⱼ))
  对应用户i

• 做梯度下降，减小损失函数：
  
  1/n ∑ⁿᵢ₌₁ L_main[i]自监督学习

参考文献：
• Tiansheng Yoo et al. Self-supervised Learning for Large-scale Item Recommendations. In CIKM, 2021.[图显示物品塔的共享参数架构]

物品塔  物品塔  共享参数  物品塔  物品塔

特征 i'   特征 i''       特征 j'   特征 j''

变换 1   物品 i   变换 2         变换 1   物品 j   变换 2[图显示物品塔共享参数架构，输出高相似度的向量]

高相似度          高相似度

b'ᵢ      b''ᵢ     b'ⱼ      b''ⱼ

物品塔  物品塔  共享参数  物品塔  物品塔

特征 i'   特征 i''       特征 j'   特征 j''

变换 1   物品 i   变换 2         变换 1   物品 j   变换 2[图显示物品塔共享参数架构，输出低相似度的向量]

低相似度

b'ᵢ      b''ᵢ     b'ⱼ      b''ⱼ

物品塔  物品塔  物品塔  物品塔

特征 i'   特征 i''  特征 j'   特征 j''

变换 1   物品 i   变换 2    变换 1   物品 j   变换 2自监督学习

• 物品 i 的两个向量表征 b'ᵢ 和 b''ᵢ 有较高的相似度。

• 物品 i 和 j 的向量表征 b'ᵢ 和 b''ⱼ 有较低的相似度。

• 鼓励 cos(b'ᵢ, b''ᵢ) 尽量大，cos(b'ᵢ, b''ⱼ) 尽量小。自监督学习

特征变换：Random Mask

• 随机选一些离散特征（比如类目），把它们遮住。

• 例：
  • 某物品的类目特征是 u = {数码, 摄影}。
  • Mask 后的类目特征是 u' = {default}。自监督学习

特征变换：Dropout（仅对多值离散特征生效）

• 一个物品可以有多个类目，那么类目是一个多值离散特征。

• Dropout：随机去弃特征中 50% 的值。

• 例：
  • 某物品的类目特征是 u = {美妆, 摄影}。
  • Dropout 后的类目特征是 u' = {美妆}。自监督学习

特征变换：互补特征（complementary）

• 假设物品一共有 4 种特征：

ID，类目，关键词，城市

• 随机分成两组：

{ID，关键词} 和 {类目，城市}

• {ID，default，关键词，default} → 物品表征

• {default，类目，default，城市} → 物品表征

鼓励两个向量相似自监督学习

特征变换：Mask一组关联的特征

• 受众性别：U = {男，女，中性}

• 类目：V = {美妆，数码，足球，摄影，科技，···}

• u = 女 和 v = 美妆 同时出现的概率 p(u,v) 大。

• u = 女 和 v = 数码 同时出现的概率 p(u,v) 小。自监督学习

特征变换：Mask一组关联的特征

• p(u)：某特征取值为 u 的概率。

  • p(男性) = 20%
  • p(女性) = 30%
  • p(中性) = 50%自监督学习

特征变换：Mask一组关联的特征

• p(u)：某特征取值为 u 的概率。

• p(u,v)：某特征取值为 u，另一个特征取值为 v，同时发生的概率。

  • p(女性，美妆) = 3%
  • p(女性，数码) = 0.1%自监督学习

特征变换：Mask一组关联的特征

• p(u)：某特征取值为 u 的概率。

• p(u,v)：某特征取值为 u，另一个特征取值为 v，同时发生的概率。

• 离线计算特征两两之间的关联，用互信息（mutual information）衡量：

MI(U,V) = ∑u∈U ∑v∈V p(u,v)·log[p(u,v)/(p(u)·p(v))]自监督学习

特征变换：Mask一组关联的特征

• 设一共有 k 种特征。离线计算特征两两之间 MI，得到 k×k 的矩阵。

• 随机选一个特征作为种子，找到种子最相关的 k/2 种特征。

• Mask 种子及其相关的 k/2 种特征，保留其余的 k/2 种特征。自监督学习

特征变换：Mask一组关联的特征

• 好处：比 random mask、dropout、互补特征等方法效果更好。

• 坏处：方法复杂，实现的难度大，不容易维护。自监督学习

特征变换：Random Mask

特征变换：Dropout（仅对多值离散特征生效）

特征变换：互补特征（complementary）

特征变换：Mask一组关联的特征训练模型

• 从全体物品中均匀抽样，得到 m 个物品，作为一个 batch。

• 做两类特征变换，物品塔输出两组向量：

b'₁, b'₂, ···, b'ₘ  和  b''₁, b''₂, ···, b''ₘ

• 第 i 个物品的损失函数：

L_self[i] = -log(exp(cos(b'ᵢ,b''ᵢ))/∑ⱼ₌₁ᵐ exp(cos(b'ᵢ,b''ⱼ)))训练模型

s_{i,1}    s_{i,2}    ⋯    s_{i,i}    ⋯    s_{i,m}

Softmax激活函数

cos(b'ᵢ, b''₁)  cos(b'ᵢ, b''₂)  ⋯  cos(b'ᵢ, b''ᵢ)  ⋯  cos(b'ᵢ, b''ₘ)
                                      正样本训练模型

向量 sᵢ : s_{i,1}    s_{i,2}    ⋯    s_{i,i}    ⋯    s_{i,m}

Softmax激活函数

cos(b'ᵢ, b''₁)  cos(b'ᵢ, b''₂)  ⋯  cos(b'ᵢ, b''ᵢ)  ⋯  cos(b'ᵢ, b''ₘ)
                                      正样本训练模型

向量 yᵢ : 0    0    ⋯    1    ⋯    0

向量 sᵢ : s_{i,1}    s_{i,2}    ⋯    s_{i,i}    ⋯    s_{i,m}

Softmax激活函数

cos(b'ᵢ, b''₁)  cos(b'ᵢ, b''₂)  ⋯  cos(b'ᵢ, b''ᵢ)  ⋯  cos(b'ᵢ, b''ₘ)
                                      正样本训练模型

向量 yᵢ : 0    0    ⋯    1    ⋯    0

CrossEntropyLoss(yᵢ, sᵢ) = -log s_{i,i} = -log(exp(cos(b'ᵢ,b''ᵢ))/∑ⱼ₌₁ᵐ exp(cos(b'ᵢ,b''ⱼ)))

向量 sᵢ : s_{i,1}    s_{i,2}    ⋯    s_{i,i}    ⋯    s_{i,m}

Softmax激活函数

cos(b'ᵢ, b''₁)  cos(b'ᵢ, b''₂)  ⋯  cos(b'ᵢ, b''ᵢ)  ⋯  cos(b'ᵢ, b''ₘ)
                                      正样本训练模型

• 自监督学习的损失函数：

L_self[i] = -log(exp(cos(b'ᵢ,b''ᵢ))/∑ⱼ₌₁ᵐ exp(cos(b'ᵢ,b''ⱼ)))

• 做梯度下降，减小自监督学习的损失：

1/m ∑ᵢ₌₁ᵐ L_self[i]总结总结

• 双塔模型学不好低曝光物品的向量表征。

• 自监督学习：
  • 对物品做随机特征变换。
  • 特征向量 b'ᵢ 和 b''ᵢ 相似度高（相同物品）。
  • 特征向量 b'ᵢ 和 b''ⱼ 相似度低（不同物品）。

• 实验效果：低曝光物品、新物品的推荐变得更准。训练模型

• 对点击做随机抽样，得到 n 对用户—物品二元组，作为一个 batch。

• 从全体物品中均匀抽样，得到 m 个物品，作为一个 batch。

• 做梯度下降，使得损失减小：

1/n ∑ⁿᵢ₌₁ L_main[i] + α · 1/m ∑ᵐⱼ₌₁ L_self[j]

双塔模型的损失        自监督学习的损失Thank You!

http://wangshusen.github.io/Deep Retrieval

王树森

http://wangshusen.github.io/Deep Retrieval

• 经典的双塔模型把用户、物品表示为向量，线上做最近邻查找。

• Deep Retrieval [1] 把物品表征为路径（path），线上查找用户最匹配的路径。

• Deep Retrieval 类似于阿里的 TDM [2]。

参考文献：
1. Weihao Gao et al. Learning A Retrievable Structure for Large-Scale Recommendations. In CIKM, 2021.
2. Han Zhu et al. Learning Tree-based Deep Model for Recommender Systems. In KDD, 2018.Outline

1. 索引：
   • 路径 → List<物品>
   • 物品 → List<路径>

2. 预估模型：神经网络预估用户对路径的兴趣。

3. 线上召回：用户 → 路径 → 物品。

4. 训练：
   • 学习神经网络参数。
   • 学习物品表征（物品 → 路径）。索引物品表征为路径

L1        L2        L3
1         1         ●
●  ────→  2  ────→  2
3         3         3
4         4         4
5         5         5
⋮         ⋮         ⋮
K         K         K

• 深度：depth = 3。

• 宽度：width = K。

• 把一个物品表示为一条路径（path），比如 [2,4,1]。物品表征为路径

L1                L2                L3                • 深度：depth = 3。

[Diagram showing three levels L1, L2, L3 with numbered nodes:
- L1: nodes 1, red node, 3, blue node, 5, ..., K
- L2: blue node 1, node 2, node 3, red node, node 5, ..., K  
- L3: red node, node 2, node 3, node 4, node 5, ..., K
- Blue and red arrows connecting nodes between levels]

• 宽度：width = K。

• 把一个物品表示为一条路径
  (path)，比如 [2, 4, 1]。

• 一个物品可以表示为多条路径，
  比如 {[2, 4, 1], [4, 1, 1]}。物品到路径的索引

索引：item → List(path)

• 一个物品对应多条路径。

• 用3个节点表示一条路径：path = [a, b, c]。

索引：path → List(item)

• 一条路径对应多个物品。预估模型预估用户对路径的兴趣

• 用3个节点表示一条路径：path = [a, b, c]。

• 给定用户特征x，预估用户对节点a的兴趣 p₁(a|x)。

• 给定x和a，预估用户对节点b的兴趣 p₂(b|a; x)。

• 给定x, a, b，预估用户对节点c的兴趣 p₃(c|a, b; x)。预估用户对路径的兴趣

• 用3个节点表示一条路径：path = [a, b, c]。

• 给定用户特征x，预估用户对节点a的兴趣 p₁(a|x)。

• 给定x和a，预估用户对节点b的兴趣 p₂(b|a; x)。

• 给定x, a, b，预估用户对节点c的兴趣 p₃(c|a, b; x)。

• 预估用户对 path = [a, b, c] 兴趣：
  p(a, b, c|x) = p₁(a|x) × p₂(b|a; x) × p₃(c|a, b; x).用户特征 x → 神经网络 → softmax → p₁ (选择) → a

[Diagram showing neural network architecture with:
- Input: user features x (shown as stacked bars)
- Neural network processing
- Softmax layer
- Output probability distribution p₁
- Selection of node a
- Below shows tree structure with levels L1, L2, L3 containing nodes 1, 2, 3, 4, ..., K]用户特征 x → 神经网络 → softmax → p₁ (选择) → a

[Top path with red arrow showing embedding connection]

x ⊕ emb(a) → 神经网络 → softmax → p₂ (选择) → b

[Diagram showing:
- User features x combined with embedding of selected node a
- Neural network processing 
- Softmax layer
- Output probability distribution p₂
- Selection of node b
- Red arrow labeled "embedding" connecting selected node a to the embedding input]用户特征 x → 神经网络 → softmax → p₁ (选择) → a

[Top path with pink arrow showing embedding connection]

x ⊕ emb(a) → 神经网络 → softmax → p₂ (选择) → b

[Middle path with red arrow showing embedding connection]

x ⊕ emb(a) ⊕ emb(b) → 神经网络 → softmax → p₃ (选择) → c

[Diagram showing three-level hierarchical selection:
- Level 1: User features x → select node a
- Level 2: x + emb(a) → select node b  
- Level 3: x + emb(a) + emb(b) → select node c
- Pink and red arrows labeled "embedding" showing how selected nodes feed back as embeddings]线上召回线上召回

召回：用户 → 路径 → 物品

• 第一步：给定用户特征，用 beam search 召回一批路径。

• 第二步：利用索引"path → List(item)"，召回一批物品。

• 第三步：对物品做打分和排序，选出一个子集。Beam Search

• 假设有3层，每层K个节点，那么一共有K³条路径。

• 用神经网络给所有K³条路径打分，计算量太大。

• 用beam search，可以减小计算量。

• 需要设置超参数beam size。Beam Search (size = 1)

L1

1  p₁(1|x)
2  p₁(2|x)  
3  p₁(3|x)
4  p₁(4|x)
5  p₁(5|x)  [red highlighted]
⋮  ⋮
K  p₁(K|x)Beam Search (size = 1)

L1                    L2

1                     1  p₂(1|5; x)
2                     2  p₂(2|5; x)
3                     3  p₂(3|5; x)
4                     4  p₂(4|5; x)  [red highlighted]
5 ----[arrows]----->  5  p₂(5|5; x)
⋮                     ⋮  ⋮
K                     K  p₂(K|5; x)

[Diagram shows arrows connecting red node 5 from L1 to all nodes in L2]Beam Search (size = 1)

L1                    L2

1                     1
2                     2
3                     3
4                     4  [red highlighted]
5 ----[red arrow]--->  5
⋮                     ⋮
K                     K

[Diagram shows red arrow connecting red node 5 from L1 to red node 4 in L2, with gray arrows to other nodes]Beam Search (size = 1)

L1                    L2                         L3

1                     1                          1  p₃(1|5, 4; x)  [red highlighted]
2                     2                          2  p₃(2|5, 4; x)
3                     3                          3  p₃(3|5, 4; x)
4                     4 ----[arrows]---->        4  p₃(4|5, 4; x)
5 ----[red arrow]--->  5                          5  p₃(5|5, 4; x)
⋮                     ⋮                          ⋮  ⋮
K                     K                          K  p₃(K|5, 4; x)

[Diagram shows progression: red node 5 in L1 → red node 4 in L2 → arrows to all nodes in L3, with node 1 highlighted red]Beam Search (size = 1)

L1                    L2                         L3

1                     1                          1  [red highlighted]
2                     2                          2
3                     3                          3
4                     4                          4
5 ----[red arrows]--> 4 ----[red arrow]---->    5
⋮                     5                          ⋮
K                     ⋮                          K
                      K

选中路径 path = [5, 4, 1]Beam Search (size = 1)

L1                    L2                         L3

1                     1                          1  [red highlighted]  
2                     2                          2
3                     3                          3  
4                     4                          4
5 ----[red arrows]--> 4 ----[red arrow]---->    5
⋮                     5                          ⋮
K                     ⋮                          K
                      K

选中路径 path = [5, 4, 1]Beam Search

• 用户对 path = [a, b, c] 兴趣：
    p(a, b, c|x) = p₁(a|x) × p₂(b|a; x) × p₃(c|a, b; x).

• 最优的路径：
    [a*, b*, c*] = argmax p(a, b, c | x)
                    a,b,c

• 贪心算法（beam size = 1）选中的路径 [a, b, c] 未必是最优的路径。Beam Search (size = 4)

L1

1  p₁(1|x)
2  p₁(2|x)  [red highlighted]
3  p₁(3|x)  [red highlighted]
4  p₁(4|x)
5  p₁(5|x)  [red highlighted]
6  p₁(6|x)
7  p₁(7|x)  [red highlighted]
8  p₁(8|x)
⋮  ⋮
K  p₁(K|x)Beam Search (size = 4)

L1                    L2

1                     1
2  ----[arrows]----> 2
3  ----[arrows]----> 3
4                     4
5  ----[arrows]----> 5
6                     6
7  ----[arrows]----> 7
8                     8
⋮                     ⋮
K                     K

[Diagram shows the top 4 nodes (2, 3, 5, 7) from L1 connecting to all nodes in L2 with arrow connections]Beam Search (size = 4)

L1                    L2

1                     1
2  ----[arrows]----> 2
3  ----[arrows]----> 3  
4                     4
5  ----[arrows]----> 5
6                     6
7  ----[arrows]----> 7
8                     8
⋮                     ⋮
K                     K

• 对于每个被选中的节点 a，计算用户对路径 [a, b] 的兴趣：
    p₁(a|x) × p₂(b|a; x).

• 算出 4×K 个分数，每个分数对应一条路径，选出分数 top 4 路径。Beam Search (size = 4)

L1                    L2

1                     1  [red highlighted]
2  ----[red arrows]-> 2  [red highlighted]
3  ----[red arrows]-> 3  [red highlighted]
4                     4
5  ----[red arrows]-> 5
6                     6
7  ----[red arrows]-> 7
8                     8  [red highlighted]
⋮                     ⋮
K                     K

• 对于每个被选中的节点 a，计算用户对路径 [a, b] 的兴趣：
    p₁(a|x) × p₂(b|a; x).

• 算出 4×K 个分数，每个分数对应一条路径，选出分数 top 4 路径。Beam Search (size = 4)

L1                    L2                         L3

1                     1  ----[arrows]---->       1
2  ----[red arrows]-> 2  ----[arrows]---->       2
3  ----[red arrows]-> 3  ----[arrows]---->       3
4                     4                          4
5  ----[red arrows]-> 5                          5
6                     6                          6
7  ----[red arrows]-> 7                          7
8                     8  ----[arrows]---->       8
⋮                     ⋮                          ⋮
K                     K                          K

[Diagram shows the top 4 selected paths from L2 (nodes 1, 2, 3, 8) connecting to all nodes in L3]Beam Search (size = 4)

L1                    L2                         L3

1                     1  ----[red arrows]---->   1  [red highlighted]
2  ----[red arrows]-> 2  ----[red arrows]---->   2
3  ----[red arrows]-> 3  ----[red arrows]---->   3
4                     4                          4  [red highlighted]
5  ----[red arrows]-> 5                          5
6                     6                          6
7  ----[red arrows]-> 7                          7
8                     8  ----[red arrows]---->   8  [red highlighted]
⋮                     ⋮                          ⋮
K                     K                          K

[Diagram shows final beam search selection with 4 paths highlighted in red, connecting through all three levels]Beam Search (size = 4)

L1                    L2                         L3

1                     1  ----[red arrow]----->   1  [red highlighted]
2  ----[red arrow]--> 2  
3  ----[red arrow]--> 3  ----[red arrow]----->   4  [red highlighted] 
4                     4                          5
5  ----[red arrow]--> 5                          6
6                     6                          7
7                     7                          8  [red highlighted]
8                     8  ----[red arrow]----->   ⋮  [red highlighted]
⋮                     ⋮                          K
K                     K

[Simplified diagram showing final 4 selected paths: top 4 scoring complete paths through the tree structure]线上召回

• 第一步：给定用户特征，用神经网络做预估，用 beam search 召回一批路径。

• 第二步：利用索引，召回一批物品。

  • 查看索引 path → List(item)。
  
  • 每条路径对应多个物品。

• 第三步：对物品做排序，选出一个子集。

线上召回：user → path → item训练训练

同时学习神经网络参数和物品表征

• 神经网络 p(a, b, c | x) 预估用户对路径 [a, b, c] 的兴趣。

• 把一个物品表征为多条路径 {[a, b, c]}，建立索引：

  • item → List(path)，
  
  • path → List(item)。

• 正样本 (user, item)：click(user, item) = 1。学习神经网络参数

• 物品表征为 J 条路径：[a₁, b₁, c₁], ⋯, [aⱼ, bⱼ, cⱼ]。

• 用户对路径 [a, b, c] 的兴趣：
    p(a, b, c | x) = p₁(a | x) × p₂(b | a; x) × p₃(c | a, b; x).

• 如果用户点击过物品，说明用户对 J 条路径感兴趣。学习神经网络参数

• 物品表征为 J 条路径：[a₁, b₁, c₁], ⋯, [aⱼ, bⱼ, cⱼ]。

• 用户对路径 [a, b, c] 的兴趣：
    p(a, b, c | x) = p₁(a | x) × p₂(b | a; x) × p₃(c | a, b; x).

• 如果用户点击过物品，说明用户对 J 条路径感兴趣。

• 应该让 ∑ʲⱼ₌₁ p(aⱼ, bⱼ, cⱼ | x) 变大。

• 损失函数：loss = -log(∑ʲⱼ₌₁ p(aⱼ, bⱼ, cⱼ | x))。学习物品表征

• 用户 user 对路径 path = [a, b, c] 的兴趣记作：
    p(path | user) = p(a, b, c | x).

• 物品 item 与路径 path 的相关性：
    score(item, path) = Σᵤₛₑᵣ p(path | user) × click(user, item).
                        |      用户对路径的兴趣        | |是否点击 (0或1)|学习物品表征

• 用户 user 对路径 path = [a, b, c] 的兴趣记作：
    p(path | user) = p(a, b, c | x).

• 物品 item 与路径 path 的相关性：
    score(item, path) = Σᵤₛₑᵣ p(path | user) × click(user, item).

• 根据 score(item, path) 选出 J 条路径作为 item 的表征。表征：物品 → 路径

用户点击物品：                     用户对路径的兴趣：
click(user, item)                  p(path | user)

1 ←────── [用户图标] ──────→ 0.1
1 ←────── [用户图标] ──────→ 0.5    ──→ [1]
1 ←────── [用户图标] ──────→ 0.8         [2]
1 ←────── [用户图标] ──────→ 0.2         [4]

Item                                    Path学习物品表征

• 选出 J 条路径 Π = {path₁, ⋯, pathⱼ}，作为物品的表征。

• 损失函数（选择与 item 高度相关的 path）：
    loss(item, Π) = -log(Σⱼⱼ₌₁ score(item, pathⱼ)).

• 正则项（避免过多的 item 集中在一条 path 上）：
    reg(pathⱼ) = (number of items on pathⱼ)⁴.学习物品表征

用贪心算法更新路径

• 假设已经把物品表征为 J 条路径 Π = {path₁, ⋯, pathⱼ}。

• 每次固定 {pathᵢ}ᵢ≠ₗ，并从未被选中的路径中，选出一条作为新的 pathₗ：
    pathₗ ← argminₚₐₜₕⱼ loss(item, Π) + α · reg(pathⱼ).

• 选中的路径有较高的分数 score(item, pathⱼ)，而且路径上的物品数量不会太多。训练

更新神经网络                    更新物品的表征

• 神经网络判断用户对路径的兴趣：
    p(path | x).

• 训练所需的数据：（1）"物品 → 路径"的索引，（2）用户点击过的物品。

• 如果用户点击过物品，且物品对应路径 path，则更新神经网络参数使 p(path | x) 变大。训练

更新神经网络                    更新物品的表征

• 神经网络判断用户对路径的兴趣：     • 判断物品与路径的相关性：
    p(path | x).                      物品 ←── 用户 ──→ 路径
                                      |用户点击过物品| |神经网络的打分|
• 训练所需的数据：（1）"物品 → 
  路径"的索引，（2）用户点击过      • 让每个物品关联 J 条路径。
  的物品。
                                    • 物品和路径要有很高的相关性。
• 如果用户点击过物品，且物品对
  应路径 path，则更新神经网络参      • 一条路径上不能有过多的物品。
  数使 p(path | x) 变大。总结Deep Retrieval

召回：用户 → 路径 → 物品

• 给定用户特征 x，用神经网络预估用户对路径 path = [a, b, c] 的兴趣，分数记作 p(path | x).

• 用 beam search 寻找分数 p(path | x) 最高的 s 条 path。Deep Retrieval

召回：用户 → 路径 → 物品

• 给定用户特征 x，用神经网络预估用户对路径 path = [a, b, c] 的兴趣，分数记作 p(path | x).

• 用 beam search 寻找分数 p(path | x) 最高的 s 条 path。

• 利用索引"path → List(item)"召回每条路径上的 n 个物品。

• 一共召回 s×n 个物品，对物品做初步排序，返回分数最高的若干物品。Deep Retrieval

训练：同时学习 用户——路径 和 物品——路径 的关系

• 一个物品被表征为 J 条路径：path₁, ⋯, pathⱼ。

• 如果用户点击过物品，则更新神经网络参数，使分数增大：
    ∑ʲⱼ₌₁ p(pathⱼ | x).Deep Retrieval

训练：同时学习 用户——路径 和 物品——路径 的关系

• 一个物品被表征为 J 条路径：path₁, ⋯, pathⱼ。

• 如果用户点击过物品，则更新神经网络参数，使分数增大：
    ∑ʲⱼ₌₁ p(pathⱼ | x).

• 如果用户对路径的兴趣分数 p(path | x) 较高，且用户点击过物品 item，则 item 与 path 具有相关性。

• 寻找与 item 最相关的 J 条 path，且避免一条路径上物品过多。Thank You!

http://wangshusen.github.io/其他召回通道

王树森

ShusenWang@xiaohongshu.com

http://wangshusen.github.io/地理位置召回GeoHash召回

• 用户可能对附近发生的事感兴趣。

• GeoHash：对经纬度的编码，地图上一个长方形区域。

• 索引：GeoHash → 优质笔记列表（按时间倒排）。

• 这条召回通道没有个性化。GeoHash召回

GeoHash：      最新发布的k篇优质笔记ID：

6G6VVF ───────────→ [video icons in different colors] ...

6FH4MC ───────────→ [video icons in different colors] ...GeoHash召回

GeoHash：      最新发布的k篇优质笔记ID：

6G6VVF ─────────→ [video icons in different colors] ...

6FH4MC ─────────→ [video icons in different colors] ...

⋮

根据用户定位的GeoHash，取回该地点最新的k篇优质笔记。同城召回

• 用户可能对同城发生的事感兴趣。

• 索引：城市 → 优质笔记列表（按时间倒排）。

• 这条召回通道没有个性化。作者召回关注作者召回

• 用户对关注的作者发布的笔记感兴趣。

• 索引：

  用户 → 关注的作者
  
  作者 → 发布的笔记

• 召回：

  用户 → 关注的作者 → 最新的笔记有交互的作者召回

• 如果用户对某笔记感兴趣（点赞、收藏、转发），
  那么用户可能对该作者的其他笔记感兴趣。

• 索引：用户 → 有交互的作者

• 召回：用户 → 有交互的作者 → 最新的笔记相似作者召回

• 如果用户喜欢某作者，那么用户喜欢相似的作者。

• 索引：作者 → 相似作者（k个作者）

• 召回：用户 → 感兴趣的作者 → 相似作者 → 最新的笔记
       (n个作者)      (nk个作者)    (nk篇笔记)缓存召回缓存召回

想法：复用前n次推荐精排的结果。

• 背景：
  • 精排输出几百篇笔记，送入重排。
  • 重排做多样性抽样，选出几十篇。
  • 精排结果一大半没有曝光，被浪费。

• 精排前50，但是没有曝光的，缓存起来，
  作为一条召回通道。缓存召回

缓存大小固定，需要退场机制。

• 一旦笔记成功曝光，就从缓存退场。

• 如果超出缓存大小，就移除最先进入缓存
  的笔记。

• 笔记最多被召回10次，达到10次就退场。

• 每篇笔记最多保存3天，达到3天就退场。总结

• 地理位置召回：
  • GeoHash召回、同城召回。

• 作者召回：
  • 关注的作者、有交互的作者、相似的作者。

• 缓存召回。Thank You!

http://wangshusen.github.io/长期招聘优秀的算法工程师

• 部门：小红书社区技术部。

• 方向：搜索、推荐。

• 职位：校招、社招、实习。

• 地点：上海、北京。

• 联系方式：ShusenWang@xiaohongshu.com曝光过滤 & Bloom Filter

王树森
ShusenWang@xiaohongshu.com

http://wangshusen.github.io/

小红书 (Red logo in bottom right corner)曝光过滤问题

• 如果用户看过某个物品，则不再把该物品曝光给该用户。

• 对于每个用户，记录已经曝光给他的物品。（小红书只召回1个月以内的笔记，因此只需要记录每个用户最近1个月的曝光历史。）

• 对于每个召回的物品，判断它是否已经给该用户曝光过，排除掉曾经曝光过的物品。

• 一位用户看过n个物品，本次召回r个物品，如果暴力对比，需要O(nr)的时间。Bloom Filter

• Bloom filter判断一个物品ID是否在已曝光的物品集合中。

• 如果判断为no，那么该物品一定不在集合中。

• 如果判断为yes，那么该物品很可能在集合中。（可能误伤，错误判断未曝光物品为已曝光，将其过滤掉。）

参考文献：

• Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 1970.Bloom Filter

• Bloom filter把物品集合表征为一个m维二进制向量。

• 每个用户有一个曝光物品的集合，表征为一个向量，需要m bit的存储。

• Bloom filter有k个哈希函数，每个哈希函数把物品ID映射成介于0和m-1之间的整数。

参考文献：

• Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 1970.Bloom Filter (k = 1)

二进制向量： [0|0|0|0|0|0|0|0|0|0|...|0]
              └─────────────────┘
                    m bitsBloom Filter (k = 1)

已曝光物品：    ID₁              ID₂
                 ↓                ↓
二进制向量：[0|1|0|0|0|0|0|0|1|0|...|0]
              └─────────────────┘
                    m bitsBloom Filter (k = 1)

已曝光物品：    ID₁    ID₃    ID₄         ID₂
                 ↓      ↓      ↓           ↓
二进制向量：[0|1|0|0|0|1|0|0|1|0|...|0]
              └─────────────────┘
                    m bitsBloom Filter (k = 1)

已曝光物品：    ID₁    ID₃    ID₄ ID₅ ID₆    ID₂
                 ↓      ↓      ↓   ↓   ↓      ↓
二进制向量：[0|1|0|0|0|1|0|0|1|0|...|0]
              └─────────────────┘
                    m bitsBloom Filter (k = 1)

已曝光物品：    ID₁    ID₃    ID₄ ID₅ ID₆    ID₂
                 ↓      ↓      ↓   ↓   ↓      ↓
二进制向量：[0|1|0|0|0|1|0|0|1|0|...|0]
                        ↑
召回的物品：             ID₇
                      未曝光Bloom Filter (k = 1)

已曝光物品：    ID₁    ID₃    ID₄ ID₅ ID₆    ID₂
                 ↓      ↓      ↓   ↓   ↓      ↓
二进制向量：[0|1|0|0|0|1|0|0|1|0|...|0]
                        ↑            ↑
召回的物品：             ID₇         ID₅
                      未曝光        已曝光Bloom Filter (k = 1)

已曝光物品：    ID₁    ID₃    ID₄ ID₅ ID₆    ID₂
                 ↓      ↓      ↓   ↓   ↓      ↓
二进制向量：[0|1|0|0|0|1|0|0|1|0|...|0]
                        ↑            ↑        ↑
                                被误判为已曝光
召回的物品：             ID₇         ID₅      ID₈
                      未曝光        已曝光    未曝光Bloom Filter (k = 3)

二进制向量：[0|0|0|0|0|0|0|0|0|0|...|0]Bloom Filter (k = 3)

已曝光物品：          ID₁
                      ↓ ↓ ↓
                    h₁ h₂ h₃
二进制向量：[0|1|0|1|0|1|0|0|0|0|...|0]Bloom Filter (k = 3)

已曝光物品：          ID₁                    ID₂
                      ↓ ↓ ↓                   ↓ ↓ ↓
                    h₁ h₂ h₃               h₂ h₁ h₃
二进制向量：[0|1|0|1|0|1|0|0|1|0|...|1]Bloom Filter (k = 3)

已曝光物品：          ID₁                    ID₂
                      ↓ ↓ ↓                   ↓ ↓ ↓
                    h₁ h₂ h₃               h₂ h₁ h₃
二进制向量：[0|1|0|1|0|1|0|0|1|0|...|1]
                      ↑     ↑     ↑
召回的物品：          ID₈           ID₄
                    未曝光        已曝光Bloom Filter (k = 3)

已曝光物品：          ID₁                    ID₂
                      ↓ ↓ ↓                   ↓ ↓ ↓
                    h₁ h₂ h₃               h₂ h₁ h₃
二进制向量：[0|1|0|1|0|1|0|0|1|0|...|1]
                      ↑     ↑     ↑              ↑
                                            被误判为已曝光
召回的物品：          ID₈           ID₄         ID₉
                    未曝光        已曝光      未曝光Bloom Filter

• 曝光物品集合大小为n，二进制向量维度为m，使用k个哈希函数。

• Bloom filter误伤的概率为δ ≈ (1 - exp(-kn/m))ᵏ。

• n越大，向量中的1越多，误伤概率越大。（未曝光物品的k个位置恰好都是1的概率大。）

• m越大，向量越长，越不容易发生哈希碰撞。

• k太大、太小都不好，k有最优取值。Bloom Filter

• 曝光物品集合大小为n，二进制向量维度为m，使用k个哈希函数。

• Bloom filter误伤的概率为δ ≈ (1 - exp(-kn/m))ᵏ。

• 设定可容忍的误伤概率为δBloom Filter

• 曝光物品集合大小为n，二进制向量维度为m，使用k个哈希函数。

• Bloom filter误伤的概率为δ ≈ (1 - exp(-kn/m))ᵏ。

• 设定可容忍的误伤概率为δ，那么最优参数为：

  k = 1.44 · ln(1/δ)，    m = 2n · ln(1/δ)。曝光过滤的链路

[召回] → [排序] → [物品1, 物品2, ⋮, 物品q]
  ↑                                    ↓
  ← [曝光过滤服务(Bloom Filter)] ← [实时流处理(Kafka+Flink)]曝光过滤的链路

[召回] → [排序] → [物品1, 物品2, ⋮, 物品q] (高亮圈出右侧部分)
  ↑                                    ↓
  ← [曝光过滤服务(Bloom Filter)] ← [实时流处理(Kafka+Flink)]曝光过滤的链路

[召回] → [排序] → [物品1, 物品2, ⋮, 物品q]
  ↑                                    ↓
  ← [曝光过滤服务(Bloom Filter)] ← [实时流处理(Kafka+Flink)] (高亮框出实时流处理部分)
二进制向量 ↑曝光过滤的链路

[召回] → [排序] → [物品1, 物品2, ⋮, 物品q]
  ↑                                    ↓
  ← [曝光过滤服务(Bloom Filter)] ← [实时流处理(Kafka+Flink)]
二进制向量 ↑ (高亮框出左侧二进制向量和曝光过滤服务部分)Bloom Filter的缺点

• Bloom filter把物品的集合表示成一个二进制向量。

• 每往集合中添加一个物品，只需要把向量k个位置的元素置为1。（如果原本就是1，则不变。）

• Bloom filter只支持添加物品，不支持删除物品。从集合中移除物品，无法消除它对向量的影响。

• 每天都需要从物品集合中移除年龄大于1个月的物品。（超龄物品不可能被召回，没必要把它们记录在Bloom filter，降低n可以降低误伤率。）Thank You!

http://wangshusen.github.io/# 多目标排序模型

**主讲人：王树森**

**网站链接：** http://wangshusen.github.io/

**小红书品牌标识** (右下角)# 推荐系统的链路

**系统架构流程图：**

1. **笔记池** (数据库) → **召回** (检索模块)
2. **笔记池** (数据库) → **召回** (检索模块)  
3. **笔记池** (数据库) → **召回** (检索模块)

**中心处理流程：**
- **召回** → **几千物品** → **粗排、精排** → **几百物品** → **重排** → **最终输出**

**最终输出：**
- 笔记1
- 笔记2
- ⋮ (省略号)
- 笔记80

**说明：** 该图展示了推荐系统从多个笔记池通过召回、排序等步骤最终生成推荐列表的完整链路。# 用户—笔记的交互

**对于每篇笔记，系统记录：**

• **曝光次数** (number of impressions)

• **点击次数** (number of clicks)  

• **点赞次数** (number of likes)

• **收藏次数** (number of collects)

• **转发次数** (number of shares)# 用户—笔记的交互

**关键指标计算公式：**

**→** • **点击率** = 点击次数 / 曝光次数

**→** • **点赞率** = 点赞次数 / 点击次数

**→** • **收藏率** = 收藏次数 / 点击次数

**→** • **转发率** = 转发次数 / 点击次数

**说明：** 红色箭头突出显示了这些重要的用户行为转化率指标。# 排序的依据

• **排序模型预估点击率、点赞率、收藏率、转发率等多种分数。**

• **融合这些预估分数。**（比如加权和。）

• **根据融合的分数做排序、截断。**# 多目标模型# 多目标模型 - 特征输入

**特征分类：**

**蓝色特征块：** 用户特征
**红色特征块：** 物品特征  
**绿色特征块：** 统计特征
**橙色特征块：** 场景特征

**说明：** 该图展示了多目标排序模型的四种主要特征类型，每种特征用不同颜色的方块表示。# 多目标模型 - 网络结构

**模型架构：**

**特征输入层：**
- **蓝色特征块：** 用户特征
- **红色特征块：** 物品特征  
- **绿色特征块：** 统计特征
- **橙色特征块：** 场景特征

**↓ concatenation (特征拼接)**

**神经网络** (灰色处理模块)

**↓**

**紫色输出块：** 最终预测结果

**说明：** 该图展示了多目标模型的完整网络结构，从特征输入到神经网络处理再到最终输出的完整流程。# 多目标模型 - 完整架构

**模型完整架构：**

**特征输入层：**
- **蓝色特征块：** 用户特征
- **红色特征块：** 物品特征  
- **绿色特征块：** 统计特征
- **橙色特征块：** 场景特征

**↓ concatenation (特征拼接)**

**神经网络** (灰色处理模块)

**↓**

**紫色共享表示层** 

**↓**

**多个输出头：**
- **点击率** → 全连接层+Sigmoid
- **点赞率** → 全连接层+Sigmoid  
- **收藏率** → 全连接层+Sigmoid
- **转发率** → 全连接层+Sigmoid

**说明：** 该图展示了多目标模型从特征输入到多个预测目标输出的完整网络架构。# 多目标模型 - 预估结果

**预估：**

**点击率** → p₁
**点赞率** → p₂  
**收藏率** → p₃
**转发率** → p₄

**说明：** 该图显示了多目标模型针对四个不同目标的预估输出结果，每个目标对应一个概率值。# 多目标模型 - 预估与目标对比

**预估：**
**点击率** → p₁
**点赞率** → p₂  
**收藏率** → p₃
**转发率** → p₄

**目标：**
**y₁** → **1** [有点击]
**y₂** → **0** [无点赞]
**y₃** → **0** [无收藏]  
**y₄** → **1** [有转发]

**说明：** 该图对比了模型的预估概率值与实际观测到的二元标签（1表示发生，0表示未发生）。# 多目标模型 - 训练过程

**预估：**
**点击率** → p₁
**点赞率** → p₂  
**收藏率** → p₃
**转发率** → p₄

**目标：**
**y₁, y₂, y₃, y₄**

**损失函数示例：**
CrossEntropy(y₁, p₁) = -(y₁ · ln p₁ + (1 - y₁) · ln(1 - p₁))

**训练：**

• **总的损失函数：** Σ⁴ᵢ₌₁ αᵢ · CrossEntropy(yᵢ, pᵢ)

• **对损失函数求梯度，做梯度下降更新参数。**

**说明：** 该图展示了多目标模型的训练过程，使用加权交叉熵损失函数来优化多个目标。# 训练

**困难：类别不平衡。**

• **每100次曝光，约有10次点击、90次无点击。**

• **每100次点击，约有10次收藏、90次无收藏。**

**解决方案：负样本降采样（down-sampling）。**

• **保留一小部分负样本。**

• **让正负样本数量平衡，节约计算。**

---

**注：不是小红书的真实数据**# 预估值校准# 预估值校准

• **正样本、负样本数量为 n₊ 和 n₋。**

• **对负样本做降采样，抛弃一部分负样本。**

• **使用 α · n₋ 个负样本，α ∈ (0,1) 是采样率。**

• **由于负样本变少，预估点击率大于真实点击率。** (红色标注)# 预估值校准

**公式对比：**

• **真实点击率：** pₜᵣᵤₑ = n₊/(n₊ + n₋) （期望）
  [蓝色下划线标注]

• **预估点击率：** pₚᵣₑd = n₊/(n₊ + α·n₋) （期望）
  [蓝色下划线标注]

**说明：** 该图展示了负样本降采样前后点击率计算公式的对比，其中α是采样率参数。# 预估值校准

**公式对比：**

• **真实点击率：** pₜᵣᵤₑ = n₊/(n₊ + n₋) （期望）

• **预估点击率：** pₚᵣₑd = n₊/(n₊ + α·n₋) （期望）

• **由上面两个等式可得校准公式[1]：**

**pₜᵣᵤₑ = (α·pₚᵣₑd)/((1-pₚᵣₑd)+α·pₚᵣₑd)** 
[蓝色下划线标注]

**参考文献：**

1. Xinran He et al. Practical lessons from predicting clicks on ads at Facebook. In *the 8th International Workshop on Data Mining for Online Advertising*.# Thank You!

**网站链接：** http://wangshusen.github.io/# Multi-gate Mixture-of-Experts (MMoE)

**主讲人：王树森**

**网站链接：** http://wangshusen.github.io/

**小红书品牌标识** (右下角)# MMoE 模型架构

**特征输入层：**
- **蓝色特征块：** 用户特征
- **红色特征块：** 物品特征  
- **绿色特征块：** 统计特征
- **橙色特征块：** 场景特征

**↑**

**多个专家神经网络：**
- **第1号神经网络** → **x₁** (紫色输出)
- **第2号神经网络** → **x₂** (紫色输出)
- **第3号神经网络** → **x₃** (紫色输出)

**说明：** 该图展示了MMoE模型的基本架构，包含多个并行的专家神经网络处理相同的特征输入。# MMoE 模型架构 - 专家网络

**特征输入层：**
- **蓝色特征块：** 用户特征
- **红色特征块：** 物品特征  
- **绿色特征块：** 统计特征
- **橙色特征块：** 场景特征

**↑**

**多个专家神经网络（高亮显示）：**
- **第1号神经网络** → **x₁** (紫色输出)
- **第2号神经网络** → **x₂** (紫色输出)
- **第3号神经网络** → **x₃** (紫色输出)

**三个"专家"** (右侧粉色标注)

**说明：** 该图强调了三个并行的专家神经网络，每个专家处理相同的输入特征但学习不同的表示。# MMoE 模型架构 - 门控机制

**特征输入层：**
- **蓝色特征块：** 用户特征
- **红色特征块：** 物品特征  
- **绿色特征块：** 统计特征
- **橙色特征块：** 场景特征

**↑**

**多个专家神经网络：**
- **第1号神经网络** → **x₁** (紫色输出)
- **第2号神经网络** → **x₂** (紫色输出)
- **第3号神经网络** → **x₃** (紫色输出)

**门控网络（左侧橙色部分）：**
- **神经网络**
- **↓**
- **Softmax激活函数**
- **↓**
- **[p₁|p₂|p₃]** (权重输出)

**说明：** 该图展示了门控网络的作用，通过神经网络和Softmax函数生成专家网络的权重分配。Multi-Task Learning Architecture for Recommendation Systems

This slide shows a multi-task learning architecture with the following components:

**Input Features (Bottom):**
- 用户特征 (User Features) - Blue sections
- 物品特征 (Item Features) - Red sections  
- 统计特征 (Statistical Features) - Green sections
- 场景特征 (Context Features) - Orange sections

**Neural Network Structure:**
- Input features feed into multiple parallel neural networks
- 第1号神经网络 (Neural Network #1) → outputs x₁ (purple vector)
- 第2号神经网络 (Neural Network #2) → outputs x₂ (purple vector)  
- 第3号神经网络 (Neural Network #3) → outputs x₃ (purple vector)

**Output Layers:**
- Left side: 神经网络 (Neural Network) → Softmax激活函数 (Softmax Activation) → outputs [p₁, p₂, p₃]
- Right side: 神经网络 (Neural Network) → Softmax激活函数 (Softmax Activation) → outputs [q₁, q₂, q₃]

**Architecture Pattern:**
- Multiple task-specific neural networks process different feature combinations
- Each network produces intermediate representations (x₁, x₂, x₃)
- Two separate output heads with softmax activation functions
- This enables joint learning of multiple related prediction tasks
- Shared lower-level representations with task-specific upper layersMulti-Task Learning - Weight Assignment

This slide shows the weight assignment phase of the multi-task learning architecture:

**Structure:**
- Three intermediate representations: x₁, x₂, x₃ (purple vectors)
- Left output: [p₁, p₂, p₃] with label 权重 (Weights)
- Right output: [q₁, q₂, q₃] with label 权重 (Weights)

**Key Components:**
- The intermediate representations (x₁, x₂, x₃) are generated by the neural networks from the previous layer
- These representations are used to compute weights for both output tasks
- The weights (p₁, p₂, p₃) and (q₁, q₂, q₃) determine the importance of each representation for their respective tasks

**Architecture Significance:**
- This demonstrates how multi-task learning assigns different importance weights to shared representations
- Each task (left and right outputs) can focus on different aspects of the learned features
- The weight assignment allows for task-specific attention to different parts of the shared representationMulti-Task Learning - Weighted Combination

This slide illustrates the weighted combination process in multi-task learning:

**Mathematical Formula:**
p₁x₁ + p₂x₂ + p₃x₃

**Process Description:**
- 对向量做加权平均 (Weighted average of vectors)
- The three intermediate representations (x₁, x₂, x₃) are combined using their respective weights (p₁, p₂, p₃)

**Architecture Flow:**
1. Input weights: [p₁, p₂, p₃] labeled as 权重 (Weights)
2. Intermediate representations: x₁, x₂, x₃ (purple vectors)
3. Weighted combination: Creates final representation (orange vector)
4. Output weights: [q₁, q₂, q₃] labeled as 权重 (Weights)

**Key Concept:**
- This demonstrates how multi-task learning creates task-specific representations through weighted averaging
- The weights determine the contribution of each intermediate representation to the final output
- Different tasks can have different weight distributions, allowing specialization while sharing underlying representationsMulti-Task Learning - Weighted Average Process

This slide shows the detailed weighted averaging mechanism:

**Mathematical Operation:**
p₁x₁ + p₂x₂ + p₃x₃

**Process Flow:**
1. **Input Stage:**
   - Weights: [p₁, p₂, p₃] (labeled 权重)
   - Intermediate representations: x₁, x₂, x₃ (purple vectors)

2. **Weighted Averaging:**
   - 对向量做加权平均 (Perform weighted average on vectors)
   - Each vector xᵢ is multiplied by its corresponding weight pᵢ
   - Results are summed to create the final representation (orange vector)

3. **Output Stage:**
   - Final output: [q₁, q₂, q₃] (labeled 权重)

**Technical Details:**
- This is a key mechanism in multi-task learning for combining multiple learned representations
- The weighted average allows the model to adaptively focus on different aspects of the input
- Different tasks can learn different weight distributions for the same set of representations
- The orange vector represents the task-specific combined representationMulti-Task Learning - Dual Task Architecture

This slide shows the complete dual-task architecture:

**Left Task (Orange Path):**
- Mathematical formula: p₁x₁ + p₂x₂ + p₃x₃
- Weighted combination creates orange vector representation
- Processed by 神经网络 (Neural Network) in orange
- Output: 点击率 (Click-Through Rate, CTR)

**Right Task (Green Path):**
- Mathematical formula: q₁x₁ + q₂x₂ + q₃x₃  
- Weighted combination creates green vector representation
- Processed by 神经网络 (Neural Network) in green
- Output: 点赞率 (Like Rate)

**Shared Components:**
- Common intermediate representations: x₁, x₂, x₃ (purple vectors)
- 对向量做加权平均 (Weighted averaging of vectors) for both tasks
- Input weights: [p₁, p₂, p₃] (权重) and [q₁, q₂, q₃] (权重)

**Architecture Benefits:**
- Two different tasks share the same underlying representations
- Each task learns its own weighting scheme for the shared features
- Enables transfer learning between related prediction tasks (CTR and Like Rate)
- Efficient parameter sharing while maintaining task-specific specializationMulti-Task Learning - Complete Architecture with Outputs

This slide presents the full multi-task learning pipeline with final outputs:

**Left Task - CTR Prediction:**
- Formula: p₁x₁ + p₂x₂ + p₃x₃
- Orange weighted combination vector
- Orange 神经网络 (Neural Network)
- Output: 点击率 (Click-Through Rate)

**Right Task - Like Rate Prediction:**
- Formula: q₁x₁ + q₂x₂ + q₃x₃
- Green weighted combination vector
- Green 神经网络 (Neural Network)
- Output: 点赞率 (Like Rate)

**Shared Foundation:**
- Base representations: x₁, x₂, x₃ (purple vectors)
- 对向量做加权平均 (Weighted vector averaging) for both branches
- Weight vectors: [p₁, p₂, p₃] and [q₁, q₂, q₃] (both labeled 权重)

**Multi-Task Learning Advantages:**
1. **Shared Representations:** Both tasks benefit from common learned features
2. **Task-Specific Weights:** Each task optimizes its own attention mechanism
3. **Joint Training:** Models learn complementary signals from both objectives
4. **Efficiency:** Shared parameters reduce model size while improving performance
5. **Transfer Learning:** Knowledge from one task helps improve the otherPolarization Phenomenon

**Title:** 极化现象 (Polarization)

**Key Concept:**
This slide introduces the concept of polarization in recommendation systems, which is a critical issue that can arise in ranking and recommendation algorithms.

**Polarization in Recommendation Systems:**
- **Definition:** The tendency of recommendation systems to amplify extreme preferences or create echo chambers
- **Manifestation:** Users may receive increasingly similar or extreme recommendations based on their past behavior
- **Impact:** Can lead to reduced content diversity and reinforced biases

**Relevance to Multi-Task Learning:**
- In the context of the previous multi-task architecture (CTR and Like Rate prediction)
- Polarization can affect both tasks differently
- Multi-task learning may help mitigate polarization by considering multiple objectives simultaneously

**Why This Matters:**
- Recommendation systems need to balance engagement metrics with content diversity
- Understanding polarization is crucial for building fair and diverse recommendation systems
- Proper handling of polarization ensures better user experience and platform healthPolarization in Multi-Task Learning Architecture

**Definition:**
极化(polarize): Softmax输出值一个接近1，其余接近0。
(Polarization: Softmax output values where one approaches 1, others approach 0.)

**Architecture Visualization:**
- **Input Features:** 用户特征 (User Features), 物品特征 (Item Features), 统计特征 (Statistical Features), 场景特征 (Context Features)
- **Neural Networks:** 第1号神经网络, 第2号神经网络, 第3号神经网络
- **Intermediate Representations:** x₁, x₂, x₃ (purple vectors)
- **Output Layers:** Two softmax layers producing [p₁, p₂, p₃] and [q₁, q₂, q₃]

**Polarization Problem:**
- The blue boxes highlight the softmax outputs that are prone to polarization
- When polarization occurs, one probability approaches 1 while others approach 0
- This creates extreme attention weights in the multi-task learning framework

**Impact on Multi-Task Learning:**
- Polarized weights can cause the model to focus exclusively on one representation
- Reduces the benefit of multi-task learning by ignoring shared information
- Can lead to unstable training and poor generalization
- Both tasks (left and right branches) are susceptible to this issuePolarization Example - Left Task Focus

**Definition:**
极化(polarize): Softmax输出值一个接近1，其余接近0。
(Polarization: Softmax output values where one approaches 1, others approach 0.)

**Example Demonstration:**
- **Left Task (Orange Branch):** Shows polarized weights [0, 0, 1]
  - Only the third representation (x₃) receives attention (weight = 1)
  - The first two representations (x₁, x₂) are ignored (weight = 0)
  - Highlighted by blue box around x₃ pathway

- **Right Task (Green Branch):** Shows balanced weights [0, 1, 0]
  - Only the second representation (x₂) receives attention (weight = 1)
  - The other representations are ignored

**Polarization Effects:**
1. **Loss of Information:** The model ignores potentially useful representations
2. **Reduced Robustness:** Over-reliance on a single feature pathway
3. **Poor Generalization:** Model becomes too specialized on specific patterns
4. **Training Instability:** Extreme weights can cause gradient issues

**Multi-Task Context:**
- Both tasks are experiencing polarization but focusing on different representations
- This defeats the purpose of shared learning in multi-task architecture
- The model fails to leverage the full potential of all learned representationsPolarization Example - Right Task Focus

**Definition:**
极化(polarize): Softmax输出值一个接近1，其余接近0。
(Polarization: Softmax output values where one approaches 1, others approach 0.)

**Example Demonstration:**
- **Left Task (Orange Branch):** Maintains polarized weights [0, 0, 1]
  - Still focusing only on x₃ representation

- **Right Task (Green Branch):** Shows polarized weights [0, 1, 0] 
  - Only the second representation (x₂) receives full attention (weight = 1)
  - The first and third representations (x₁, x₃) are completely ignored (weight = 0)
  - Highlighted by blue box around the x₂ pathway and output

**Key Observations:**
1. **Different Polarization Patterns:** Each task polarizes to different representations
2. **Complete Specialization:** Each task uses only one out of three available representations
3. **Loss of Shared Learning:** The tasks don't benefit from each other's learned features
4. **Wasted Capacity:** Two-thirds of the learned representations are unused by each task

**Problems This Creates:**
- **Brittleness:** Model performance depends entirely on single representations
- **Limited Transfer Learning:** Tasks cannot share useful information
- **Inefficient Architecture:** Multi-task setup provides no advantage over separate models
- **Potential Overfitting:** Over-specialization on specific feature patternsPolarization and Dead Neurons

**Definition:**
极化(polarize): Softmax输出值一个接近1，其余接近0。
(Polarization: Softmax output values where one approaches 1, others approach 0.)

**Dead Neuron Problem:**
- **Left Task:** Shows weights [0, 0, 1] - first neural network marked as "dead"
- **Right Task:** Shows weights [0, 1, 0] - continues to use only x₂

**Visual Indicators:**
- **Red Box:** Highlights the "dead" neural network (第1号神经网络)
- **Blue Box:** Shows the active pathways (x₂ and x₃)
- The word "dead" explicitly marks the unused neural network pathway

**Consequences of Dead Neurons:**
1. **Wasted Computational Resources:** Neural networks that receive zero attention become unused
2. **Reduced Model Capacity:** Effective model size decreases as neurons become inactive
3. **Training Inefficiency:** Gradients stop flowing through dead pathways
4. **Loss of Diversity:** Model loses the ability to learn different feature representations

**Multi-Task Impact:**
- Different tasks may kill different neural pathways
- This creates an unbalanced architecture where some parts are overused while others are ignored
- The shared learning benefit of multi-task architecture is severely compromised
- Model becomes equivalent to multiple smaller, specialized single-task modelsSolutions to Polarization Problem

**Title:** 解决极化问题 (Solving Polarization Problems)

**Key Solutions:**

1. **Multi-Expert Architecture:**
   - 如果有 n 个“专家”，那么每个 softmax 的输入和输出都是 n 维向量。
   (If there are n "experts", then each softmax input and output are n-dimensional vectors.)

2. **Dropout Regularization:**
   - 在训练时，对 softmax 的输出使用 dropout。
   (During training, apply dropout to softmax outputs.)

3. **Specific Dropout Configuration:**
   - Softmax 输出的 n 个数值被 mask 的概率都是 10%。
   (The probability of masking each of the n values in softmax output is 10%.)
   
   - 每个“专家”被随机丢弃的概率都是 10%。
   (Each "expert" has a 10% probability of being randomly dropped.)

**Why These Solutions Work:**
- **Prevents Over-specialization:** Dropout forces the model to use multiple experts
- **Maintains Diversity:** Random dropping ensures all pathways remain active
- **Improves Robustness:** Model cannot rely on single experts during training
- **Regularization Effect:** Reduces overfitting and improves generalizationReferences

**Title:** 参考文献 (References)

**Key Research Papers:**

1. **Google's MMoE Model:**
   - Google 的论文 [1] 提出 MMoE 模型。
   (Google's paper [1] proposes the MMoE model.)

2. **YouTube's Polarization Solution:**
   - YouTube 的论文 [2] 提出极化问题的解决方案。
   (YouTube's paper [2] proposes solutions to the polarization problem.)

**Full References:**

1. Jiaqi Ma et al. **Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts.** In *KDD*, 2018.

2. Zhe Zhao et al. **Recommending What Video to Watch Next: A Multitask Ranking System.** In *RecSys*, 2019.

**Significance:**
- The first paper introduces the foundational MMoE (Multi-gate Mixture-of-Experts) architecture
- The second paper addresses practical implementation challenges, particularly the polarization problem
- These works represent key contributions to multi-task learning in recommendation systems
- Both papers provide solutions that are widely used in industry applicationsThank You!

**Conclusion Slide**

This slide marks the end of the presentation on multi-task learning and polarization in recommendation systems.

**Contact Information:**
http://wangshusen.github.io/

**Summary of Topics Covered:**
1. Multi-task learning architecture for recommendation systems
2. Mixture of experts approach with shared representations
3. Weighted averaging mechanisms for task-specific attention
4. Polarization phenomenon in softmax attention weights
5. Dead neuron problems and their impact on model performance
6. Solutions including dropout regularization for multi-expert systems
7. Key research contributions from Google (MMoE) and YouTube

**Key Takeaways:**
- Multi-task learning can improve recommendation systems by sharing representations
- Polarization is a critical problem that can reduce the effectiveness of multi-task architectures
- Proper regularization techniques like dropout can mitigate polarization issues
- Industry applications have successfully implemented these techniques at scaleFusion of Predictive Scores

**Title:** 预估分数的融合 (Fusion of Predictive Scores)

**Presenter:** 王树森 (Wang Shusen)

**Contact Information:** http://wangshusen.github.io/

**Course Context:**
This appears to be the title slide for a lecture or presentation section on how to combine multiple predictive scores in recommendation systems. The topic follows the previous discussion on multi-task learning and polarization issues.

**Key Topic:**
Score fusion is a critical component in recommendation systems where multiple prediction models or scoring functions need to be combined to produce final rankings or recommendations.

**Relevance:**
- Builds upon multi-task learning concepts from previous slides
- Addresses practical implementation challenges in recommendation systems
- Focuses on techniques for combining different types of predictive scores effectivelyFusion of Predictive Scores - Simple Weighted Sum

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Method 1: Simple Weighted Sum**
简单的加权和 (Simple Weighted Sum)

**Formula:**
p_click + w₁ · p_like + w₂ · p_collect + ...

**Components:**
- **p_click:** Click-through rate probability (highlighted in blue)
- **p_like:** Like probability (highlighted in blue)
- **p_collect:** Collection/Save probability
- **w₁, w₂, ...:** Weight parameters for different actions
- **"...":** Additional user actions and their probabilities

**Characteristics:**
- **Simplicity:** Direct linear combination of different probability scores
- **Interpretability:** Easy to understand and explain
- **Baseline Method:** Often used as a starting point for more complex fusion approaches
- **Weight Optimization:** Weights can be learned through training or set based on business priorities

**Use Cases:**
- Multi-objective recommendation systems
- Combining engagement signals (clicks, likes, shares, etc.)
- Business-driven score combination where different actions have different valuesFusion with Click-Through Rate Weighting

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Method 1: Simple Weighted Sum**
简单的加权和 (Simple Weighted Sum)
p_click + w₁ · p_like + w₂ · p_collect + ...

**Method 2: CTR-Weighted Combination**
点击率乘以其他项的加权和 (Click-through rate multiplied by weighted sum of other items)

**Formula (highlighted in blue box):**
p_click · (1 + w₁ · p_like + w₂ · p_collect + ...)

**Key Differences:**
1. **Multiplicative Structure:** CTR acts as a gating mechanism
2. **Base Score:** Click probability serves as the foundation score
3. **Enhancement Factor:** Other actions provide multiplicative boosts
4. **Interaction Effects:** Non-linear combination creates interaction between different prediction scores

**Mathematical Interpretation:**
- If p_click is low, the overall score remains low regardless of other factors
- High p_click allows other positive signals (likes, collections) to amplify the final score
- The "(1 + ...)" structure ensures the base CTR is never diminished

**Practical Benefits:**
- **Business Logic:** Aligns with the intuition that items must first be clicked to generate other engagements
- **Filtering Effect:** Naturally downweights items with very low click probability
- **Amplification:** Allows high-engagement items to rise in rankingsScore Fusion with Probability Interpretations

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Method 1: Simple Weighted Sum**
简单的加权和 (Simple Weighted Sum)
p_click + w₁ · p_like + w₂ · p_collect + ...

**Method 2: CTR-Weighted Combination**
点击率乘以其他项的加权和 (CTR multiplied by weighted sum of other items)
p_click · (1 + w₁ · p_like + w₂ · p_collect + ...)

**Probabilistic Interpretations:**

**For p_click (highlighted in blue box):**
= #点击 / #曝光
(= #clicks / #exposures)

**For p_like (highlighted in blue box):**
= #点赞 / #点击
(= #likes / #clicks)

**Statistical Foundation:**
- **p_click:** Traditional click-through rate - probability of click given exposure
- **p_like:** Conditional probability - probability of like given that user clicked
- Both metrics represent different stages in the user engagement funnel

**Funnel Interpretation:**
1. **Exposure → Click:** p_click captures the initial attraction
2. **Click → Like:** p_like captures the satisfaction after interaction
3. **Sequential Nature:** Like can only happen after click, establishing natural hierarchy

**Benefits of This Interpretation:**
- **Clear Business Meaning:** Each probability has distinct interpretation in user journey
- **Data Collection:** Aligns with how metrics are typically collected and computed
- **Optimization:** Different probabilities can be optimized using appropriate models and featuresInternational Short Video App Scoring Formula

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Case Study:** 海外某短视频APP的融分公式
(Scoring formula from an international short video app)

**Advanced Fusion Formula:**
(1 + w₁ · p_time)^α₁ · (1 + w₂ · p_like)^α₂ ...

**Components (highlighted in blue):**
- **(1 + w₁ · p_time)^α₁:** Watch time component with exponential weighting
- **(1 + w₂ · p_like)^α₂:** Like probability component with exponential weighting
- **"...":** Additional engagement factors

**Key Features:**
1. **Multiplicative Structure:** All components multiply together
2. **Exponential Weighting:** Each component raised to power αᵢ
3. **Non-linear Interactions:** Creates complex interactions between different signals
4. **Flexible Tuning:** Both wᵢ (linear weights) and αᵢ (exponential weights) are tunable

**Mathematical Properties:**
- **Base Value:** Each term starts with 1, ensuring no component can make the score negative
- **Amplification:** High values in any component can significantly boost the overall score
- **Compound Effects:** Multiple high-performing components create multiplicative benefits
- **Sensitivity Control:** α parameters control how much each component influences the final score

**Practical Benefits:**
- **Rich Interaction Modeling:** Captures complex relationships between engagement signals
- **Industry-Proven:** Based on real-world implementation from successful short video platform
- **Flexible Optimization:** Multiple parameters allow fine-tuning for different business objectivesChinese Short Video App Scoring Formula

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Case Study:** 国内某短视频APP的融分公式
(Scoring formula from a domestic Chinese short video app)

**Key Points:**

1. **Ranking Strategy:**
   - 根据预估时长 p_time，对 n 篇候选视频做排序。
   (Based on predicted watch time p_time, rank n candidate videos.)

2. **Ranking-Based Scoring:**
   - 如果某视频排名第 r_time，则它得分 1/(r_time^α + β)。
   (If a video ranks at position r_time, it gets score 1/(r_time^α + β).)

3. **Multi-Signal Processing:**
   - 对点击、点赞、转发、评论等预估分数做类似处理。
   (Apply similar processing to predicted scores for clicks, likes, shares, comments, etc.)

4. **Final Fusion Score:**
   - 最终融合分数：
   (Final fusion score:)
   
   **Formula (with blue highlighting):**
   w₁/(r_time^α₁ + β₁) + w₂/(r_click^α₂ + β₂) + w₃/(r_like^α₃ + β₃) + ...

**Unique Approach:**
- **Rank-Based Scoring:** Uses ranking positions instead of raw probability values
- **Inverse Relationship:** Higher rank (lower number) gives higher score
- **Diminishing Returns:** Power function creates non-linear relationship with rank
- **Regularization:** β parameters prevent division by zero and control score range
- **Multi-Objective:** Combines rankings from different prediction tasksE-commerce Scoring Formula

**Title:** 融合预估分数 (Fusion of Predictive Scores)

**Case Study:** 某电商的融分公式
(Scoring formula from an e-commerce platform)

**E-commerce Conversion Funnel:**
电商的转化流程：(E-commerce conversion process:)

**曝光 → 点击 → 加购物车 → 付款**
(Exposure → Click → Add to Cart → Payment)

**Model Predictions:**
模型预估：(Model predictions:) p_click、p_cart、p_pay。

**Final Fusion Score:**
最终融合分数：(Final fusion score:)

**Formula:**
p_click^α₁ × p_cart^α₂ × p_pay^α₃ × price^α₄

**Components:**
1. **p_click^α₁:** Click-through probability with exponential weighting
2. **p_cart^α₂:** Add-to-cart probability with exponential weighting
3. **p_pay^α₃:** Payment probability with exponential weighting
4. **price^α₄:** Product price with exponential weighting

**Key Characteristics:**
- **Sequential Process:** Follows natural e-commerce user journey
- **Revenue Optimization:** Incorporates price as a direct business metric
- **Multiplicative Structure:** All components must be positive for high overall score
- **Exponential Control:** α parameters allow fine-tuning importance of each stage

**Business Logic:**
- **Funnel-Based:** Respects the sequential nature of e-commerce conversions
- **Revenue-Focused:** Price component directly optimizes for business value
- **Risk Management:** Low probability at any stage significantly reduces final score
- **Balanced Optimization:** Can balance between engagement and revenue objectivesThank You!

**Conclusion Slide**

This slide marks the end of the presentation on fusion of predictive scores in recommendation systems.

**Contact Information:**
http://wangshusen.github.io/

**Summary of Topics Covered:**
1. **Simple Weighted Sum:** Basic linear combination of multiple prediction scores
2. **CTR-Weighted Fusion:** Using click-through rate as a gating mechanism
3. **Probabilistic Interpretations:** Understanding scores as conditional probabilities in user funnel
4. **International Short Video App:** Advanced multiplicative formula with exponential weighting
5. **Chinese Short Video App:** Rank-based scoring approach with inverse ranking relationships
6. **E-commerce Platform:** Revenue-optimized scoring incorporating price and conversion funnel

**Key Takeaways:**
- **Multiple Approaches:** Different industries and applications require different fusion strategies
- **Business Alignment:** Scoring formulas should reflect the specific business objectives and user journey
- **Non-linear Combinations:** Advanced techniques use multiplicative and exponential relationships
- **Industry Experience:** Real-world examples demonstrate practical implementation considerations
- **Flexibility:** Multiple parameters allow fine-tuning for specific performance goals

**Practical Implications:**
- Score fusion is critical for multi-objective optimization in recommendation systems
- The choice of fusion method should align with business logic and user behavior patterns
- Complex formulas require careful parameter tuning and validation视频播放建模

王树森

http://wangshusen.github.io/

[小红书 logo in bottom right corner]视频播放时长图文 vs 视频

• 图文笔记排序的主要依据：
    点击、点赞、收藏、转发、评论……

• 视频排序的依据还有播放时长和完播。

• 直接用回归拟合播放时长效果不好。建议用 YouTube 的时长建模 [1]。


参考文献：

1. Paul Covington, Jay Adams, & Emre Sargin. Deep Neural Networks for YouTube Recommendations. In RecSys, 2016.Neural Network Architecture Diagram:

Multiple fully connected layers (全连接层) shown as grey rectangles with upward arrows and "..." indicating continuation. These layers feed into a central "神经网络" (Neural Network) component.

The output "z" (shown in red) connects upward to the neural network.

At the bottom, there's a feature input bar showing different feature types:
- 用户特征 (User Features) - shown in blue
- 视频特征 (Video Features) - shown in red  
- 统计特征 (Statistical Features) - shown in brown/orange
- 场景特征 (Context Features) - shown in yellow-green

Arrows indicate the flow from features upward through the fully connected layers to the neural network.Neural Network Architecture with Mathematical Formulations:

Same neural network diagram as previous page, but with added mathematical components:

y = t/(1+t) (shown with blue dot)

CE(y, p) = y · log p + (1 - y) · log(1 - p) (Cross-entropy loss function)

p = exp(z)/(1+exp(z)) (Sigmoid function, shown with green dot)

The diagram shows:
- Multiple fully connected layers (全连接层) feeding into neural network (神经网络)
- Output z (red dot) 
- Feature types at bottom: 用户特征 (User Features), 视频特征 (Video Features), 统计特征 (Statistical Features), 场景特征 (Context Features)
- Mathematical relationships between t (watch time), y (normalized watch time), z (network output), and p (predicted probability)Neural Network Architecture with Mathematical Relationship:

Same neural network diagram as previous pages, with highlighted mathematical relationship:

y = t/(1+t) (shown with blue dot)

如果 p = y，那么 exp(z) = t。(If p = y, then exp(z) = t.)

p = exp(z)/(1+exp(z)) (Sigmoid function, shown with green dot)

The diagram shows:
- Multiple fully connected layers (全连接层) feeding into neural network (神经网络)  
- Output z (red dot)
- Feature types at bottom: 用户特征 (User Features), 视频特征 (Video Features), 统计特征 (Statistical Features), 场景特征 (Context Features)

Key mathematical insight: When prediction p equals the normalized watch time y, then exp(z) = t (actual watch time).Neural Network Usage Framework:

Same neural network architecture diagram with two usage scenarios highlighted:

用作推理，          用作训练
预估时长 t          (For inference,     For training
                    predict duration t)

exp(z) ← (blue dot)     p = exp(z)/(1+exp(z)) ← (green dot)

The diagram shows:
- Multiple fully connected layers (全连接层) feeding into neural network (神经网络)
- Output z (red dot) 
- Feature types at bottom: 用户特征 (User Features), 视频特征 (Video Features), 统计特征 (Statistical Features), 场景特征 (Context Features)

Two modes of operation:
1. For inference: Use exp(z) to predict watch time t
2. For training: Use sigmoid function p = exp(z)/(1+exp(z))视频播放时长建模

• 把最后一个全连接层的输出记作 z。设 p = sigmoid(z)。

• 实际观测的播放时长记作 t。（如果没有点击，则 t = 0。）

• 做训练：最小化交叉熵损失
    -(t/(1+t) · log p + 1/(1+t) · log(1-p)).视频播放时长建模

• 把最后一个全连接层的输出记作 z。设 p = sigmoid(z)。

• 实际观测的播放时长记作 t。（如果没有点击，则 t = 0。）

• 做训练：最小化交叉熵损失
    -(t/(1+t) · log p + 1/(1+t) · log(1-p)).
    [Note: The fractions 1+t are highlighted with red underlines]

• 做推理：把 exp(z) 作为播放时长的预估。

• 把 exp(z) 作为融分公式中的一项。视频完播视频完播

回归方法

• 例：视频长度10分钟，实际播放4分钟，则实际播放率为 y = 0.4。

• 让预估播放率 p 拟合 y：
    loss = y · log p + (1 - y) · log(1 - p).

• 线上预估完播率，模型输出 p = 0.73，意思是预计播放 73%。视频完播

二元分类方法

• 定义完播指标，比如完播80%。

• 例：视频长度10分钟，播放>8分钟作为正样本，播放<8分钟作为负样本。

• 做二元分类训练模型：播放>80% vs 播放<80%。

• 线上预估完播率，模型输出 p = 0.73，意思是
    P(播放 > 80%) = 0.73.视频完播

不能直接把预估的完播率用到融分公式 (why?)

Graph showing completion rate (完播率) on y-axis (0 to 0.6) versus video duration in seconds (视频时长(秒)) on x-axis. 

The graph shows a declining curve with blue data points and a red fitted line labeled "用函数 f(视频时长) 拟合完播率". The completion rate starts around 0.5 for short videos and decreases to around 0.05 for longer videos, showing an inverse relationship between video length and completion rate.视频完播

• 线上预估完播率，然后做调整：
    p_finish = 预估完播率 / f(视频长度)

• 把 p_finish 作为融分公式中的一项。Thank You!

http://wangshusen.github.io/排序模型的特征

王树森

http://wangshusen.github.io/

[小红书 logo in bottom right corner]用户画像 (User Profile)

• 用户 ID（在召回、排序中做 embedding）。

• 人口统计学属性：性别、年龄。

• 账号信息：新老、活跃度……

• 感兴趣的类目、关键词、品牌。物品画像 (Item Profile)

• 物品 ID（在召回、排序中做 embedding）。

• 发布时间（或者年龄）。

• GeoHash（经纬度编码）、所在城市。

• 标题、类目、关键词、品牌……

• 字数、图片数、视频清晰度、标签数……

• 内容信息量、图片美学……用户统计特征

• 用户最近30天（7天、1天、1小时）的曝光数、点击数、点赞数、收藏数……

• 按照笔记图文/视频分桶。（比如最近7天，该用户对图文笔记的点击率、对视频笔记的点击率。）

• 按照笔记类目分桶。（比如最近30天，用户对美妆笔记的点击率、对美食笔记的点击率、对科技数码笔记的点击率。）笔记统计特征

• 笔记最近30天（7天、1天、1小时）的曝光数、点击数、点赞数、收藏数……

• 按照用户性别分桶、按照用户年龄分桶……

• 作者特征：
  • 发布笔记数
  • 粉丝数
  • 消费指标（曝光数、点击数、点赞数、收藏数）场景特征 (Context)

• 用户定位 GeoHash（经纬度编码）、城市。

• 当前时刻（分段，做 embedding）。

• 是否是周末、是否是节假日。

• 手机品牌、手机型号、操作系统。特征处理

• 离散特征：做 embedding。
  • 用户ID、笔记ID、作者ID。
  • 类目、关键词、城市、手机品牌。

• 连续特征：做分桶，变成离散特征。
  • 年龄、笔记字数、视频长度。

• 连续特征：其他变换。
  • 曝光数、点击数、点赞数等数值做 log(1 + x)。
  • 转化为点击率、点赞率等值，并做平滑。小结

1. 用户画像特征。

2. 笔记画像特征。

3. 用户统计特征。

4. 笔记统计特征。

5. 场景特征。特征覆盖率

• 很多特征无法覆盖 100% 样本。

• 例：很多用户不填年龄，因此用户年龄特征的覆盖率远小于 100%。

• 例：很多用户设置隐私权限，APP 不能获得用户地理定位，因此场景特征有缺失。

• 提高特征覆盖率，可以让精排模型更准。数据服务

1. 用户画像 (User Profile)。

2. 物品画像 (Item Profile)。

3. 统计数据。数据服务

Architecture diagram showing:

用户请求 → 主服务器 → 排序服务器
                ↑              ↓
                ↓        物品ID、用户ID、场景特征
            召回服务器

Three database cylinders connected to the ranking server:
- 用户画像 (User Profile) - labeled as 较为静态 (relatively static)
- 物品画像 (Item Profile) - labeled as 静态 (static)  
- 统计数据 (Statistical Data) - labeled as 动态 (dynamic)数据服务

Architecture diagram showing the complete data flow:

用户请求 → 主服务器 → 排序服务器 → TF Serving
                ↑              ↓           
                ↓        物品ID、用户ID、场景特征    特征打包
            召回服务器

Three database cylinders providing features to the ranking server:
- 用户特征 from 用户画像 (User Profile) - 较为静态 (relatively static)
- 物品特征 from 物品画像 (Item Profile) - 静态 (static)
- 统计特征 from 统计数据 (Statistical Data) - 动态 (dynamic)数据服务

Complete system architecture diagram:

用户请求 → 主服务器 → 排序服务器 → 特征打包 → TF Serving
                ↑              ↑           
                ↓              ↓      
            召回服务器     物品ID、用户ID、场景特征

Three data sources feeding the ranking server:
- 用户特征 from 用户画像 database (较为静态 - relatively static)
- 物品特征 from 物品画像 database (静态 - static)
- 统计特征 from 统计数据 database (动态 - dynamic)

The ranking server packages features and sends them to TF Serving for model inference.Thank You!

http://wangshusen.github.io/粗排

王树森

http://wangshusen.github.io/

[小红书 logo in bottom right corner]粗排 vs 精排

粗排                           精排

• 给几千篇笔记打分。          • 给几百篇笔记打分。

• 单次推理代价必须小。        • 单次推理代价很大。

• 预估的准确性不高。          • 预估的准确性更高。精排模型 & 双塔模型精排模型

Multi-task learning architecture with shared bottom:

Four output heads:
- 点击率 (Click-through Rate)
- 点赞率 (Like Rate) 
- 收藏率 (Save Rate)
- 转发率 (Share Rate)

Each head has: 全连接层+Sigmoid

Shared neural network backbone labeled as:
神经网络 (shared bottom)

Input features (concatenated):
- 用户特征 (User Features) - shown in blue
- 物品特征 (Item Features) - shown in red  
- 统计特征 (Statistical Features) - shown in green
- 场景特征 (Context Features) - shown in orange

The features are concatenated and fed into the shared neural network, which then branches out to the four task-specific heads.精排模型

• 前期融合：先对所有特征做 concatenation，再输入神经网络。

• 线上推理代价大：如果有 n 篇候选笔记，整个大模型要做 n 次推理。双塔模型

Two-tower architecture diagram:

Left tower (用户塔 - User Tower):
- Input: 用户特征 (User Features) - blue features
- Output: vector a (blue)

Right tower (物品塔 - Item Tower):  
- Input: 物品特征 (Item Features) - red features
- Output: vector b (red) - 存储在数据库 (stored in database)

Online inference (线上推理):
- Cosine similarity calculation: 余弦相似度: cos(a, b)
- The purple dot represents the similarity score computation between vectors a and b双塔模型

• 后期融合：把用户、物品特征分别输入不同的神经网络，不对用户、物品特征做融合。

• 线上计算量小：

  • 用户塔只需要做一次线上推理，计算用户表征 a。

  • 物品表征 b 事先储存在向量数据库中，物品塔在线上不做推理。

• 预估准确性不如精排模型。粗排的三塔模型

参考文献：

• Zhe Wang et al. COLD: Towards the Next Generation of Pre-Ranking System. In DLP-KDD, 2020.Three-tower Architecture:

Left tower - 用户塔 (User Tower, very large):
- Input: 用户特征 场景特征 (User Features + Context Features) - blue and orange features
- Output: blue vector

Middle tower - 物品塔 (Item Tower, relatively large):
- Input: 物品特征 (静态) (Item Features - static) - red features
- Output: red vector

Right tower - 交叉塔 (Cross Tower, relatively small):
- Input: 统计特征、交叉特征 (Statistical Features, Cross Features) - green features  
- Output: green vector

The three towers operate independently and process different types of features with different computational complexities.Complete Three-tower Architecture with Multi-task Learning:

Top layer - Four prediction heads:
- 点击率 (Click-through Rate)
- 点赞率 (Like Rate)  
- 收藏率 (Save Rate)
- 转发率 (Share Rate)

Each head: 全连接层+Sigmoid

Middle layer: Concatenation & Cross - combines outputs from all three towers

Three towers:
1. 用户塔 (User Tower, very large):
   - Input: 用户特征 场景特征 (User + Context Features)
   - Output: blue vector

2. 物品塔 (Item Tower, relatively large):
   - Input: 物品特征 (静态) (Static Item Features)  
   - Output: red vector

3. 交叉塔 (Cross Tower, relatively small):
   - Input: 统计特征、交叉特征 (Statistical + Cross Features)
   - Output: green vector

The architecture combines the efficiency of tower-based feature processing with multi-task learning for comprehensive prediction.粗排的三塔模型

• 只有一个用户，用户塔只做一次推理。

• 即使用户塔很大，总计算量也不大。

Three towers architecture (same as previous diagrams):
- 用户塔 (User Tower, very large) - processes 用户特征 场景特征
- 物品塔 (Item Tower, relatively large) - processes 物品特征 (静态)
- 交叉塔 (Cross Tower, relatively small) - processes 统计特征、交叉特征粗排的三塔模型

• 只有一个用户，用户塔只做一次推理。

• 即使用户塔很大，总计算量也不大。

• 有 n 个物品，理论上物品塔需要做 n 次推理。

• PS 缓存物品塔的输出向量，避免绝大部分推理。

Three towers with caching indication:
- 用户塔 (User Tower, very large) - processes 用户特征 场景特征  
- 物品塔 (Item Tower, relatively large) - processes 物品特征 (静态), output marked as 缓存 (cached)
- 交叉塔 (Cross Tower, relatively small) - processes 统计特征、交叉特征粗排的三塔模型

• 只有一个用户，用户塔只做一次推理。

• 即使用户塔很大，总计算量也不大。

• 有 n 个物品，理论上物品塔需要做 n 次推理。

• PS 缓存物品塔的输出向量，避免绝大部分推理。

• 统计特征动态变化，缓存不可行。

• 有 n 个物品，交叉塔必须做 n 次推理。

Three towers architecture showing the computational requirements for each tower with n items.Complete Three-tower Architecture with Multi-task Learning and highlighted computation requirements:

Top layer (highlighted in orange) - Four prediction heads:
- 点击率 (Click-through Rate)
- 点赞率 (Like Rate)
- 收藏率 (Save Rate)  
- 转发率 (Share Rate)

Each head: 全连接层+Sigmoid

Middle layer: Concatenation & Cross - combines outputs from all three towers

Three towers:
1. 用户塔 (User Tower, very large):
   - Input: 用户特征 场景特征 (User + Context Features)
   - Output: blue vector

2. 物品塔 (Item Tower, relatively large):
   - Input: 物品特征 (静态) (Static Item Features)
   - Output: red vector

3. 交叉塔 (Cross Tower, relatively small):
   - Input: 统计特征、交叉特征 (Statistical + Cross Features)
   - Output: green vector

The top layer (multi-task heads) is highlighted to show where most computation occurs.Complete Three-tower Architecture Summary:

Same architecture as previous slide with key insights:

• 有 n 个物品，模型上层需要做 n 次推理。

• 粗排推理的大部分计算量在模型上层。

The diagram shows the full three-tower architecture with multi-task learning heads, emphasizing that the computational bottleneck is in the upper layers (Concatenation & Cross + multi-task heads) rather than in the tower feature processing.三塔模型的推理

• 从多个数据源取特征：

  • 1 个用户的画像、统计特征。

  • n 个物品的画像、统计特征。

• 用户塔：只做 1 次推理。

• 物品塔：未命中缓存时需要做推理。

• 交叉塔：必须做 n 次推理。

• 上层网络做 n 次推理，给 n 个物品打分。Thank You!

http://wangshusen.github.io/Factorized Machine (FM)

王树森

http://wangshusen.github.io/

[Small red logo in bottom right corner: 小红书]线性模型

• 有 d 个特征，记作 x = [x₁, ⋯, xd]。

• 线性模型：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ.
  [Linear model components highlighted in blue]

• 模型有 d + 1 个参数：w = [w₁, ⋯, wd] 和 b。

• 预测是特征的加权和。（只有加，没有乘。）二阶交叉特征

• 有 d 个特征，记作 x = [x₁, ⋯, xd]。

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.
  [Linear and interaction components highlighted in blue]

• 模型有 O(d²) 个参数。二阶交叉特征

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.二阶交叉特征

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.
  [Interaction term highlighted in blue]

[DIAGRAM: A square matrix visualization labeled as:
- "d 行" (d rows) on the left side
- "d 列" (d columns) on top
- "矩阵 U" (Matrix U) at the bottom
- The matrix appears as a green grid showing the interaction matrix structure]二阶交叉特征

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.
  [Interaction term highlighted in blue]

[DIAGRAM SHOWING MATRIX DECOMPOSITION:
- Left side: "矩阵 U" (Matrix U) - d行×d列 (d rows × d columns) green matrix
- Approximately equals (≈) symbol
- Middle: "矩阵 V" (Matrix V) - d行×k列 (d rows × k columns) blue matrix
- Multiplication dot (•)
- Right side: "矩阵 Vᵀ" (Matrix V transpose) - k行×d列 (k rows × d columns) blue matrix]二阶交叉特征

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.
  ≈ vᵢᵀ vⱼ [shown in highlighted box]

[DIAGRAM SHOWING MATRIX DECOMPOSITION WITH ELEMENT SELECTION:
- Left: "矩阵 U" (Matrix U) - green matrix with element "uᵢⱼ" highlighted
- Approximately equals (≈) symbol
- Middle: "矩阵 V" (Matrix V) - blue matrix with row "vᵢᵀ" highlighted in darker blue
- Multiplication dot (•)
- Right: "矩阵 Vᵀ" (Matrix V transpose) - blue matrix with column "vⱼ" highlighted in darker blue]二阶交叉特征

• 线性模型 + 二阶交叉特征：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ uᵢⱼxᵢxⱼ.
  ≈ vᵢᵀ vⱼ [shown in highlighted box]

• Factorized Machine (FM)：
  p = b + Σᵢ₌₁ᵈ wᵢxᵢ + Σᵢ₌₁ᵈ Σⱼ₌ᵢ₊₁ᵈ (vᵢᵀ vⱼ)xᵢxⱼ.
  [Interaction term highlighted in blue]

• FM 模型有 O(kd) 个参数。 (k ≪ d)Factorized Machine

• FM 是线性模型的替代品，能用线性回归、逻辑回归的场景，都可以用 FM。

• FM 使用二阶交叉特征，表达能力比线性模型更强。Factorized Machine

• FM 是线性模型的替代品，能用线性回归、逻辑回归的场景，都可以用 FM。

• FM 使用二阶交叉特征，表达能力比线性模型更强。

• 通过做近似 uᵢⱼ ≈ vᵢᵀ vⱼ，FM 把二阶交叉权重的数量从 O(d²) 降低到 O(kd)。
  [O(d²) and O(kd) highlighted in blue and red respectively]

参考文献：
• Steffen Rendle. Factorization machines. In ICDM, 2010.Thank You!

http://wangshusen.github.io/深度交叉网络 (DCN)

王树森

http://wangshusen.github.io/

[Small red logo in bottom right corner: 小红书]召回、排序模型双塔模型

[DIAGRAM: Two-tower model architecture showing:
- User features (用户特征) flowing into User Tower (用户塔) producing embedding 'a' (blue vector)
- Item features (物品特征) flowing into Item Tower (物品塔) producing embedding 'b' (red vector)  
- Both towers connected to cosine similarity computation: cos(a, b)
- Text note: "可以用任意网络结构" (Can use arbitrary network structure)][DIAGRAM: Multi-task neural network architecture showing:

Top level: Four output tasks
- 点击率 (Click Rate)
- 点赞率 (Like Rate) 
- 收藏率 (Favorite Rate)
- 转发率 (Share Rate)

Each connected to "全连接层+Sigmoid" (Fully Connected Layer + Sigmoid)

Middle: Shared embedding layer (purple vector)

Bottom: Shared neural network (shared bottom) with text "可以用任意网络结构" (Can use arbitrary network structure)

Input: Four feature types concatenated
- 用户特征 (User Features) - blue
- 物品特征 (Item Features) - red  
- 统计特征 (Statistical Features) - green
- 场景特征 (Context Features) - orange

Connected by "concatenation" arrows][DIAGRAM: Multi-expert neural network architecture showing:

Top level: Two prediction heads
- Left: [p₁|p₂|p₃] → Softmax activation → Neural Network
- Right: [q₁|q₂|q₃] → Softmax activation → Neural Network

Both computing weighted averages: 
- Left: p₁x₁ + p₂x₂ + p₃x₃
- Right: q₁x₁ + q₂x₂ + q₃x₃

Middle: Three expert networks (第1号神经网络, 第2号神经网络, 第3号神经网络)
Each producing outputs x₁, x₂, x₃ (purple vectors)

Note: "可以用任意网络结构" (Can use arbitrary network structure)

Bottom: Input features concatenated
- 用户特征 (User Features) - blue
- 物品特征 (Item Features) - red
- 统计特征 (Statistical Features) - green
- 场景特征 (Context Features) - orange]交叉层
(Cross Layer)交叉层 (Cross Layer)

[DIAGRAM: Simple flow showing:
x₀ (red vector) → ⋯ → xᵢ (purple vector)]交叉层 (Cross Layer)

[DIAGRAM: Cross layer architecture showing:
- Input: x₀ (red vector) flows through ⋯ to xᵢ (purple vector)
- xᵢ goes to "全连接层" (Fully Connected Layer)
- Output y (green vector)
- Hadamard Product operation (circle with ⊙)
- Final output z (blue vector)]交叉层 (Cross Layer)

[DIAGRAM: Cross layer with residual connection showing:
- Input: x₀ (red vector) flows through ⋯ to xᵢ (purple vector)
- xᵢ goes to "全连接层" (Fully Connected Layer)
- Output y (green vector)
- Hadamard Product operation
- z (blue vector)
- Residual connection with + operation
- Final output: xᵢ₊₁ (purple vector)]交叉层 (Cross Layer)

[DIAGRAM: Cross layer with input/output labels showing:
- Input side: x₀ labeled as "交叉层的输入" (Cross Layer Input)
- Processing: x₀ → ⋯ → xᵢ → 全连接层 → y
- Hadamard Product operation
- z (blue vector)
- Residual connection with + operation
- Output side: xᵢ₊₁ labeled as "交叉层的输出" (Cross Layer Output)]交叉层 (Cross Layer)

[MATHEMATICAL FORMULA showing cross layer computation:
xᵢ₊₁ = x₀ ∘ [W • xᵢ + b] + xᵢ

Where:
- xᵢ₊₁ (purple vector) = output
- x₀ (red vector) = input to cross layer  
- W (yellow matrix) = weight matrix
- xᵢ (purple vector) = current layer input
- b (yellow vector) = bias vector
- ∘ represents Hadamard product
- Final addition creates residual connection]

交叉层的输入交叉层 (Cross Layer)

[MATHEMATICAL FORMULA with highlighted fully connected layer:
xᵢ₊₁ = x₀ ∘ [W • xᵢ + b] + xᵢ

The fully connected layer portion [W • xᵢ + b] is highlighted in gray box labeled "全连接层" (Fully Connected Layer)]交叉层 (Cross Layer)

[MATHEMATICAL FORMULA with Hadamard Product highlighted:
xᵢ₊₁ = x₀ ∘ [W • xᵢ + b] + xᵢ

The x₀ input and operation are highlighted, with "Hadamard Product" label below]交叉层 (Cross Layer)

[MATHEMATICAL FORMULA with components highlighted:
xᵢ₊₁ = x₀ ∘ [W • xᵢ + b] + xᵢ

Various components are highlighted with gray boxes around different terms]交叉层 (Cross Layer)

[MATHEMATICAL FORMULA with output labeled:
xᵢ₊₁ = x₀ ∘ [W • xᵢ + b] + xᵢ

With "输出" (Output) labeled below the formula]Cross Network (交叉网络)

Title: Cross Network
Chinese: 交叉网络Cross Network (交叉网络)

Architecture diagram showing:
- Input: x₀ (red vector)
- Cross layer (交叉层) with parameters: W₀, b₀
- Output: x₁ (purple vector)

Mathematical formula:
x₁ = x₀ ∘ (W₀x₀ + b₀) + x₀

Where:
- ∘ represents element-wise product (Hadamard product)
- W₀, b₀ are the parameters of the cross layer
- x₀ is the input vector
- x₁ is the output of the first cross layerCross Network (交叉网络)

Extended architecture with two cross layers:
- Input: x₀ (red vector)
- First cross layer (交叉层) with parameters: W₀, b₀ → x₁
- Second cross layer (交叉层) with parameters: W₁, b₁ → x₂

Mathematical formula for second layer:
x₂ = x₀ ∘ (W₁x₁ + b₁) + x₁

The architecture shows the sequential application of cross layers, where each layer maintains a connection to the original input x₀ while also using the previous layer's output.Cross Network (交叉网络)

Full cross network architecture with three layers:
- Input: x₀ (red vector)
- First cross layer (交叉层) with parameters: W₀, b₀ → x₁
- Second cross layer (交叉层) with parameters: W₁, b₁ → x₂  
- Third cross layer (交叉层) with parameters: W₂, b₂ → x₃

The diagram illustrates the complete cross network structure where:
- Each cross layer takes the original input x₀ and the previous layer's output
- The network builds increasingly complex feature interactions layer by layer
- All layers maintain residual connections with both the original input and previous outputsReferences (参考文献)

Main content:
• 这节课介绍的是 Cross Network V2 [1]。
(This lesson introduces Cross Network V2 [1].)

• 老版本的 Cross Network 在论文 [2] 中提出。
(The older version of Cross Network was proposed in paper [2].)

参考文献: (References:)

1. Ruoxi Wang et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.

2. Ruoxi Wang et al. Deep & Cross Network for Ad Click Predictions. In ADKDD, 2017.Deep & Cross Network (深度交叉网络)

Architecture diagram showing the parallel structure:

Left side input features:
- 用户特征 (User Features) - blue section
- 物品特征 (Item Features) - red section  
- 其它特征 (Other Features) - green section

Two parallel paths:
1. 全连接网络 (Fully Connected Network) - upper path
2. 交叉网络 (Cross Network) - lower path

Both networks process the same input features in parallel to capture different types of feature interactions.Deep & Cross Network (深度交叉网络)

Complete DCN architecture showing:

Left side input features:
- 用户特征 (User Features) - blue section
- 物品特征 (Item Features) - red section  
- 其它特征 (Other Features) - green section

Two parallel processing paths:
1. 全连接网络 (Fully Connected Network) → yellow output vector
2. 交叉网络 (Cross Network) → purple output vector

Final combination:
- The outputs from both networks are concatenated
- Fed into a 全连接层 (Fully Connected Layer)
- Produces final brown output vector

This architecture combines the memorization ability of cross networks with the generalization ability of deep networks.Deep & Cross Network (深度交叉网络)

DCN architecture with highlighted components:

The diagram shows the complete Deep & Cross Network with a pink highlight box emphasizing the parallel processing structure.

Key components:
- Input features: 用户特征 (User Features), 物品特征 (Item Features), 其它特征 (Other Features)
- Parallel processing: 全连接网络 (Fully Connected Network) and 交叉网络 (Cross Network)
- Feature combination and final 全连接层 (Fully Connected Layer)
- Final output

Bottom text: 深度交叉网络 (Deep Cross Network)

This highlights the key innovation of DCN: combining explicit cross-feature learning with deep neural network learning in a parallel architecture.Thank You!

End slide with acknowledgment.

URL: http://wangshusen.github.io/LHUC Network Architecture (LHUC网络结构)

Title: LHUC网络结构 (LHUC Network Architecture)
Author: 王树森

URL: http://wangshusen.github.io/Multi-Task Learning Architecture

Architecture diagram showing:

Input features (bottom):
- 用户特征 (User Features) - blue section
- 物品特征 (Item Features) - red section  
- 统计特征 (Statistical Features) - green section
- 场景特征 (Context Features) - beige section

Features are concatenated and fed to:
- 神经网络 (Neural Network) - shared bottom

Output tasks (top):
- 点击率 (Click-Through Rate) - 全连接层+Sigmoid
- 点赞率 (Like Rate) - 全连接层+Sigmoid  
- 收藏率 (Favorite Rate) - 全连接层+Sigmoid
- 转发率 (Share Rate) - 全连接层+Sigmoid

This shows a multi-task learning setup where a shared neural network bottom processes all features and feeds into multiple task-specific heads.Learning Hidden Unit Contributions (LHUC)

Main Content:
• LHUC 起源于语音识别 [1]。
(LHUC originated from speech recognition [1].)

• 快手将 LHUC 应用在推荐精排 [2]，称作 PPNet。
(Kuaishou applied LHUC to recommendation ranking [2], called PPNet.)

参考文献: (References:)

1. Pawel Swietojanski, Jinyu Li, & Steve Renals. Learning hidden unit contributions for unsupervised acoustic model adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.

2. 快手落地万亿参数推荐精排模型，2021。链接：
https://ai.51cto.com/art/202102/644214.htmlLHUC in Speech Recognition (语音识别中的LHUC)

Visual representation showing two input types:

1. 语音信号 (Speech Signal) - red vector/matrix representing audio input

2. 说话者的特征 (Speaker Features) - blue vector representing speaker characteristics

This illustrates the original application of LHUC in speech recognition where the model needs to adapt to both the speech signal content and speaker-specific characteristics.LHUC in Speech Recognition (语音识别中的LHUC)

Architecture showing:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer)
- → red output vector

Lower path (Speaker Features):  
- 说话者的特征 (Speaker Features) - blue vector
- → 神经网络 (Neural Network)
- → blue output vector

Note at bottom:
[多个全连接层] → [Sigmoid 乘以2]
(Multiple fully connected layers → Sigmoid times 2)

This shows how LHUC processes both speech content and speaker-specific information through separate pathways.LHUC in Speech Recognition (语音识别中的LHUC)

Architecture with Hadamard Product:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer)  
- → red output vector

Lower path (Speaker Features):
- 说话者的特征 (Speaker Features) - blue vector
- → 神经网络 (Neural Network)
- → blue output vector

Interaction:
- Hadamard Product (element-wise multiplication) ∘
- The blue vector modulates the red vector through element-wise multiplication
- Final result: modulated red vector

This demonstrates how speaker features can selectively amplify or suppress different aspects of the speech signal processing.LHUC in Speech Recognition (语音识别中的LHUC)

Extended architecture with multiple layers:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer) → red vector
- → Hadamard Product ∘ (modulated by speaker features)
- → 全连接层 (Fully Connected Layer) → final red output

Lower path (Speaker Features):
- 说话者的特征 (Speaker Features) - blue vector  
- → 神经网络 (Neural Network) → blue vector
- → Additional pathway to second layer
- → 神经网络 (Neural Network) → final blue output

This shows how speaker features can influence multiple layers of speech processing, allowing for more sophisticated adaptation to different speakers throughout the network.LHUC in Speech Recognition (语音识别中的LHUC)

Complete LHUC architecture with multiple Hadamard Products:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer) → red vector
- → Hadamard Product ∘ (first modulation)
- → 全连接层 (Fully Connected Layer) → red vector  
- → Hadamard Product ∘ (second modulation)
- → final red output

Lower path (Speaker Features):
- 说话者的特征 (Speaker Features) - blue vector
- → 神经网络 (Neural Network) → blue vector (feeds first Hadamard Product)
- → Additional pathway
- → 神经网络 (Neural Network) → blue vector (feeds second Hadamard Product)

This demonstrates the full LHUC mechanism where speaker characteristics can modulate speech processing at multiple points in the network, enabling fine-grained speaker adaptation.LHUC in Speech Recognition (语音识别中的LHUC)

Complete LHUC architecture with output highlighted:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer) → red vector
- → Hadamard Product ∘ (first modulation)
- → 全连接层 (Fully Connected Layer) → red vector  
- → Hadamard Product ∘ (second modulation)
- → 输出 (Output) - final red output highlighted in pink box

Lower path (Speaker Features):
- 说话者的特征 (Speaker Features) - blue vector
- → 神经网络 (Neural Network) → blue vector (feeds first Hadamard Product)
- → Additional pathway
- → 神经网络 (Neural Network) → blue vector (feeds second Hadamard Product)

This shows the complete LHUC system producing the final output after multiple stages of speaker-adaptive modulation.LHUC in Speech Recognition (语音识别中的LHUC)

Generalized LHUC architecture overview:

Shows pink vectors representing multiple layers/stages with:
- 神经网络 (Neural Network) components at bottom left and right

The diagram illustrates the general concept with:
[多个全连接层] → [Sigmoid 乘以2]
(Multiple fully connected layers → Sigmoid times 2)

This represents the abstract architecture of LHUC where multiple neural network stages can be modulated through the learned gating mechanism.LHUC in Speech Recognition (语音识别中的LHUC)

Standard LHUC architecture without specific highlighting:

Upper path (Speech Signal):
- 语音信号 (Speech Signal) - red vector
- → 全连接层 (Fully Connected Layer) → red vector
- → Hadamard Product ∘ (first modulation)
- → 全连接层 (Fully Connected Layer) → red vector  
- → Hadamard Product ∘ (second modulation)
- → final red output

Lower path (Speaker Features):
- 说话者的特征 (Speaker Features) - blue vector
- → 神经网络 (Neural Network) → blue vector (feeds first Hadamard Product)
- → Additional pathway
- → 神经网络 (Neural Network) → blue vector (feeds second Hadamard Product)

This represents the standard LHUC architecture for speech recognition with multiple adaptation points.LHUC in Recommendation Ranking Models (推荐系统排序模型中的LHUC)

LHUC applied to recommendation systems:

Upper path (Item Features):
- 物品特征 (Item Features) - red vector
- → 全连接层 (Fully Connected Layer) → red vector
- → Hadamard Product ∘ (first modulation)
- → 全连接层 (Fully Connected Layer) → red vector  
- → Hadamard Product ∘ (second modulation)
- → final red output

Lower path (User Features):
- 用户特征 (User Features) - blue vector
- → 神经网络 (Neural Network) → blue vector (feeds first Hadamard Product)
- → Additional pathway
- → 神经网络 (Neural Network) → blue vector (feeds second Hadamard Product)

This shows how LHUC is adapted for recommendation systems, where user features modulate the processing of item features, enabling personalized item representation learning.Thank You!

End slide with acknowledgment.

URL: http://wangshusen.github.io/SENet & Bilinear Cross

Title: SENet & Bilinear Cross
Author: 王树森

URL: http://wangshusen.github.io/SENet

参考文献: (References:)

1. Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Networks. In CVPR, 2018.

2. Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction. In RecSys, 2019.SENet

Embedding layer showing feature transformation:

Input features (red dots):
- 用户 ID (User ID)
- 物品 ID (Item ID) 
- 物品类目 (Item Category)
- ⋮ (more features)
- 物品关键词 (Item Keywords)

Each input feature is transformed through embedding into blue vector representations:
- Multiple blue embedding vectors shown on the right
- Each embedding vector represents the learned dense representation of the corresponding categorical feature

This demonstrates the standard embedding process where categorical features are mapped to dense vector representations.SENet

Embedding output representation:

Shows the result of the embedding process:
- Multiple blue embedding vectors arranged vertically
- Each vector represents a dense embedding of a categorical feature
- Dots (⋮) indicate there are more embedding vectors
- All embeddings have the same dimensionality (shown as equal-width vectors)

This represents the concatenated or stacked embedding vectors that serve as input to subsequent neural network layers.SENet

Embedding vectors representation:

Shows the embedded feature vectors arranged as a matrix:
- m×k matrix where:
  - m = number of features/fields
  - k = embedding dimension
- Multiple blue vectors stacked vertically
- Each vector represents an embedding of a categorical feature
- All embeddings have the same dimension kSENet

SENet Architecture Pipeline:

1. Input: m×k embedding matrix (blue vectors)
2. AvgPool: Average pooling → m×1 vector (cyan)
3. FC+ReLU: Fully connected layer with ReLU activation → m/r×1 vector (dark blue)
   - r is the reduction ratio
4. FC+Sigmoid: Fully connected layer with Sigmoid activation → m×1 vector (orange)

This shows the squeeze-and-excitation mechanism that learns field-wise attention weights.SENet

Complete SENet with Excitation:

1. Input: m×k embedding matrix (blue vectors)
2. Squeeze path: AvgPool → FC+ReLU → FC+Sigmoid → m×1 attention weights (orange)
3. Excitation: Row-wise multiply the attention weights with original embeddings
4. Output: m×k re-weighted embedding matrix (pink vectors)

The curved arrow shows the excitation step where attention weights are applied to modulate the importance of each embedding vector.

Result: Field-aware feature representations where important fields are emphasized and less important ones are suppressed.SENet

SENet with Highlighted Important Field:

1. Input: m×k embedding matrix (blue vectors with one darker blue vector highlighted)
2. Squeeze-and-Excitation process: AvgPool → FC+ReLU → FC+Sigmoid
3. Output: m×k re-weighted embeddings (pink vectors with one darker pink vector)

The darker colored vectors (both input and output) represent a field that receives higher attention weight, demonstrating how SENet can learn to emphasize more important features while de-emphasizing less relevant ones.

This illustrates the adaptive field weighting capability of SENet.SENet

Variable Embedding Dimensions (Embedding 向量维度可以不同):

Input features with different embedding dimensions:
- 用户 ID (User ID) → longer blue embedding vector
- 物品 ID (Item ID) → medium blue embedding vector  
- 物品类目 (Item Category) → shorter blue embedding vector
- ⋮ (more features)
- 物品关键词 (Item Keywords) → medium blue embedding vector

After SENet processing:
- Corresponding pink vectors of the same dimensions
- Each field maintains its original embedding dimension
- SENet learns field-wise importance regardless of embedding size

Labels:
- m个离散特征 (m discrete features)
- m个向量 (m vectors) for both input and output

This demonstrates SENet's flexibility in handling features with different embedding dimensions.SENet

Field-wise Weighting Explanation:

• SENet 对离散特征做 field-wise 加权。
(SENet performs field-wise weighting on discrete features.)

• Field:
  • 用户 ID Embedding 是 64 维向量。
  (User ID Embedding is a 64-dimensional vector.)
  • 64 个元素算一个 field，获得相同的权重。
  (64 elements count as one field and get the same weight.)

• 如果有 m 个 fields，那么权重向量是 m 维。
(If there are m fields, then the weight vector is m-dimensional.)

This explains how SENet treats each embedding vector as a single field and assigns one weight to all dimensions within that field.Field Feature Interaction (Field 间特征交叉)

Title: Field 间特征交叉
(Inter-field Feature Interaction)Feature Interaction (特征交叉)

Comparison of two approaches:

内积 (Inner Product):
• f_{ij} = x_i^T · x_j
• Left side: purple scalar f_{ij} = blue vector x_i^T (transposed) · red vector x_j
• m fields → m² 个实数 (m² real numbers)

哈达玛乘积 (Hadamard Product):
• f_{ij} = x_i ∘ x_j  
• Right side: purple vector f_{ij} = blue vector x_i ∘ red vector x_j (element-wise multiplication)
• m fields → m² 个向量 (m² vectors)

This shows two different ways to compute feature interactions between embedding vectors.Feature Interaction (特征交叉)

Bilinear Cross (内积):

Formula: f_{ij} = x_i^T · W_{ij} · x_j

Components:
• x_i^T: blue vector (transposed)
• W_{ij}: gray weight matrix  
• x_j: red vector
• f_{ij}: purple scalar result

Output: m fields → m² 个交叉特征 (实数)
(m fields → m² cross features as real numbers)

This shows bilinear interaction where a learned weight matrix W_{ij} modulates the interaction between field i and field j.Feature Interaction (特征交叉)

Bilinear Cross (内积) with Highlighted Weight Matrix:

Formula: f_{ij} = x_i^T · W_{ij} · x_j

Components:
• x_i^T: blue vector (transposed)
• W_{ij}: gray weight matrix (highlighted with blue border)
• x_j: red vector  
• f_{ij}: purple scalar result

Output: m fields → m²/2 个参数矩阵
(m fields → m²/2 parameter matrices)

The weight matrix W_{ij} is emphasized, showing it's a learnable parameter that captures the interaction between fields i and j.Feature Interaction (特征交叉)

Bilinear Cross (哈达玛乘积):

Formula: f_{ij} = x_i ∘ [W_{ij} · x_j]

Components:
• x_i: blue vector
• W_{ij}: gray weight matrix
• x_j: red vector
• ∘: Hadamard product (element-wise multiplication)
• f_{ij}: purple vector result

The operation shows:
1. W_{ij} transforms x_j
2. Element-wise multiplication with x_i
3. Results in a vector output rather than scalar

This variant produces vector-valued interactions instead of scalar interactions.Feature Interaction (特征交叉)

Bilinear Cross (哈达玛乘积) with Highlighted Components:

Formula: f_{ij} = x_i ∘ [W_{ij} · x_j]

Components (with blue border highlighting):
• f_{ij}: purple vector result (highlighted)
• x_i: blue vector (highlighted)  
• W_{ij}: gray weight matrix
• x_j: red vector

Output: m fields → m² 个向量
(m fields → m² vectors)

The highlighting emphasizes the vector nature of both the input field x_i and the resulting interaction f_{ij}, showing how this method preserves the vector structure in the output.Summary (小结)

1. SENet 对离散特征做 field-wise 加权。
(SENet performs field-wise weighting on discrete features.)

2. Field 间特征交叉：
(Inter-field feature interactions:)

• 向量内积
(Vector inner product)

• 哈达玛乘积  
(Hadamard product)

• Bilinear cross

This summarizes the key concepts covered in the presentation about SENet and bilinear cross methods for cross-domain recommendation systems.FiBiNet

参考文献: (References:)

• Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction. In RecSys, 2019.

This slide introduces FiBiNet (Feature Importance and Bilinear feature Interaction Network), which combines the concepts of SENet for feature importance weighting and bilinear interactions for cross-feature learning.FiBiNet Architecture Overview

Input processing flow:

1. 离散特征 (Discrete Features) - red dots
   ↓
2. Embedding layer
   ↓  
3. Blue embedding vectors (multiple vectors shown)
   ↓
4. Two parallel paths:
   - Top path: Concatenate → Blue concatenated vector
   - Bottom path: Bilinear processing → Cyan interaction vectors

This shows the basic architecture where discrete features are embedded and then processed through both concatenation and bilinear interaction paths to capture different types of feature relationships.FiBiNet Architecture with SENet

Input processing flow:

1. 离散特征 (Discrete Features) - red dots
   ↓
2. Embedding layer  
   ↓
3. Blue embedding vectors
   ↓
4. Two parallel paths:
   - Top path: Concatenate → Blue concatenated vector
   - Bottom path with SENet: 
     * SENet processing on embeddings → Pink re-weighted vectors
     * Bilinear processing → Purple interaction vectors (highlighted in red box)

This enhanced architecture incorporates SENet to learn field importance weights before applying bilinear interactions, combining both feature importance and feature interaction learning.Complete FiBiNet Architecture

Full processing pipeline:

1. 离散特征 (Discrete Features) - red dots
   ↓
2. Embedding layer
   ↓
3. Multiple processing paths (highlighted in red box):
   - Original embeddings → Bilinear → Cyan vectors
   - SENet-weighted embeddings → Bilinear → Purple vectors
   ↓
4. Feature combination:
   - Top: Original concatenated embeddings → Blue vector
   - Middle: 连续特征 (Continuous Features) → Yellow vector
   ↓
5. Final concatenation → 上层网络 (Upper Network)

This complete architecture combines:
- Original embeddings and their bilinear interactions
- SENet-weighted embeddings and their bilinear interactions  
- Continuous features
- All features are concatenated and fed to upper layers for final predictionThank You!

End slide with acknowledgment.

URL: http://wangshusen.github.io/用户行为序列建模

王树森

http://wangshusen.github.io/Multi-Task Learning Architecture

[Architecture Diagram showing:]

Top layer (red boxes):
- 点击率 (Click Rate)
- 点赞率 (Like Rate) 
- 收藏率 (Favorite Rate)
- 转发率 (Share Rate)

Each connected to:
- 全连接层+Sigmoid (Fully Connected Layer + Sigmoid)

Middle layer (purple bar):
神经网络 (Neural Network)

Bottom layer (concatenation):
Four feature types:
- 用户特征 (User Features) - blue
- 物品特征 (Item Features) - red  
- 统计特征 (Statistical Features) - green
- 场景特征 (Context Features) - orange

Note: The 用户特征 (User Features) section is highlighted with a red box, indicating it's the focus.Item Embedding Architecture

[Diagram showing:]

向量: (Vector:)
- Six red rectangular vectors arranged horizontally
- Arrow pointing up labeled "平均" (Average)
- Final averaged vector at top

物品 ID: (Item ID:)
- Six video/media icons arranged horizontally
- Arrows pointing up to corresponding vectors
- Label "Embedding" on right side
- "..." indicating continuationLastN特征

• LastN：用户最近的 n 次交互（点击、点赞等）的物品 ID。

• 对 LastN 物品 ID 做 embedding，得到 n 个向量。

• 把 n 个向量取平均，作为用户的一种特征。

• 适用于召回双塔模型、粗排三塔模型、精排模型。


参考文献：

• Covington, Adams, and Sargin. Deep neural networks for YouTube recommendations. In ACM Conference on Recommender Systems, 2016.小红书的实践

[Diagram showing:]

Average pooling architecture for LastN features:
- Top: Single red vector labeled "平均" (Average)
- Middle: Two red vectors with "..." indicating more vectors
- Bottom: Two video icons labeled "Embedding"
- Label at bottom: "点击的LastN" (Clicked LastN)

The diagram illustrates how multiple item embeddings are averaged to create a user feature.小红书的实践

[Architecture diagram showing multiple LastN features:]

Three parallel branches:
1. 点击的LastN (Clicked LastN) - Red vectors and video icons
2. 点赞的LastN (Liked LastN) - Green vectors and video icons  
3. 收藏的LastN (Favorited LastN) - Blue vectors and video icons

Each branch shows:
- Top: Average pooling (平均)
- Middle: Multiple embeddings with "..." indicating continuation
- Bottom: Video/media icons representing user interactions
- "Embedding" labeled on the side

All three branches are enclosed in red boxes, indicating they are used together as features.Thank You!

http://wangshusen.github.io/DIN模型

王树森

http://wangshusen.github.io/Item Embedding Architecture

[Diagram showing:]

向量: (Vector:)
- Six red rectangular vectors arranged horizontally
- Arrow pointing up labeled "平均" (Average)
- Final averaged vector at top

物品 ID: (Item ID:)
- Six video/media icons arranged horizontally
- Arrows pointing up to corresponding vectors
- Label "Embedding" on right side
- "..." indicating continuationDIN模型

• DIN 用加权平均代替平均，即注意力机制（attention）。

• 权重：候选物品与用户 LastN 物品的相似度。


参考文献：

• Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.DIN Attention Mechanism

[Diagram showing:]

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

The diagram shows the relationship between user's LastN item vectors and the candidate item vector for computing attention weights.DIN Attention Mechanism - Step 1

[Diagram showing:]

相似度: (Similarity)
- Purple circle labeled α₁ with arrow pointing to it

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

Shows the computation of similarity α₁ between the first LastN vector x₁ and candidate item vector q.DIN Attention Mechanism - Step 2

[Diagram showing:]

相似度: (Similarity)
- Two purple circles labeled α₁, α₂ with arrows pointing to them

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

Shows the computation of similarities α₁ and α₂ between the first two LastN vectors and candidate item vector q.DIN Attention Mechanism - Step 3

[Diagram showing:]

相似度: (Similarity)
- Three purple circles labeled α₁, α₂, α₃ with arrows pointing to them

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

Shows the computation of similarities α₁, α₂, and α₃ between the first three LastN vectors and candidate item vector q.DIN Attention Mechanism - Complete

[Diagram showing:]

相似度: (Similarity)
- Four purple circles labeled α₁, α₂, α₃, ..., αₙ with arrows pointing to them

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

Shows the computation of all similarities α₁ through αₙ between all LastN vectors and the candidate item vector q.DIN Weighted Sum

[Diagram showing:]

Final weighted sum output:
- Purple vector at top labeled "加权和" (Weighted Sum)
- Arrows pointing up from similarity scores

相似度: (Similarity)
- Four gray boxes containing purple circles labeled α₁, α₂, α₃, ..., αₙ
- Each box shows multiplication operation (×)

LastN向量: (LastN Vectors)
- Four red vectors labeled x₁, x₂, x₃, ..., xₙ

候选物品向量: (Candidate Item Vector)
- One blue vector labeled q

Shows how the similarity scores are used as weights to compute the final weighted sum of LastN vectors.DIN模型

• 对于某候选物品，计算它与用户 LastN 物品的相似度。

• 以相似度为权重，求用户 LastN 物品向量的加权和，结果是一个向量。

• 把得到的向量作为一种用户特征，输入排序模型，预估（用户，候选物品）的点击率、点赞率等指标。

• 本质是注意力机制（attention）。DIN的本质是注意力机制

[Diagram showing attention mechanism:]

Left side (红色虚线框):
- Four red vectors labeled x₁, x₂, x₃, xₙ
- Text: "作为 key 和 value" (As key and value)

Right side (红色虚线框):
- One blue vector labeled q  
- Text: "作为 query" (As query)

This illustrates that DIN essentially implements an attention mechanism where LastN item vectors serve as both keys and values, while the candidate item vector serves as the query.DIN的本质是注意力机制

[Architecture diagram showing:]

Top: Purple vector (output from attention layer)
↑
Middle: "单头注意力层" (Single-head Attention Layer) - gray box

Input arrows pointing up from:

Left side (红色虚线框):
- Four red vectors labeled x₁, x₂, x₃, xₙ  
- Text: "作为 key 和 value" (As key and value)

Right side (红色虚线框):
- One blue vector labeled q
- Text: "作为 query" (As query)

This shows the complete attention mechanism architecture where the single-head attention layer processes the keys/values and query to produce the final output vector.DIN有效的原因

[2D scatter plot diagram showing different item categories:]

Vertical axis: Arrow pointing up
Horizontal axis: Arrow pointing right

Categories plotted:
- 美妆 (Beauty): Red triangles clustered in upper left quadrant
- 美食 (Food): Red circles clustered in upper right quadrant  
- 汽车 (Cars): Orange squares clustered in lower left quadrant

Special markers:
- 简单平均 (Simple Average): Purple star in center-left
- 候选物品（美食）(Candidate Item - Food): Blue star in upper right quadrant

The diagram illustrates why DIN is effective by showing how different item categories cluster in the embedding space, and how attention mechanism can focus on relevant items rather than using simple averaging.DIN有效的原因

[2D scatter plot diagram - same layout as previous slide]

Vertical axis: Arrow pointing up
Horizontal axis: Arrow pointing right

Categories plotted:
- 美妆 (Beauty): Light pink triangles (faded) in upper left quadrant
- 美食 (Food): Red circles in upper right quadrant  
- 汽车 (Cars): Light orange squares (faded) in lower left quadrant

Special markers:
- 加权平均 (Weighted Average): Purple star positioned closer to food category
- Text label: "作为用户特征" (As User Feature) in gray box

This slide shows how the weighted average (attention mechanism) produces a user feature vector that is positioned closer to the relevant category (food) compared to simple averaging, demonstrating the effectiveness of the DIN approach.DIN有效的原因

[2D scatter plot diagram - same layout]

Vertical axis: Arrow pointing up
Horizontal axis: Arrow pointing right

Categories plotted:
- 美妆 (Beauty): Red triangles in upper left quadrant
- 美食 (Food): Light pink circles (faded) in upper right quadrant  
- 汽车 (Cars): Orange squares in lower left quadrant

Special markers:
- 加权平均 (Weighted Average): Purple star positioned in center-left area
- 候选物品（新闻）(Candidate Item - News): Blue star in lower right quadrant

This slide demonstrates how the attention mechanism adapts the weighted average position based on different candidate items, showing the dynamic nature of DIN's attention weights.DIN有效的原因

[2D scatter plot diagram - same layout]

Vertical axis: Arrow pointing up  
Horizontal axis: Arrow pointing right

Categories plotted:
- 美妆 (Beauty): Light pink triangles (faded) in upper left quadrant
- 美食 (Food): Light pink circles (faded) in upper right quadrant
- 汽车 (Cars): Light orange squares (faded) in lower left quadrant

Special markers:
- 加权平均 (Weighted Average): Purple star positioned in center area
- 候选物品（新闻）(Candidate Item - News): Blue star in lower right quadrant

This slide shows the final state where the weighted average adapts to be more representative of the user's interests relative to the candidate item (news), demonstrating DIN's ability to create contextual user representations.DIN有效的原因

[2D scatter plot diagram - same layout]

Vertical axis: Arrow pointing up  
Horizontal axis: Arrow pointing right

Categories plotted:
- 美妆 (Beauty): Light pink triangles (faded) in upper left quadrant
- 美食 (Food): Light pink circles (faded) in upper right quadrant
- 汽车 (Cars): Light orange squares (faded) in lower left quadrant

Special markers:
- 加权平均 (Weighted Average): Purple star positioned in lower area
- 候选物品（新闻）(Candidate Item - News): Blue star in lower right quadrant

This slide continues to demonstrate how DIN's weighted averaging mechanism adapts to different candidate items, showing the contextual nature of user representation.简单平均 v.s. 注意力机制

• 简单平均和注意力机制都适用于精排模型。

• 简单平均适用于双塔模型、三塔模型。
  • 简单平均只需要用到 LastN，属于用户自身的特征。
  • 把 LastN 向量的平均作为用户塔的输入。

• 注意力机制不适用于双塔模型、三塔模型。
  • 注意力机制需要用到 LastN + 候选物品。
  • 用户塔看不到候选物品，不能把注意力机制用在用户塔。Thank You!

http://wangshusen.github.io/SIM模型

王树森

http://wangshusen.github.io/DIN模型

[Architecture diagram showing:]

Left side (红色虚线框):
- Four red vectors labeled x₁, x₂, x₃, xₙ
- Text: "用户LastN交互记录" (User LastN Interaction Records)

Right side (红色虚线框):
- One blue vector labeled q
- Text: "候选物品" (Candidate Item)

This illustrates the basic DIN model setup with user's LastN interaction records and candidate item.DIN模型

[Architecture diagram showing:]

Top: Purple vector (output from attention layer)
↑
Middle: "注意力层" (Attention Layer) - gray box

Input arrows pointing up from:

Left side (红色虚线框):
- Four red vectors labeled x₁, x₂, x₃, xₙ
- Text: "用户LastN交互记录" (User LastN Interaction Records)

Right side (红色虚线框):
- One blue vector labeled q
- Text: "候选物品" (Candidate Item)

This shows the complete DIN architecture with the attention layer processing user LastN records and candidate item to produce the final output vector.DIN模型

• 计算用户 LastN 向量的加权平均。

• 权重是候选物品与 LastN 物品的相似度。


参考文献：

• Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.DIN模型的缺点

• 注意力层的计算量 ∝ n（用户行为序列的长度）。

• 只能记录最近几百个物品，否则计算量太大。

• 缺点：关注短期兴趣，遗忘长期兴趣。


参考文献：

• Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.如何改进DIN?

• 目标：保留用户长期行为序列（n 很大），而且计算量不会过大。

• 改进 DIN：
  • DIN 对 LastN 向量做加权平均，权重是相似度。
  • 如果某 LastN 物品与候选物品差异很大，则权重接近零。
  • 快速排除掉与候选物品无关的 LastN 物品，降低注意力层的计算量。SIM模型

• 保留用户长期行为记录，n 的大小可以是几千。

• 对于每个候选物品，在用户 LastN 记录中做快速查找，找到 k 个相似物品。

• 把 LastN 变成 TopK，然后输入到注意力层。

• SIM 模型减小计算量（从 n 降到 k）。


参考文献：

• Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020.第一步：查找

• 方法一：Hard Search
  • 根据候选物品的类目，保留 LastN 物品中类目相同的。
  • 简单，快速，无需训练。第一步：查找

• 方法二：Soft Search  
  • 使用机器学习模型，根据候选物品向量查找相似的 LastN 物品。
  • 更精确，但计算开销较大。第二步：注意力机制

• 对查找到的 TopK 物品应用注意力机制。
• 计算候选物品与 TopK 物品的相似度作为权重。
• 对 TopK 物品向量进行加权平均。SIM模型架构

[Architecture diagram showing:]

用户长期行为序列 → 查找模块 → TopK相似物品 → 注意力层 → 用户兴趣表示

候选物品 → [参与查找和注意力计算]

这展示了SIM模型的完整流程：从长期行为序列中查找TopK相似物品，然后应用注意力机制。SIM模型优势

• 能够处理长期用户行为序列（几千个物品）
• 通过查找机制减少计算复杂度
• 保留长期兴趣的同时关注短期相关性
• 在大规模工业应用中更实用实验结果

• SIM模型在CTR预测任务上优于DIN模型
• 能够有效利用长期用户行为数据
• 计算效率显著提升
• 在阿里巴巴广告系统中取得良好效果总结

• DIN: 注意力机制处理用户序列，但受序列长度限制
• SIM: 通过查找+注意力两阶段处理长序列
• 查找阶段: 从长序列中筛选TopK相似物品
• 注意力阶段: 对TopK物品应用注意力机制
• 实现了效率和效果的平衡应用建议

• 短序列（<100项）：使用DIN模型
• 长序列（>1000项）：使用SIM模型
• 根据计算资源和精度要求选择Hard/Soft Search
• 在线服务中优先考虑计算效率Thank You!

http://wangshusen.github.io/推荐系统中的多样性

王树森

Website: http://wangshusen.github.io/

[A small red icon/logo appears in the bottom right corner]物品相似性的度量相似性的度量

• 基于物品属性标签。
  • 类目、品牌、关键词......

• 基于物品向量表征。
  • 用召回的双塔模型学到的物品向量（不好）。
  • 基于内容的向量表征（好）。基于物品属性标签

• 物品属性标签：类目、品牌、关键词......

• 根据一级类目、二级类目、品牌计算相似度。

  • 物品i：美妆、彩妆、香奈儿。
  
  • 物品j：美妆、香水、香奈儿。
  
  • 相似度：sim₁(i,j) = 1，sim₂(i,j) = 0，sim₃(i,j) = 1。双塔模型的物品向量表征

[DIAGRAM DESCRIPTION: Shows a diagram illustrating the dual-tower model for item vector representation. The diagram contains:
- Top section labeled "余弦相似度：cos(a, b)"
- Two towers: left tower labeled "用户塔" (User Tower) with a red vector labeled "a", right tower labeled "物品塔" (Item Tower) with a blue vector labeled "b" (surrounded by a blue oval labeled "物品向量表征")
- Arrows pointing upward from "用户特征" (User Features) at bottom left to User Tower, and from "物品特征" (Item Features) at bottom right to Item Tower
- The vectors a and b point toward the cosine similarity calculation at the top]基于图文内容的物品表征

[VISUAL CONTENT DESCRIPTION: The slide shows two examples of content-based item representation:

1. Top example:
   - Image: A photo of a Shiba Inu dog with Chinese text overlay about Japanese Shiba Inu
   - Arrow pointing to: CNN (Convolutional Neural Network box)
   - Arrow pointing to: A vertical cyan/blue vector representation

2. Bottom example: 
   - Text content in Chinese: "在日本买柴犬真详细解说，我是店长八公，我是日本人。今年是我在东京开宠物店的第16年。很多中国客人特意来我家买柴犬。有客人说大概要预定60万日元（约4万人民币）一条柴犬。日本柴犬真那么贵吗？怎么可能那么贵。这里八公为住在客人办理柴犬的费用，中国的朋友只看这一贴就可以了解大致的行情!!!"
   - Arrow pointing to: BERT (Language model box)  
   - Arrow pointing to: A vertical dark blue vector representation]基于图文内容的物品向量表征

• CLIP [1] 是当前公认最有效的预训练方法。

• 思想：对于图片—文本二元组，预测图文是否匹配。

• 优势：无需人工标注。小红书的笔记天然包含图片+文字，大部分笔记图文相关。

参考文献：

1. Radford et al. Learning transferable visual models from natural language supervision. In ICML, 2021.基于图文内容的物品表征

图片：                    文字：

[IMAGE 1: Shiba Inu dog photo] ←→ [Document icon with lines]

[IMAGE 2: Car photo] ←→ [Document icon with lines]

⋮                        ⋮

[IMAGE 3: Food/avocado photo] ←→ [Document icon with lines]

[This slide shows the pairing relationship between images and text content for content-based item representation]基于图文内容的物品表征

图片：                    文字：

[IMAGE 1: Shiba Inu dog photo] ←→ [Document icon with lines]

[IMAGE 2: Car photo (highlighted with purple oval)] ←→ [Document icon with lines (highlighted with purple oval)] 正样本

⋮                        ⋮

[IMAGE 3: Food/avocado photo] ←→ [Document icon with lines]基于图文内容的物品表征

图片：                    文字：                    
                                                • 一个 batch 内有 m 对正样本。

[IMAGE 1: Shiba Inu dog photo (highlighted with green)] ←→ [Document icon with lines]      • 一张图片和 m-1 条文本组成
                                                负样本。
[IMAGE 2: Car photo] ←→ [Document icon with lines (highlighted with green)]

⋮                        ⋮         负样本      • 这个 batch 内一共有 m(m-1) 
                                                对负样本。
[IMAGE 3: Food/avocado photo] ←→ [Document icon with lines]

[Green highlighting and area shows the negative sample pairing concept]提升多样性的方法推荐系统的链路

[FLOW DIAGRAM: Shows the recommendation system pipeline with following elements connected by arrows:]

几亿物品 → 召回 → 几千物品 → 粗排 → 后处理 → 几百物品 → 精排 → 后处理 → 物品1, 物品2, ⋮, 物品k

[The "粗排" and "精排" boxes are highlighted with red borders]

• 粗排和精排用多目标模型对物品做 pointwise 打分。

• 对于物品 i，模型输出点击率、交互率的预估，融合成分数 reward_i。

• reward_i 表示用户对物品 i 的兴趣，即物品本身价值。推荐系统的链路

[FLOW DIAGRAM: Shows the recommendation system pipeline with following elements connected by arrows:]

几亿物品 → 召回 → 几千物品 → 粗排 → 后处理 → 几百物品 → 精排 → 后处理 → 物品1, 物品2, ⋮, 物品k

[The "后处理" boxes are highlighted with red borders]

• 给定 n 个候选物品，排序模型打分 reward₁, ⋯, rewardₙ。

• 从 n 个候选物品中选出 k 个，既要它们的总分高，也需要它们有多样性。推荐系统的链路

[FLOW DIAGRAM: Shows the recommendation system pipeline with following elements connected by arrows:]

几亿物品 → 召回 → 几千物品 → 粗排 → 后处理 → 几百物品 → 精排 → 后处理 → 物品1, 物品2, ⋮, 物品k

[The "后处理" boxes are highlighted with red borders, with additional text below:]
也需要多样性算法                被称为"重排"Thank You!

Website: http://wangshusen.github.io/Maximal Marginal Relevance (MMR)

王树森

Website: http://wangshusen.github.io/

[A small red icon/logo appears in the bottom right corner]多样性

• 精排给 n 个候选物品打分，融合之后的分数为

  reward₁, ⋯ , rewardₙ

• 把第 i 和 j 个物品的相似度记作 sim(i,j)。

• 从 n 个物品中选出 k 个，既要有高精排分数，
  也要有多样性。MMR多样性算法

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: 4 red video icons representing selected items (S)
- Right side: 6 blue video icons representing unselected items (R)]MMR多样性算法

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: 4 red video icons representing selected items (S)  
- Right side: 6 blue video icons representing unselected items (R)]

• 计算集合 R 中每个物品 i 的 Marginal Relevance 分数：

  MRᵢ = θ · rewardᵢ - (1 - θ) · max sim(i,j).
                              j∈S
        [物品 i 的精排分数]    [物品 i 的多样性分数]MMR多样性算法

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: 4 red video icons representing selected items (S)
- Right side: 6 blue video icons representing unselected items (R)]

• 计算集合 R 中每个物品 i 的 Marginal Relevance 分数：

  MRᵢ = θ · rewardᵢ - (1 - θ) · max sim(i,j).
                              j∈S

• Maximal Marginal Relevance (MMR)：

  argmax MRᵢ.
  i∈RMMR多样性算法

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: 4 red video icons representing selected items (S)
- Right side: 6 blue video icons representing unselected items (R)]

• 计算集合 R 中每个物品 i 的 Marginal Relevance 分数：

  MRᵢ = θ · rewardᵢ - (1 - θ) · max sim(i,j).
                              j∈S

• Maximal Marginal Relevance (MMR)：

  argmax MRᵢ.
  i∈RMMR多样性算法

1. 已选中的物品 S 初始化为空集，未选中的物品 R 初始化
   为全集 {1,⋯,n}。

2. 选择精排分数 rewardᵢ 最高的物品，从集合 R 移到 S。

3. 做 k-1 轮循环：

   a. 计算集合 R 中所有物品的分数 {MRᵢ}ᵢ∈ᴿ。

   b. 选出分数最高的物品，将其从 R 移到 S。滑动窗口

• MMR : argmax {θ · rewardᵢ - (1 - θ) · max sim(i,j)}.
        i∈R                            j∈S滑动窗口

• MMR : argmax {θ · rewardᵢ - (1 - θ) · max sim(i,j)}.
        i∈R                            j∈S

• 已选中的物品越多（即集合 S 越大），越难找出物品 i ∈ R，
  使得 i 与 S 中的物品都不相似。

• 设 sim 的取值范围是 [0,1]。当 S 很大时，多样性分数
  max sim(i,j) 总是约等于 1，导致 MMR 算法失效。
  j∈S

• 解决方案：设置一个滑动窗口 W，比如最近选中的 10 个物
  品，用 W 代替 MMR 公式中的 S。滑动窗口

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: Red video icons, followed by green video icons marked as "滑动窗口（记作 W）", representing selected items (S)
- Right side: Blue video icons representing unselected items (R)]滑动窗口

• 标准 MMR : argmax {θ · rewardᵢ - (1 - θ) max sim(i,j)}.
            i∈R                            j∈S

• 用滑动窗口 : argmax {θ · rewardᵢ - (1 - θ) max sim(i,j)}.
              i∈R                            j∈W

选中的物品（记作 S）                    未选中的物品（记作 R）

[VISUAL: Shows two groups of video/content icons:
- Left side: Red video icons, followed by green video icons marked as "滑动窗口（记作 W）", representing selected items (S)
- Right side: Blue video icons representing unselected items (R)]Thank You!

Website: http://wangshusen.github.io/重排的规则

王树森

Website: http://wangshusen.github.io/

[A small red icon/logo appears in the bottom right corner]重排的规则

规则：最多连续出现 k 篇某种笔记

• 小红书推荐系统的物品分为图文笔记、视频笔记。

• 最多连续出现 k = 5 篇图文笔记，最多连续出现 k = 5 篇视频笔记。

• 如果排 i 到 i + 4 的全都是图文笔记，那么排在 i + 5 的必须是视频笔记。

注：不是小红书的真实数据重排的规则

规则：每 k 篇笔记最多出现 1 篇某种笔记

• 运营推广笔记的精排分会乘以大于 1 的系数（boost），帮助笔记获得更多曝光。

• 为了防止 boost 影响体验，限制每 k = 9 篇笔记最多出现 1 篇运营推广笔记。

• 如果排第 i 位的是运营推广笔记，那么排 i + 1 到 i + 8 的不能是运营推广笔记。

注：不是小红书的真实数据重排的规则

规则：前 t 篇笔记最多出现 k 篇某种笔记

• 排名前 t 篇笔记最容易被看到，对用户体验最重要。
  （小红书的 top 4 为首屏）

• 小红书推荐系统有带电商卡片的笔记，过多可能会影响体验。

• 前 t = 1 篇笔记最多出现 k = 0 篇带电商卡片的笔记。

• 前 t = 4 篇笔记最多出现 k = 1 篇带电商卡片的笔记。

注：不是小红书的真实数据MMR + 重排规则

• MMR 每一轮选出一个物品：

  argmax {θ · rewardᵢ - (1 - θ) · max sim(i,j)}.
  i∈R                            j∈W

  R 是未选中的物品            MRᵢ 分数MMR + 重排规则

• MMR 每一轮选出一个物品：

  argmax {θ · rewardᵢ - (1 - θ) · max sim(i,j)}.
  i∈R                            j∈W

  R 是未选中的物品            MRᵢ 分数

• 重排结合 MMR 与规则，在满足规则的前提下最大化 MR。

• 每一轮先用规则排除掉 R 中的部分物品，得到子集 R'。MMR + 重排规则

• MMR 每一轮选出一个物品：

  argmax {θ · rewardᵢ - (1 - θ) · max sim(i,j)}.
  i∈R                            j∈W

  把 R 替换成子集 R'

• 重排结合 MMR 与规则，在满足规则的前提下最大化 MR。

• 每一轮先用规则排除掉 R 中的部分物品，得到子集 R'。

• MMR 公式中的 R 替换成子集 R'，选中的物品符合规则。Thank You!

Website: http://wangshusen.github.io/DPP : 数学基础

王树森

http://wangshusen.github.io/超平行体

• 2 维空间的超平行体为平行四边形。

• 平行四边形中的点可以表示为：
    x = α₁v₁ + α₂v₂.

• 系数 α₁ 和 α₂ 取值范围是 [0, 1]。超平行体

• 2 维空间的超平行体为平行四边形。

• 平行四边形中的点可以表示为：
    x = α₁v₁ + α₂v₂.

• 系数 α₁ 和 α₂ 取值范围是 [0, 1]。

• 图中红点示例：x = (1/2)v₁ + (1/2)v₂超平行体

• 2 维空间的超平行体为平行四边形。

• 平行四边形中的点可以表示为：
    x = α₁v₁ + α₂v₂.

• 系数 α₁ 和 α₂ 取值范围是 [0, 1]。

• 图中红点示例：x = v₁ + v₂（对应右上角顶点）超平行体

• 3 维空间的超平行体为平行六面体。

• 平行六面体中的点可以表示为：
    x = α₁v₁ + α₂v₂ + α₃v₃.

• 系数 α₁, α₂, α₃ 取值范围是 [0, 1]。超平行体

• 一组向量 v₁, ···, vₖ ∈ ℝᵈ 可以确定一个 k 维超平行体：

    P(v₁, ···, vₖ) = {α₁v₁ + ··· + αₖvₖ | 0 ≤ α₁, ···, αₖ ≤ 1}.超平行体

• 一组向量 v₁, ···, vₖ ∈ ℝᵈ 可以确定一个 k 维超平行体：

    P(v₁, ···, vₖ) = {α₁v₁ + ··· + αₖvₖ | 0 ≤ α₁, ···, αₖ ≤ 1}.

• 要求 k ≤ d，比如 d = 3 维空间中有 k = 2 维平行四边形。

• 如果 v₁, ···, vₖ 线性相关，则体积 vol(P) = 0。（例：有 k = 3 个向量，落在一个平面上，则平行六面体的体积为 0。）平行四边形的面积

• 面积 = ‖底‖₂ × ‖高‖₂。

• 以 v₁ 为底，计算高 q₂，两个向量必须正交。平行四边形的面积

以 v₁ 为底，如何计算高 q₂？

• 计算 v₂ 在 v₁ 上的投影：

    Proj_{v₁}(v₂) = (v₁ᵀv₂)/(‖v₁‖₂²) · v₁.平行四边形的面积

以 v₁ 为底，如何计算高 q₂？

• 计算 v₂ 在 v₁ 上的投影：

    Proj_{v₁}(v₂) = (v₁ᵀv₂)/(‖v₁‖₂²) · v₁.

• 计算 q₂ = v₂ - Proj_{v₁}(v₂)。

• 性质：底 v₁ 与高 q₂ 正交。平行四边形的面积

以 v₂ 为底，如何计算高 q₁？

• 计算 v₁ 在 v₂ 上的投影：

    Proj_{v₂}(v₁) = (v₁ᵀv₂)/(‖v₂‖₂²) · v₂.平行四边形的面积

以 v₂ 为底，如何计算高 q₁？

• 计算 v₁ 在 v₂ 上的投影：

    Proj_{v₂}(v₁) = (v₁ᵀv₂)/(‖v₂‖₂²) · v₂.

• 计算 q₁ = v₁ - Proj_{v₂}(v₁)。

• 性质：底 v₂ 与高 q₁ 正交。平行六面体的体积

• 体积 = 底面积 × ‖高‖₂。

• 平行四边形 P(v₁, v₂) 是平行六面体 P(v₁, v₂, v₃) 的底。

• 高 q₃ 垂直于底 P(v₁, v₂)。平行六面体的体积

体积何时最大化、最小化？

• 设 v₁、v₂、v₃ 都是单位向量。

• 当三个向量正交时，平行六面体为正方体，体积最大化，vol = 1。

• 当三个向量线性相关时，体积最小化，vol = 0。衡量物品多样性

• 给定 k 个物品，把它们表征为单位向量 v₁, ···, vₖ ∈ ℝᵈ。（d ≥ k）

• 用超平行体的体积衡量物品的多样性，体积介于 0 和 1 之间。

• 如果 v₁, ···, vₖ 两两正交（多样性好），则体积最大化，vol = 1。

• 如果 v₁, ···, vₖ 线性相关（多样性差），则体积最小化，vol = 0。衡量物品多样性

• 给定 k 个物品，把它们表征为单位向量 v₁, ···, vₖ ∈ ℝᵈ。（d ≥ k）

• 把它们作为矩阵 V ∈ ℝᵈˣᵏ 的列。

• 设 d ≥ k，行列式与体积满足：

    det(VᵀV) = vol(P(v₁, ···, vₖ))².

• 因此，可以用行列式 det(VᵀV) 衡量向量 v₁, ···, vₖ 的多样性。Thank You!

http://wangshusen.github.io/DPP : 多样性算法

王树森

http://wangshusen.github.io/多样性问题

• 精排给 n 个物品打分：reward₁, ···, reward_n。

• n 个物品的向量表征：v₁, ···, v_n ∈ ℝᵈ。

• 从 n 个物品中选出 k 个物品，组成集合 S。

• 价值大：分数之和 Σ_{j∈S} reward_j 越大越好。

• 多样性好：S 中 k 个向量组成的超平形体 P(S) 的体积越大越好。多样性问题

• 集合 S 中的 k 个物品的向量作为列，组成矩阵 V_S ∈ ℝᵈˣᵏ。

• 以这 k 个向量作为边，组成超平形体 P(S)。

• 体积 vol(P(S)) 可以衡量 S 中物品的多样性。

• 设 k ≤ d，行列式与体积满足：

    det(V_S^T V_S) = vol(P(S))².行列式点过程 (DPP)

• DPP 是一种传统的统计机器学习方法：

    argmax log det(V_S^T V_S).
    S:|S|=k

• Hulu 的论文[1] 将 DPP 应用在推荐系统：

    argmax θ · (Σ_{j∈S} reward_j) + (1 - θ) · log det(V_S^T V_S).
    S:|S|=k

参考文献：
1. Chen et al. Fast greedy map inference for determinantal point process to improve recommendation diversity. In NIPS, 2018.行列式点过程 (DPP)

• DPP 应用在推荐系统：

    argmax θ · (Σ_{j∈S} reward_j) + (1 - θ) · log det(V_S^T V_S).
    S:|S|=k                                            = A_S (k×k)

• 设 A 为 n×n 的矩阵，它的 (i,j) 元素为 a_{ij} = v_i^T v_j。

• 给定向量 v_1, ···, v_n ∈ ℝ^d，需要 O(n^2 d) 时间计算 A。

• A_S = V_S^T V_S 为 A 的一个 k×k 子矩阵。如果 i,j ∈ S，则 a_{ij} 是 A_S 的一个元素。行列式点过程 (DPP)

• DPP 应用在推荐系统：

    argmax θ · (Σ_{j∈S} reward_j) + (1 - θ) · log det(A_S).
    S:|S|=k

• DPP 是个组合优化问题，从集合 {1, ···, n} 中选出一个大小为 k 的子集 S。

• 用 S 表示已选中的物品，用 R 表示未选中的物品，贪心算法求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
    i∈R求解 DPP暴力算法

• 贪心算法求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
    i∈R

• 对于单个 i，计算 A_{S∪{i}} 的行列式需要 O(|S|³) 时间。

• 对于所有的 i ∈ R，计算行列式需要时间 O(|S|³ · |R|)。

• 需要求解上式 k 次才能选出 k 个物品。如果暴力计算行列式，那么总时间复杂度为

    O(|S|³ · |R| · k) = O(nk⁴).暴力算法

• 贪心算法求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
    i∈R

• 暴力算法的总时间复杂度为

    O(n²d + nk⁴).Hulu的快速算法

• Hulu 的论文设计了一种数值算法，仅需 O(n²d + nk²) 的时间从 n 个物品中选出 k 个物品。

• 给定向量 v₁, ···, vₙ ∈ ℝᵈ，需要 O(n²d) 时间计算 A。

• 用 O(nk²) 时间计算所有的行列式（利用 Cholesky 分解）。Hulu的快速算法

• Cholesky 分解 Aₛ = LLᵀ，其中 L 是下三角矩阵（对角线以上的元素全零）。

• Cholesky 分解可供计算 Aₛ 的行列式。

• 下三角矩阵 L 的行列式 det(L) 等于 L 对角线元素乘积。

• Aₛ 的行列式为 det(Aₛ) = det(L)² = ∏ᵢ l²ᵢᵢ.

• 已知 Aₛ = LLᵀ，则可以快速求出所有 Aₛ∪{ᵢ} 的 Cholesky 分解，因此可以快速算出所有 Aₛ∪{ᵢ} 的行列式。Hulu的快速算法

• 贪心算法求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
    i∈R

• 初始时 S 中只有一个物品，Aₛ 是 1×1 的矩阵，

• 每一轮循环，基于上一轮算出的 Aₛ = LLᵀ，快速求出 A_{S∪{i}} 的 Cholesky 分解（∀i ∈ R），从而求出 log det(A_{S∪{i}})。DPP 的扩展滑动窗口

• 用 S 表示已选中的物品，用 R 表示未选中的物品，DPP 的贪心算法求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
    i∈R

• 随着集合 S 增大，其中相似物品越来越多，物品向量会趋近线性相关。

• 行列式 det(A_S) 会拥缩到零，对数趋于负无穷。滑动窗口

选中的物品（记作 S）                           未选中的物品（记作 R）
[红色视频图标] [红色视频图标] ··· [红色视频图标] [红色视频图标] [绿色视频图标] [绿色视频图标] ··· [绿色视频图标] [绿色视频图标] [蓝色视频图标] [蓝色视频图标] [蓝色视频图标] ··· [蓝色视频图标] [蓝色视频图标] [蓝色视频图标]

                    滑动窗口（记作 W）滑动窗口

• 贪心算法：    argmax θ · reward_i + (1 - θ) · log det(A_{S∪{i}}).
                i∈R

• 用滑动窗口：  argmax θ · reward_i + (1 - θ) · log det(A_{W∪{i}})
                i∈R

选中的物品（记作 S）                           未选中的物品（记作 R）
[红色视频图标] [红色视频图标] ··· [红色视频图标] [红色视频图标] [绿色视频图标] [绿色视频图标] ··· [绿色视频图标] [绿色视频图标] [蓝色视频图标] [蓝色视频图标] [蓝色视频图标] ··· [蓝色视频图标] [蓝色视频图标] [蓝色视频图标]

                    滑动窗口（记作 W）规则约束

• 贪心算法每轮从 R 中选出一个物品：

    argmax θ · reward_i + (1 - θ) · log det(A_{W∪{i}}).
    i∈R

• 有很多规则约束，例如最多连续出 5 篇视频笔记（如果已经连续出了 5 篇视频笔记，下一篇必须是图文笔记）。

• 用规则排除掉 R 中的部分物品，得到子集 R'，然后求解：

    argmax θ · reward_i + (1 - θ) · log det(A_{W∪{i}}).
    i∈R'Thank You!

http://wangshusen.github.io/# Cold Start: Evaluation Metrics

**物品冷启动：评价指标**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on evaluation metrics. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.]# Item Cold Start (物品冷启动)

## Research UGC's Item Cold Start (研究 UGC 的物品冷后)

• New notes published by users on Xiaohongshu (小红书上用户新发布的笔记。)

• New videos uploaded by users on Bilibili (B站上用户新上传的视频。)  

• New articles published by authors on Jinri Toutiao (今日头条上作者新发布的文章。)# New Note Cold Start (新笔记冷启动)

## Why do we need special treatment for new notes? (为什么要特殊对待新笔记？)

• New notes have limited interaction with users, leading to high difficulty and poor effectiveness in recommendation. (新笔记缺少与用户的交互，导致推荐的难度大、效果差。)

• Supporting newly published, low-exposure notes can enhance authors' publishing intention. (扶持新发布、低曝光的笔记，可以增强作者发布意愿。)# Objectives for Optimizing Cold Start (优化冷启动的目标)

1. **Accurate Recommendation**: Overcome cold start difficulties and recommend new notes to suitable users without causing user aversion. (精准推荐：克服冷启的困难，把新笔记推荐给合适的用户，不引起用户反感。)

2. **Encourage Publishing**: Drive traffic toward low-exposure new notes to motivate authors to publish. (激励发布：流量向低曝光新笔记倾斜，激励作者发布。)

3. **Explore High Quality**: Through early small-volume testing, find high-quality notes and provide traffic incentives. (挖掘高潜：通过初期小流量的试探，找到高质量的笔记，给与流量倾斜。)# Evaluation Metrics (评价指标)

## Author-side Metrics (作者侧指标):
• Publishing penetration rate, average publishing volume. (发布渗透率、人均发布量。)

## User-side Metrics (用户侧指标):
• New note metrics: Click-through rate, interaction rate for new notes. (新笔记指标：新笔记的点击率、交互率。)
• Overall metrics: Consumption duration, daily active users, monthly active users. (大盘指标：消费时长、日活、月活。)

## Content-side Metrics (内容侧指标):
• Proportion of popular notes. (高热笔记占比。)

[Note: This slide shows the key evaluation metrics for cold start optimization, organized by three stakeholder perspectives: authors, users, and content. Red arrows point to each major category.]# Author-side Metrics (作者侧指标)

[Note: This is a section header slide with minimal content, introducing the topic of author-side metrics for cold start evaluation.]# Author-side Metrics (作者侧指标)

## Publishing Penetration Rate (发布渗透率 - penetration rate)

• **Formula**: Publishing penetration rate = Number of publishers per day / Daily active users
  (发布渗透率 = 当日发布人数 / 日活人数)

• Publishing one article or more counts as one publisher.
  (发布一篇或以上，就算一个发布人数。)

• **Example**:
  • Number of publishers per day = 1 million (当日发布人数 = 100万)
  • Daily active users = 20 million (日活人数 = 2000万)
  • Publishing penetration rate = 100 / 2000 = 5% (发布渗透率 = 100 / 2000 = 5%)# Author-side Metrics (作者侧指标)

## Average Publishing Volume (人均发布量)

• **Formula**: Average publishing volume = Number of notes published per day / Daily active users
  (人均发布量 = 当日发布笔记数 / 日活人数)

• **Example**:
  • Daily published notes = 2 million (每日发布笔记数 = 200万)
  • Daily active users = 20 million (日活人数 = 2000万)
  • Average publishing volume = 200 / 2000 = 0.1 (人均发布量 = 200 / 2000 = 0.1)# Author-side Metrics (作者侧指标)

## Key Insights

• Publishing penetration rate and average publishing volume reflect authors' publishing activity.
  (发布渗透率、人均发布量反映出作者的发布积极性。)

• The important optimization goal for cold start is to promote publishing and expand the content pool.
  (冷启的重要优化目标是促进发布，增大内容池。)

• The more exposure new notes receive, the earlier first exposure and interaction appear, the higher the authors' publishing activity becomes.
  (新笔记获得的曝光越多，首次曝光和交互出现得越早，作者发布积极性越高。)# User-side Metrics (用户侧指标)

[Note: This is a section header slide with minimal content, introducing the topic of user-side metrics for cold start evaluation.]# User-side Metrics (用户侧指标)

## New Notes Consumption Metrics (新笔记的消费指标)

• **Primary metrics**: Click-through rate and interaction rate for new notes.
  (新笔记的点击率、交互率。)

• **Problem**: The exposure baseline is very large.
  (问题：曝光的基尼系数很大。)

• A small number of head new notes occupy most of the exposure.
  (少数头部新笔记占据了大部分的曝光。)

• **Analysis approach**: Separately examine high-exposure and low-exposure new notes.
  (分别考察高曝光、低曝光新笔记。)

  • **High exposure**: For example, >1000 exposures.
    (高曝光：比如>1000次曝光。)
  
  • **Low exposure**: For example, <1000 exposures.
    (低曝光：比如<1000次曝光。)# User-side Metrics (用户侧指标)

## Overall Consumption Metrics (大盘消费指标)

• **Platform-wide metrics**: Consumption duration, daily active users, monthly active users.
  (大盘的消费时长、日活、月活。)

• **What happens when we strongly support low-exposure new notes?**
  (大力扶持低曝光新笔记会发生什么？)

  • **Author-side publishing metrics improve**.
    (作者侧发布指标变好。)
  
  • **User-side overall consumption metrics may deteriorate**.
    (用户侧大盘消费指标变差。)# Content-side Metrics (内容侧指标)

[Note: This is a section header slide with minimal content, introducing the topic of content-side metrics for cold start evaluation.]# Content-side Metrics (内容侧指标)

## Proportion of Popular Notes (高热笔记占比)

• **Popular notes definition**: Notes that received 1000+ clicks in the past 30 days.
  (高热笔记：前30天获得1000+次点击。)

• **The higher the proportion of popular notes, the stronger the cold start phase's ability to identify high-quality notes.**
  (高热笔记占比越高，说明冷启阶段挖掘优质笔记的能力越强。)# Summary (总结)

• **Author-side metrics**: Publishing penetration rate, average publishing volume.
  (作者侧指标：发布渗透率、人均发布量。)

• **User-side metrics**: New notes consumption metrics, overall consumption metrics.
  (用户侧指标：新笔记消费指标、大盘消费指标。)

• **Content-side metrics**: Proportion of popular notes.
  (内容侧指标：高热笔记占比。)# Cold Start Optimization Points (冷启动的优化点)

• **Optimize the entire pipeline** (including recall and ranking).
  (优化全链路（包括召回和排序）。)

• **Traffic adjustment** (how traffic is distributed between new items and old items).
  (流量调控（流量怎么在新物品、老物品中分配）。)# Thank You!

Website: http://wangshusen.github.io/

[Note: This is a simple thank you slide marking the end of the presentation, with the presenter's website URL at the bottom.]# Long-term Recruitment of Excellent Algorithm Engineers (长期招聘优秀的算法工程师)

• **Department**: Xiaohongshu Community Technology Department.
  (部门：小红书社区技术部。)

• **Direction**: Search, Recommendation.
  (方向：搜索、推荐。)

• **Positions**: Campus recruitment, social recruitment, internships.
  (职位：校招、社招、实习。)

• **Locations**: Shanghai, Beijing.
  (地点：上海、北京。)

• **Contact**: ShusenWang@xiaohongshu.com

[Note: This is a recruitment slide for Xiaohongshu (Little Red Book), seeking algorithm engineers for search and recommendation roles. The company logo appears in the bottom right corner.]# Cold Start: Simple Recall Pathways

**物品冷启动：简单的召回通道**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on simple recall pathways. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.]# Recall Difficulties (召回的难点)

[Note: This is a section header slide with minimal content, introducing the topic of recall difficulties in cold start scenarios.]# Available Data for Recall (召回的依据)

## What's Available (✓):
✓ **Content features**: Images, text, location information.
  (自带图片、文字、地点。)

✓ **Algorithm or manually annotated tags**.
  (算法或人工标注的标签。)

## What's Missing (✗):
✗ **No user interaction data**: No clicks, likes, or other engagement information.
  (没有用户点击、点赞等信息。)

✗ **No note ID embedding**.
  (没有笔记ID embedding。)# Difficulties of Cold Start Recall (冷启召回的困难)

• **Limited user interaction**: Haven't learned good note ID embeddings yet, resulting in poor effectiveness of dual-tower models.
  (缺少用户交互，还没学好笔记ID embedding，导致双塔模型效果不好。)

• **Limited user interaction**: Resulting in ItemCF being unsuitable.
  (缺少用户交互，导致ItemCF不适用。)# Difficulties of Cold Start Recall (冷启召回的困难)

## ItemCF is Not Suitable for Item Cold Start (ItemCF不适用于物品冷启动)

**Diagram Description**: The diagram shows why ItemCF doesn't work for cold start items. It displays two items (represented by red and green video icons) at the top, with multiple users (represented by profile icons) at the bottom. The arrows between them represent user interactions. 

The users highlighted in the red box represent those who have interacted with both items, which is necessary for ItemCF to establish item-item relationships. However, for new/cold start items (like the green video), there are very few or no users who have interacted with both the new item and existing items, making it impossible to compute meaningful item-item similarities.

[Note: This diagram effectively illustrates the fundamental problem with using ItemCF (Item-based Collaborative Filtering) for cold start items - there's insufficient interaction data to establish relationships between new items and existing items.]Challenges in Cold Start Callback

ItemCF is not suitable for item cold start

[The image shows a diagram illustrating the cold start problem for items in recommendation systems. There are two items represented by red and green video icons at the top, with arrows pointing down to six different user icons at the bottom. The diagram demonstrates how ItemCF (Item-based Collaborative Filtering) faces challenges when trying to recommend new items (cold start items) because these items don't have sufficient interaction history to establish collaborative relationships with existing items.]Callback Channels

❌ ItemCF Callback (Not suitable)

? Two-Tower Model (Improved applicability)

✓ Category, Keyword Callback (Suitable)

✓ Hot Item Callback (Suitable)

✓ Look-Alike Callback (Suitable)Two-Tower ModelTwo-Tower Model

[The image shows a two-tower architecture diagram]

Left Tower (a - User Tower):
- User ID, discrete features, continuous features
- Feature transformation
- Neural network
- Generates user embedding vector 'a' (shown in blue)

Right Tower (b - Item Tower):
- Item ID, discrete features, continuous features (highlighted in blue box)
- Feature transformation  
- Neural network
- Generates item embedding vector 'b' (shown in red)

Both towers feed into a cosine similarity calculation at the top, which outputs the final similarity score.ID Embedding

Improvement Plan 1: New items use default embedding.

• When training item ID embeddings, let all new items share one ID instead of using their real IDs.

• Default embedding: The embedding vector corresponding to the shared ID.

• When the model is trained next time, new items will have their own ID embedding vectors.ID Embedding

Improvement Plan 1: New items use default embedding.

Improvement Plan 2: Use similar item embedding vectors.

• Find the top k most similar high-quality items in terms of content.

• Average the embedding vectors of k high-quality items as the embedding for new items.Multiple Vector Callback Pools

• Multiple callback pools give new items more exposure opportunities.
  • 1-hour new items,
  • 6-hour new items,
  • 24-hour new items,
  • 30-day items.

• Sharing the same two-tower model, so multiple callback pools don't add training cost.Category CallbackUser Portrait

• Interested categories:
  Food, Tech & Digital, Movies......

• Interested keywords:
  New York, Workplace, Humor, Programmer, University......Category-Based Callback

Categories:        Item ID List (sorted by time):

Food      →       [4 video icons in different shades of red/orange] ...

Travel    →       [4 video icons in different shades of brown/yellow] ...

Beauty    →       [4 video icons in different shades of blue/gray] ...

...Category-Based Callback

• System maintains category index:
  Category → Item List (sorted by time)

• Category-based callback using:
  User Portrait → Category → Item List

• Retrieve the first k items from the item list (i.e., the newest k items).Keyword-Based Callback

• System maintains keyword index:
  Keyword → Item List (sorted by time)

• Callback based on keywords from user portrait.Defects

• Defect 1: Only effective for newly published new items.
  • Retrieve the newest k items under a category/keyword.
  • After a few hours of publication, there's no chance to be recalled again.

• Defect 2: Weak personalization, not precise enough.Summary

❌ ItemCF Callback (Not suitable)

? Two-Tower Model (Improved applicability)

✓ Category, Keyword Callback (Suitable)

✓ Hot Item Callback (Suitable)

✓ Look-Alike Callback (Suitable)Thank You!

http://wangshusen.github.io/Looking for Outstanding Algorithm Engineers

• Department: Xiaohongshu Community Technology Department.

• Direction: Search, Recommendation.

• Position: Campus recruitment, Social recruitment, Internship.

• Location: Shanghai, Beijing.

• Contact: ShusenWang@xiaohongshu.com# Cold Start: Aggregated Recall

**物品冷启动：聚类召回**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on aggregated/clustering recall methods. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.]Hot Callback

Basic Idea

• If a user likes one item, then he will also like items with similar content.

• First train a neural network based on item categories and image/text content, mapping items to vectors.

• Cluster item vectors into 1000 clusters and record the center direction of each cluster. (k-means clustering, using cosine similarity.)Hot Callback

Clustering Index

• After a new item is published, use the neural network to map it to a feature vector.

• Find the most similar vector from 1000 vectors (corresponding to 1000 clusters) as the cluster for the new item.

• Index:
  cluster → Item ID List (sorted by time)Hot Callback

Online Callback

• Given user ID, find his last-n interacted item list and use these items as seed items.

• Map each seed item to a vector and find the most similar cluster. (Know which clusters the user is interested in.)

• From each cluster's item list, retrieve the newest m items.Hot Callback

Online Callback

• Given user ID, find his last-n interacted item list and use these items as seed items.

• Map each seed item to a vector and find the most similar cluster. (Know which clusters the user is interested in.)

• From each cluster's item list, retrieve the newest m items.

• Retrieve at most mn new items.Content Similarity ModelExtract Image and Text Features

[The image shows a two-part feature extraction process:]

Top: Image feature extraction
- Input: An image showing a Shiba Inu dog with Chinese text overlays including "日本买柴犬多少钱" (How much does it cost to buy a Shiba Inu in Japan?)
- Process: CNN (Convolutional Neural Network)
- Output: Feature vector (shown as blue stacked rectangles)

Bottom: Text feature extraction  
- Input: Chinese text content discussing buying Shiba Inu dogs in Japan, mentioning costs and experiences
- Process: BERT (Bidirectional Encoder Representations from Transformers)
- Output: Feature vector (shown as dark blue stacked rectangles)Extract Image and Text Features

[The image shows the same two-part feature extraction process as before, but with additional details:]

Top: Image feature extraction
- Input: Shiba Inu image with Chinese text
- Process: CNN → concatenation → Fully Connected Layer → Item's feature vector

Bottom: Text feature extraction  
- Input: Chinese text content
- Process: BERT → concatenation → Fully Connected Layer → Item's feature vector

Both image and text features are concatenated and passed through a fully connected layer to produce the final item feature vector.Content Similarity Between Two Items

[The image shows a symmetric architecture for comparing two items:]

Left side (Item a):
- Image → CNN → Fully Connected Layer → Feature vector a (blue)
- Text → BERT → Fully Connected Layer → Feature vector a (blue)

Right side (Item b):
- Image → CNN → Fully Connected Layer → Feature vector b (red)
- Text → BERT → Fully Connected Layer → Feature vector b (red)

Both feature vectors are generated using the same neural network architecture but with different input content.Content Similarity Between Two Items

Cosine Similarity: cos(a, b) = ⟨a, b⟩ / (||a||₂ · ||b||₂)

[The diagram shows the same architecture as the previous slide, but now includes the cosine similarity calculation at the top, which computes the similarity score between the two feature vectors a and b.][Slide 011 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_011.png][Slide 012 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_012.png][Slide 013 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_013.png][Slide 014 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_014.png][Slide 015 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_015.png][Slide 016 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_016.png][Slide 017 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_017.png][Slide 018 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_018.png][Slide 019 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_019.png][Slide 020 from 07_ColdStart_03]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_03/07_ColdStart_03_page_020.png]# Cold Start: Look-Alike User Groups

**物品冷启动：Look-Alike人群扩散**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on Look-Alike user group expansion methods. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.][Slide 002 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_002.png][Slide 003 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_003.png][Slide 004 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_004.png][Slide 005 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_005.png][Slide 006 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_006.png][Slide 007 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_007.png][Slide 008 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_008.png][Slide 009 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_009.png][Slide 010 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_010.png][Slide 011 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_011.png][Slide 012 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_012.png][Slide 013 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_013.png][Slide 014 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_014.png][Slide 015 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_015.png][Slide 016 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_016.png][Slide 017 from 07_ColdStart_04]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_04/07_ColdStart_04_page_017.png]# Cold Start: Traffic Control

**物品冷启动：流量调控**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on traffic control methods. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.][Slide 002 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_002.png][Slide 003 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_003.png][Slide 004 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_004.png][Slide 005 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_005.png][Slide 006 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_006.png][Slide 007 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_007.png][Slide 008 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_008.png][Slide 009 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_009.png][Slide 010 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_010.png][Slide 011 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_011.png][Slide 012 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_012.png][Slide 013 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_013.png][Slide 014 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_014.png][Slide 015 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_015.png][Slide 016 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_016.png][Slide 017 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_017.png][Slide 018 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_018.png][Slide 019 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_019.png][Slide 020 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_020.png][Slide 021 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_021.png][Slide 022 from 07_ColdStart_05]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_05/07_ColdStart_05_page_022.png]# Cold Start: AB Testing

**物品冷启动：AB测试**

Author: 王树森
Email: ShusenWang@xiaohongshu.com

Website: http://wangshusen.github.io/

[Note: This appears to be a title slide for a presentation on Cold Start problems in recommendation systems, specifically focusing on AB testing methods. The slide is in Chinese and includes the author's name and contact information. There is also a red logo for "小红书" (Xiaohongshu/Little Red Book) in the bottom right corner.][Slide 002 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_002.png][Slide 003 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_003.png][Slide 004 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_004.png][Slide 005 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_005.png][Slide 006 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_006.png][Slide 007 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_007.png][Slide 008 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_008.png][Slide 009 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_009.png][Slide 010 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_010.png][Slide 011 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_011.png][Slide 012 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_012.png][Slide 013 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_013.png][Slide 014 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_014.png][Slide 015 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_015.png][Slide 016 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_016.png][Slide 017 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_017.png][Slide 018 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_018.png][Slide 019 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_019.png][Slide 020 from 07_ColdStart_06]

[This slide requires manual transcription from the PNG image file at: /Users/htong/Desktop/recommendation_systems/extracted_slides/07_ColdStart_06/07_ColdStart_06_page_020.png]涨指标的方法

王树森

http://wangshusen.github.io/推荐系统的评价指标

• 日活用户数（DAU）和留存是最核心的指标。推荐系统的评价指标

• 日活用户数（DAU）和留存是最核心的指标。

• 目前工业界最常用 LT7 和 LT30 衡量留存。

• 某用户今天（t₀）登录APP，未来7天（t₀~t₆）中有4天
  登录APP，那么该用户今天（t₀）的LT7等于4。

• 显然有1≤LT7≤7和1≤LT30≤30。

• LT增长通常意味着用户体验提升。（除非LT增长且DAU
  下降。）

• 假设APP禁止低活用户登录，则DAU下降，LT增长。推荐系统的评价指标

• 日活用户数（DAU）和留存是最核心的指标。

• 目前工业界最常用 LT7 和 LT30 衡量留存。

• 其他核心指标：用户使用时长、总阅读数（即总点击数）、
  总曝光数。这些指标的重要性低于DAU和留存。

• 时长增长，LT通常会增长。

• 时长增长，阅读数、曝光数可能会下降。推荐系统的评价指标

• 日活用户数（DAU）和留存是最核心的指标。

• 目前工业界最常用 LT7 和 LT30 衡量留存。

• 其他核心指标：用户使用时长、总阅读数（即总点击数）、
  总曝光数。这些指标的重要性低于DAU和留存。

• 非核心指标：点击率、交互率、等等。

• 对于UGC平台，发布量和发布渗透率也是核心指标。涨指标的方法有哪些？

➤ 1. 改进召回模型，添加新的召回模型。

➤ 2. 改进粗排和精排模型。

➤ 3. 提升召回、粗排、精排中的多样性。

➤ 4. 特殊对待新用户、低活用户等特殊人群。

➤ 5. 利用关注、转发、评论这三种交互行为。Thank You!

http://wangshusen.github.io/涨指标的方法: 召回

王树森

http://wangshusen.github.io/召回模型 & 召回通道

• 推荐系统有几十条召回通道，它们的召回总量是固定的。
  总量越大，指标越好，粗排计算量越大。

• 双塔模型（two-tower）和item-to-item（I2I）是最重要的
  两类召回模型，占据召回的大部分配额。

• 有很多小众的模型，占据的配额很少。在召回总量不变的
  前提下，添加某些召回模型可以提升核心指标。

• 有很多内容池，比如30天物品、1天物品、6小时物品、新
  用户优质内容池、分人群内容池。

• 同一个模型可以用于多个内容池，得到多条召回通道。双塔模型改进双塔模型

方向1：优化正样本、负样本。

• 简单正样本：有点击的（用户，物品）二元组。

• 简单负样本：随机组合的（用户，物品）二元组。

• 困难负样本：排序靠后的（用户，物品）二元组。改进双塔模型

方向2：改进神经网络结构。

• Baseline：用户塔、物品塔分别是全连接网络，各输出一个向量，分别作为用户、物品的表征。

• 改进：用户塔、物品塔分别用DCN代替全连接网络。

• 改进：在用户塔中使用用户行为序列（last-n）。

• 改进：使用多向量模型代替单向量模型。（标准的双塔模型也叫单向量模型。）改进双塔模型

[DIAGRAM: 双塔模型架构图]
- 显示用户塔和物品塔的结构
- 用户特征输入用户塔，输出用户embedding
- 物品特征输入物品塔，输出物品embedding
- 多个预测任务：点击率、点赞率、收藏率、完播率等
- 用户塔和物品塔通过全连接层连接到各个预测目标改进双塔模型

[DIAGRAM: 双塔模型架构图 - 突出用户塔]
- 显示用户塔和物品塔的结构
- 用户特征输入用户塔，输出用户embedding（红框突出显示用户塔部分）
- 物品特征输入物品塔，输出物品embedding
- 多个预测任务：点击率、点赞率、收藏率、完播率等
- 用户塔和物品塔通过全连接层连接到各个预测目标改进双塔模型

[DIAGRAM: 双塔模型架构图 - 突出特定部分]
- 显示用户塔和物品塔的结构
- 用户特征输入用户塔，输出用户embedding（红框突出显示点击率相关的用户embedding部分）
- 物品特征输入物品塔，输出物品embedding（红框突出显示物品embedding部分）
- 多个预测任务：点击率、点赞率、收藏率、完播率等
- 用户塔和物品塔通过全连接层连接到各个预测目标# Improving Dual Tower Models

## 改进双塔模型 (Improving Dual Tower Models)

### Model Architecture Overview
The slide shows a dual tower model architecture with:

#### Multi-Task Learning:
- **Top layer metrics**: 点击率 (Click Rate), 点赞率 (Like Rate), 收藏率 (Save Rate), 完播率 (Completion Rate)
- These multiple objectives are combined and fed back into the system

#### Tower Structure:
- **Left Tower (用户塔)**: User Tower
  - Fed by 用户特征 (User Features)
  - Represented by a sequence of colored embedding blocks

- **Right Tower (物品塔)**: Item Tower  
  - Fed by 物品特征 (Item Features)
  - Represented by a sequence of colored embedding blocks

#### Key Improvement Strategy:
The slide highlights **点赞率 (Like Rate)** with a red box, indicating this is a focus metric for optimization.

#### Technical Architecture:
- The dual tower architecture allows for independent encoding of user and item representations
- Multiple behavioral signals (click, like, save, completion) are used for multi-task optimization
- The model learns to predict various engagement metrics simultaneously
- User and item embeddings are learned separately and then combined for final predictions

This approach enables better representation learning by leveraging multiple behavioral signals while maintaining computational efficiency through the dual tower design.# Improving Dual Tower Models - Multi-Task Focus

## 改进双塔模型 (Improving Dual Tower Models)

### Enhanced Multi-Task Learning Architecture
This slide emphasizes the multi-task learning approach with all metrics highlighted:

#### Multi-Task Objectives (All Highlighted):
- **点击率 (Click Rate)** - Primary engagement metric
- **点赞率 (Like Rate)** - User appreciation signal  
- **收藏率 (Save Rate)** - Content value indicator
- **完播率 (Completion Rate)** - Content quality measure

#### Architecture Components:
- **用户塔 (User Tower)**: Enhanced with multi-task learning
  - Processes user features through embedding layers
  - Learns user representations optimized for multiple objectives
  
- **物品塔 (Item Tower)**: Optimized for multi-task prediction
  - Processes item features through embedding layers
  - Learns item representations across different behavioral signals

#### Multi-Task Learning Benefits:
1. **Shared Representations**: Common patterns across tasks improve overall performance
2. **Task Regularization**: Multiple objectives prevent overfitting to single metrics
3. **Data Efficiency**: Leverages correlations between different user behaviors
4. **Balanced Optimization**: Prevents bias toward any single engagement metric

#### Technical Implementation:
- Joint optimization across all behavioral signals
- Shared bottom layers with task-specific heads
- Gradient sharing across multiple objectives
- Balanced loss weighting for optimal performance

This comprehensive approach ensures the recommendation system optimizes for multiple aspects of user engagement simultaneously.# Improving Dual Tower Models - Training Methods

## 改进双塔模型 (Improving Dual Tower Models)

### 方向3：改进模型的训练方法 (Direction 3: Improving Model Training Methods)

#### Training Method Improvements:

**• Baseline：做二分类，让模型学会区分正样本和负样本。**
- **Baseline**: Binary classification approach
- Model learns to distinguish between positive and negative samples
- Traditional approach using simple positive/negative sample pairs

**• 改进：结合二分类、batch 内负采样。（对于 batch 内负采样，需要做纠偏。）**
- **Improvement**: Combine binary classification with in-batch negative sampling
- Requires bias correction for in-batch negative sampling
- More sophisticated sampling strategy within training batches

**• 改进：使用自监督学习方法，让冷门物品的 embedding 学得更好。**
- **Improvement**: Use self-supervised learning methods
- Enables better embedding learning for cold/unpopular items
- Addresses the long-tail problem in recommendation systems

#### Key Training Enhancements:
1. **Advanced Negative Sampling**: In-batch negative sampling with bias correction
2. **Self-Supervised Learning**: Better representations for rare items
3. **Multi-Modal Training**: Combines different learning paradigms
4. **Cold Item Optimization**: Specific focus on improving embeddings for less popular content

#### Technical Benefits:
- Better sample efficiency during training
- Improved handling of data imbalance
- Enhanced representation quality for tail items
- More robust model performance across different item popularity ranges

These training improvements address key challenges in recommendation systems, particularly around sample selection and representation learning for diverse item catalogs.# Item-to-Item (I2I) Recommendation

## Introduction to I2I Systems

### Core Concept:
**Item-to-Item (I2I)** is a fundamental recommendation model class based on similar item recall.

This slide introduces the basic concept of Item-to-Item collaborative filtering, which forms the foundation for many recommendation systems. I2I systems focus on finding and recommending items that are similar to items a user has already interacted with.

### Key Characteristics:
- **Similarity-Based**: Relies on computing similarities between items
- **Collaborative Filtering**: Uses collective user behavior patterns
- **Scalable**: Can handle large item catalogs efficiently
- **Interpretable**: Easy to explain recommendations to users

### Application Context:
I2I models are widely used in various recommendation scenarios including:
- E-commerce product recommendations
- Content recommendations (articles, videos, music)
- Similar item suggestions
- Cross-selling and upselling strategies

The slide sets up the foundation for more detailed discussion of I2I implementation methods and variations that will follow in subsequent slides.# Item-to-Item (I2I) - Implementation Details

## I2I 是一大类模型，基于相似物品做召回

### Core Architecture:
**I2I is a major class of models based on similar item recall**

### Most Common Usage Pattern:
**• 最常见的用法是 U2I2I (user → item → item)**
- **U2I2I Flow**: user → item → item
- This represents the typical recommendation pipeline

### Implementation Steps:
**• 用户 u 喜欢物品 i₁ (用户历史上交互过的物品)**
- User u likes item i₁ (items the user has historically interacted with)
- This establishes the user's preference history

**• 寻找 i₁ 的相似物品 i₂ (即 I2I)**  
- Find similar items i₂ to i₁ (this is the I2I component)
- Core similarity computation between items

**• 将 i₂ 推荐给 u**
- Recommend i₂ to user u
- Complete the recommendation chain

### Key Process Flow:
1. **User History Analysis**: Identify items user has interacted with
2. **Similarity Computation**: Find items similar to user's historical items  
3. **Recommendation Generation**: Recommend similar items to the user

### Technical Foundation:
- Based on item-item collaborative filtering
- Leverages user behavior patterns to establish item similarities
- Efficient for real-time recommendation generation
- Scalable approach for large item catalogs# Item-to-Item (I2I) - Similarity Calculation Methods

## 如何计算物品相似度？(How to Calculate Item Similarity?)

### Method 1: ItemCF and Its Variants
**• 方法1：ItemCF 及其变体**

#### Core ItemCF Principle:
**• 一些用户同时喜欢物品 i₁ 和 i₂，则认为 i₁ 和 i₂ 相似**
- If some users like both items i₁ and i₂ simultaneously, then i₁ and i₂ are considered similar
- Based on co-occurrence in user preference patterns

#### ItemCF Algorithm Family:
**• ItemCF、Online ItemCF、Swing、Online Swing 都是基于相同的思想**
- **ItemCF**: Classic collaborative filtering approach
- **Online ItemCF**: Real-time version of ItemCF
- **Swing**: Enhanced similarity calculation method  
- **Online Swing**: Real-time version of Swing
- All share the same fundamental concept of user co-occurrence

#### Production Implementation:
**• 线上同时使用上述 4 种 I2I 模型，各分配一定配额**
- Use all 4 I2I models simultaneously in production
- Each model is allocated a specific quota/weight
- Ensemble approach for better coverage and performance

### Technical Benefits:
1. **Diversified Similarity**: Multiple algorithms capture different similarity patterns
2. **Risk Distribution**: No single point of failure in similarity computation
3. **Performance Optimization**: Different algorithms excel in different scenarios
4. **Real-time Capability**: Mix of offline and online algorithms for various use cases

This multi-model approach ensures robust and comprehensive item similarity calculation in production recommendation systems.# Item-to-Item (I2I) - Extended Similarity Methods

## Method 2: Vector-Based Similarity Calculation

### 方法2：基于物品向量表征，计算向量相似度
**Method 2: Based on item vector representations, calculate vector similarity**

#### Applicable Models:
**• (双塔模型、图神经网络均可计算物品向量表征。)**
- **Dual Tower Models**: Can compute item vector representations
- **Graph Neural Networks**: Can compute item vector representations  
- Both architectures enable vector-based similarity computation

### Comprehensive I2I Approach:
The slide introduces the second major method for calculating item similarity, expanding beyond collaborative filtering to include:

#### Vector-Based Similarity:
1. **Representation Learning**: Items are encoded into dense vector representations
2. **Similarity Computation**: Use vector similarity metrics (cosine, dot product, etc.)
3. **Deep Learning Integration**: Leverages neural network architectures

#### Architecture Compatibility:
- **Dual Tower Models**: Generate item embeddings through dedicated item tower
- **Graph Neural Networks**: Learn item representations through graph structure
- **Multi-Modal Learning**: Can incorporate various item features (text, images, metadata)

#### Advantages:
- **Semantic Similarity**: Captures deeper item relationships beyond co-occurrence
- **Cold Start Friendly**: Can handle new items with limited interaction data
- **Feature Rich**: Incorporates multiple types of item information
- **Scalable**: Efficient computation using vector operations

This method complements traditional collaborative filtering approaches by providing richer, more nuanced similarity calculations based on learned representations rather than just behavioral co-occurrence patterns.# Recall Models Summary

## 小众的召回模型 (Niche Recall Models)

This slide serves as a section header introducing specialized or less common recall models used in recommendation systems.

### Context:
After covering the main I2I (Item-to-Item) approaches, this section will likely explore:

#### Potential Niche Models:
- **Specialized Collaborative Filtering**: Advanced variants beyond basic CF
- **Content-Based Methods**: Using item attributes and metadata
- **Hybrid Approaches**: Combining multiple recommendation strategies
- **Domain-Specific Models**: Tailored for specific verticals or use cases
- **Experimental Techniques**: Newer or research-oriented methods

#### Why Niche Models Matter:
1. **Coverage Enhancement**: Fill gaps left by mainstream methods
2. **Specialized Use Cases**: Address specific domain requirements
3. **Diversity Improvement**: Provide varied recommendation sources
4. **Performance Optimization**: Excel in particular scenarios
5. **Innovation Pipeline**: Test new approaches before mainstream adoption

#### Integration Strategy:
- Often used alongside primary recall methods
- Allocated smaller traffic percentages for testing
- Focused on specific user segments or item categories
- Part of ensemble recommendation systems

This introduction sets up the discussion of alternative and specialized recall methods that complement the primary I2I and collaborative filtering approaches in a comprehensive recommendation system.# I2I-Like Models (I2I类似的模型)

## Alternative Recommendation Pathways

### U2U2I Model:
**• U2U2I (user → user → item)：已知用户 u₁ 与 u₂ 相似，且 u₂ 喜欢物品 i，那么给用户 u₁ 推荐物品 i。**

#### Process Flow:
1. **User Similarity**: Identify that users u₁ and u₂ are similar
2. **Preference Transfer**: u₂ likes item i
3. **Recommendation**: Recommend item i to user u₁

### U2A2I Model:
**• U2A2I (user → author → item)：已知用户 u 喜欢作者 a，且 a 发布物品 i，那么给用户 u 推荐物品 i。**

#### Process Flow:
1. **Author Preference**: User u likes author a
2. **Content Creation**: Author a publishes item i
3. **Recommendation**: Recommend item i to user u

### U2A2A2I Model:
**• U2A2A2I (user → author → author → item)：已知用户 u 喜欢作者 a₁，且 a₁ 与 a₂ 相似，a₂ 发布物品 i，那么给用户 u 推荐物品 i。**

#### Process Flow:
1. **Initial Author Preference**: User u likes author a₁
2. **Author Similarity**: a₁ and a₂ are similar authors
3. **Content Creation**: Author a₂ publishes item i
4. **Recommendation**: Recommend item i to user u

### Model Characteristics:
- **Extended Pathways**: Longer chains of relationships for recommendations
- **Multi-Hop Logic**: Traverse multiple nodes in the recommendation graph
- **Relationship Diversity**: Leverage user-user, user-author, and author-author similarities
- **Content Creator Focus**: Emphasize the role of content creators in recommendations
- **Graph-Based Thinking**: Model recommendations as paths through a knowledge graph

These models expand beyond simple I2I by incorporating user similarities and author relationships, creating more diverse and potentially serendipitous recommendations.# Advanced Recall Models (更复杂的模型)

## State-of-the-Art Recall Architectures

### Advanced Deep Learning Models:

**• Path-based Deep Network (PDN) [1]**
- Graph-based approach using path information
- Leverages multi-hop relationships in user-item graphs
- Published in SIGIR 2021

**• Deep Retrieval [2]**
- End-to-end retrieval structure for large-scale recommendations
- Scalable deep learning approach
- Published in CIKM 2021

**• Sparse-Interest Network (SINE) [3]**
- Handles sequential recommendation with sparse user interests
- Addresses temporal patterns in user behavior
- Published in WSDM 2021

**• Multi-task Multi-view Graph Representation Learning (M2GRL) [4]**
- Combines multiple tasks and views for better representations
- Graph-based multi-task learning framework
- Published in KDD 2020

### References (参考文献):

1. **Li et al.** Path-based Deep Network for Candidate Item Matching in Recommenders. In *SIGIR*, 2021.
2. **Gao et al.** Learning an end-to-end structure for retrieval in large-scale recommendations. In *CIKM*, 2021.
3. **Tan et al.** Sparse-interest network for sequential recommendation. In *WSDM*, 2021.
4. **Wang et al.** M2GRL: A multitask multi-view graph representation learning framework for web-scale recommender systems. In *KDD*, 2020.

### Model Characteristics:
- **Research-Oriented**: Cutting-edge techniques from top conferences
- **Complex Architectures**: Advanced neural network designs
- **Scalability Focus**: Designed for large-scale production systems
- **Multi-Modal**: Incorporate various data sources and relationship types
- **Performance-Driven**: State-of-the-art results on benchmark datasets

These advanced models represent the current frontier in recommendation system research, offering sophisticated approaches beyond traditional collaborative filtering methods.# Summary: Improving Recall Models

## 总结：改进召回模型 (Summary: Improving Recall Models)

### Dual Tower Model Improvements:
**• 双塔模型：优化正负样本、改进神经网络结构、改进训练的方法。**

#### Key Enhancement Areas:
1. **Positive/Negative Sample Optimization**: Improve training data quality and sampling strategies
2. **Neural Network Architecture Improvements**: Enhance model structure and capacity
3. **Training Method Improvements**: Advanced training techniques and optimization strategies

### I2I Model Enhancements:
**• I2I 模型：同时使用 ItemCF 及其变体、使用物品向量表征计算物品相似度。**

#### Dual Approach Strategy:
1. **ItemCF and Variants**: Traditional collaborative filtering methods (ItemCF, Online ItemCF, Swing, Online Swing)
2. **Vector-Based Similarity**: Use item embeddings from neural networks for similarity computation

### Advanced Model Integration:
**• 添加小众的召回模型，比如 PDN、Deep Retrieval、SINE、M2GRL 等模型。**

#### Specialized Models:
- **PDN**: Path-based Deep Network for graph-based recommendations
- **Deep Retrieval**: End-to-end retrieval architectures  
- **SINE**: Sparse-Interest Network for sequential patterns
- **M2GRL**: Multi-task Multi-view Graph Representation Learning

### Production Strategy:
**• 在召回总量不变的前提下，调整各召回通道的配额。（可以让各用户群体用不同的配额。）**

#### Traffic Allocation Optimization:
- **Fixed Total Recall**: Maintain overall recall volume while optimizing distribution
- **Channel Rebalancing**: Adjust quotas across different recall methods
- **User Segment Customization**: Different user groups can have different channel allocations
- **A/B Testing Framework**: Systematic evaluation of allocation strategies

### Comprehensive Improvement Framework:
This approach combines traditional methods with cutting-edge techniques, creating a robust, multi-layered recall system that can adapt to different user segments and use cases while maintaining production stability.# Thank You!

## Presentation Conclusion

This slide marks the end of the presentation on improving recommendation systems.

### Contact Information:
**http://wangshusen.github.io/**

### Presentation Summary:
The presentation covered comprehensive strategies for improving recommendation system recall models, including:

1. **Dual Tower Model Enhancements**:
   - Sample optimization techniques
   - Neural architecture improvements  
   - Advanced training methodologies

2. **I2I Model Improvements**:
   - Traditional collaborative filtering variants
   - Vector-based similarity computation
   - Multi-method ensemble approaches

3. **Advanced Model Integration**:
   - State-of-the-art deep learning models
   - Graph-based recommendation systems
   - Multi-task learning frameworks

4. **Production Implementation**:
   - Traffic allocation strategies
   - A/B testing frameworks
   - User segment customization

### Key Takeaways:
- Multi-layered approach combining traditional and advanced methods
- Systematic optimization across all components of recall systems
- Production-ready strategies for large-scale implementation
- Research-backed techniques for performance improvement

The presentation provides a comprehensive roadmap for practitioners looking to enhance their recommendation system performance through systematic improvements in recall model design and implementation.# Ranking Label Methods: Ranking Models

## 涨指标的方法：排序模型

### Course Title and Instructor
**王树森 (Wang Shusen)**

### Course Website
**http://wangshusen.github.io/**

### Overview
This presentation focuses on methods to improve performance metrics through ranking model enhancements. The title suggests this is about systematic approaches to boost key performance indicators (KPIs) in recommendation systems through advanced ranking techniques.

### Key Focus Areas:
- **Ranking Model Improvements**: Advanced techniques to enhance ranking performance
- **Metric Optimization**: Systematic approaches to improve recommendation system KPIs
- **Practical Implementation**: Real-world strategies for ranking model deployment

### Context
This appears to be part of a comprehensive series on recommendation system improvements, specifically focusing on the ranking stage of the recommendation pipeline. The ranking stage is crucial as it determines the final order of items presented to users, directly impacting user engagement and satisfaction metrics.

### Expected Content:
- Advanced ranking algorithms and architectures
- Multi-objective optimization strategies
- Feature engineering for ranking models
- Training methodologies for ranking systems
- Online learning and adaptation techniques
- Performance evaluation and metric optimization# What Methods Are There for Improving Metrics?

## 涨指标的方法有哪些？(What Methods Are There for Improving Metrics?)

### Primary Improvement Strategies (Highlighted):

**• 改进召回模型，添加新的召回模型。**
- **Improve recall models, add new recall models**
- Enhance existing recall mechanisms and introduce novel approaches
- Focus on expanding the candidate item pool effectively

**• 改进粗排和精排模型。**
- **Improve coarse ranking and fine ranking models**
- Optimize both preliminary filtering and detailed ranking stages
- Enhance the entire ranking pipeline for better performance

### Additional Improvement Areas:

**• 提升召回、粗排、精排中的多样性。**
- **Improve diversity in recall, coarse ranking, and fine ranking**
- Address filter bubble effects and recommendation homogeneity
- Enhance user experience through varied content exposure

**• 特殊对待新用户、低活用户等特殊人群。**
- **Special treatment for new users, low-activity users, and other special groups**
- Implement targeted strategies for different user segments
- Address cold start problems and user engagement challenges

**• 利用关注、转发、评论这三种交互行为。**
- **Leverage three interaction behaviors: follow, share, and comment**
- Incorporate diverse user engagement signals beyond basic interactions
- Utilize social behaviors for better recommendation understanding

### Strategic Focus:
The slide emphasizes a comprehensive approach to metric improvement, covering the entire recommendation pipeline from recall through final ranking, while also addressing user diversity and engagement patterns. The highlighted items suggest these are the primary focus areas for the presentation.# Ranking Models

## 排序模型 (Ranking Models)

### Table of Contents:

**1. 精排模型的改进**
   - **Improvements to Fine Ranking Models**
   - Enhancements to detailed ranking algorithms and architectures

**2. 粗排模型的改进**
   - **Improvements to Coarse Ranking Models**
   - Optimizations for preliminary ranking and filtering stages

**3. 用户行为序列建模**
   - **User Behavior Sequence Modeling**
   - Techniques for modeling temporal patterns in user interactions

**4. 在线学习**
   - **Online Learning**
   - Real-time adaptation and continuous model updates

**5. 老汤模型**
   - **Old Soup Models**
   - Likely referring to ensemble or traditional model approaches

### Presentation Structure:
This outline covers a comprehensive approach to ranking model improvements in recommendation systems:

#### Core Components:
- **Two-Stage Ranking**: Both coarse and fine ranking optimizations
- **Sequential Modeling**: Understanding user behavior patterns over time
- **Adaptive Learning**: Real-time model improvements
- **Ensemble Strategies**: Combining multiple model approaches

#### Focus Areas:
The presentation will systematically cover improvements across the entire ranking pipeline, from initial filtering through final item ordering, incorporating both static improvements and dynamic learning capabilities.

This structure suggests a thorough exploration of ranking model enhancements with practical implementation considerations for large-scale recommendation systems.# Improvements to Fine Ranking Models

## 精排模型的改进 (Improvements to Fine Ranking Models)

### Section Introduction

This slide serves as a section header introducing improvements to fine ranking models in recommendation systems.

### Context:
Fine ranking models represent the final, most detailed ranking stage in recommendation systems where:

#### Key Characteristics:
- **Highest Precision**: Most sophisticated and computationally intensive ranking
- **Rich Features**: Can utilize extensive feature sets and complex interactions
- **Final Ordering**: Determines the ultimate presentation order to users
- **Performance Critical**: Directly impacts user engagement metrics

#### Expected Improvements:
The following slides will likely cover:

1. **Advanced Neural Architectures**: 
   - Deep learning models for ranking
   - Attention mechanisms and transformers
   - Multi-task learning approaches

2. **Feature Engineering**:
   - Cross-feature interactions
   - Embedding techniques
   - Feature selection strategies

3. **Training Methodologies**:
   - Loss function optimization
   - Regularization techniques
   - Online learning adaptations

4. **Multi-Objective Optimization**:
   - Balancing different engagement metrics
   - Trade-offs between accuracy and diversity
   - Long-term value optimization

This section focuses on the most sophisticated ranking stage where computational resources can be allocated for maximum model complexity and prediction accuracy.# Multi-Task Fine Ranking Architecture

## Neural Network Architecture for Fine Ranking

### Architecture Components:

#### Multi-Task Prediction Heads:
- **预估点击率 (Click Rate Prediction)**: CTR estimation
- **预估点赞率 (Like Rate Prediction)**: Engagement prediction  
- **预估转发率 (Share Rate Prediction)**: Viral potential estimation
- **预估评论率 (Comment Rate Prediction)**: Discussion engagement

#### Network Structure:
- **全连接网络 (Fully Connected Network)**: Dense layers for feature interaction
- **Concatenation Layer**: Feature fusion across different modalities
- **Embedding Layers**: Dense representations for categorical features

#### Feature Types:
- **离散特征 (Discrete Features)**: Categorical variables (shown as green circles)
- **连续特征 (Continuous Features)**: Numerical variables (shown as blue rectangles)

### Technical Architecture:
1. **Feature Processing**: 
   - Discrete features → Embedding layers
   - Continuous features → Direct input
   
2. **Feature Combination**:
   - Embedding + Fully Connected Network processing for discrete features
   - Direct fully connected processing for continuous features
   
3. **Multi-Task Learning**:
   - Shared representation learning
   - Task-specific prediction heads
   - Joint optimization across multiple objectives

### Key Benefits:
- **Shared Learning**: Common patterns across different engagement types
- **Task Regularization**: Prevents overfitting to single metrics
- **Efficient Architecture**: Shared computation for multiple predictions
- **Production Ready**: Scalable for real-time inference

This architecture enables simultaneous optimization for multiple engagement metrics while sharing computational resources and learned representations.# Enhanced Multi-Task Architecture with Feature Processing

## 基座 (Foundation) Architecture

### Advanced Feature Processing Pipeline:

#### Highlighted Enhancement (Red Box):
The **基座 (Foundation/Base)** component represents a sophisticated feature processing layer that serves as the foundation for multi-task learning.

#### Architecture Details:

**Left Side - Discrete Feature Processing**:
- **离散特征 (Discrete Features)**: Categorical inputs
- **Embedding + 全连接网络**: Embedding layer + Fully Connected Network
- Advanced feature transformation and interaction learning

**Right Side - Continuous Feature Processing**:
- **连续特征 (Continuous Features)**: Numerical inputs  
- **全连接网络**: Direct fully connected processing
- Efficient handling of continuous variables

#### Foundation Layer Benefits:
1. **Unified Feature Representation**: Harmonizes discrete and continuous features
2. **Shared Knowledge Base**: Common feature interactions across tasks
3. **Scalable Processing**: Efficient computation for large feature sets
4. **Transfer Learning**: Shared representations improve individual task performance

#### Multi-Task Integration:
- **Shared Foundation**: Common feature processing across all prediction tasks
- **Task-Specific Heads**: Specialized outputs for different engagement metrics
- **Joint Optimization**: Simultaneous learning across multiple objectives
- **Cross-Task Regularization**: Improved generalization through multi-task constraints

### Technical Innovation:
The **基座** approach represents a sophisticated foundation architecture that enables efficient multi-task learning while maintaining computational efficiency and model interpretability. This design pattern is crucial for production recommendation systems requiring multiple simultaneous predictions.# 精排模型架构图

预估点击率 → 预估点赞率 → 预估转发率 → 预估评论率
   ↑              ↑              ↑              ↑
全连接网络    全连接网络    全连接网络    ···   全连接网络
   ↑              ↑              ↑              ↑

concatenation

绿色方块序列 ←─→ 蓝色方块序列
     ↑                    ↑

Embedding + 全连接网络     全连接网络
     ↑                    ↑
     
● ● ● ● ● ● ● ●      蓝色方块序列
     
离散特征                连续特征# 精排模型：基座

• 基座的输入包括离散特征和连续特征，输出一个向量，作为多目标预估的输入。

• 改进1：基座加宽加深，计算量更大，预测更准确。# 精排模型：基座

• 基座的输入包括离散特征和连续特征，输出一个向量，作为多目标预估的输入。

• 改进1：基座加宽加深，计算量更大，预测更准确。

• 改进2：做自动的特征交叉，比如 bilinear [1] 和 LHUC [2]。

• 改进3：特征工程，比如添加统计特征、多模态内容特征。

## 参考文献

1. Huang et al. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In RecSys, 2019.

2. Swietojanski et al. Learning hidden unit contributions for unsupervised acoustic model adaptation. In WSDM, 2016.# 精排模型多目标架构图（带红色框）

预估点击率 → 预估点赞率 → 预估转发率 → 预估评论率
   ↑              ↑              ↑              ↑
全连接网络    全连接网络    全连接网络    ···   全连接网络
   ↑              ↑              ↑              ↑

concatenation

绿色方块序列 ←─→ 蓝色方块序列
     ↑                    ↑

Embedding + 全连接网络     全连接网络
     ↑                    ↑
     
● ● ● ● ● ● ● ●      蓝色方块序列
     
离散特征                连续特征

注：红色框标出多目标预估部分# 精排模型：多目标预估

• 基于基座输出的向量，同时预估点击率等多个目标。

• 改进1：增加新的预估目标，并把预估结果加入融合公式。

  • 最标准的目标包括点击率、点赞率、收藏率、转发率、评论率、关注率、完播率......

  • 寻找更多目标，比如进入评论区、给他人写的评论点赞......

  • 把新的预估目标加入融合公式。# 精排模型：多目标预估

• 基于基座输出的向量，同时预估点击率等多个目标。

• 改进1：增加新的预估目标，并把预估结果加入融合公式。

• 改进2：MMoE [1]、PLE [2] 等结构可能有效，但往往无效。

• 改进3：纠正 position bias [3] 可能有效，也可能无效。

## 参考文献

1. Ma et al. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In KDD, 2018.

2. Tang et al. Progressive layered extraction (PLE): A novel multi-task learning (MTL) model for personalized recommendations. In RecSys, 2020.

3. Zhou et al. Recommending what video to watch next: a multitask ranking system. In RecSys, 2019.# 粗排模型的改进# 粗排模型

• 粗排的打分量比精排大 10 倍，因此粗排模型必须够快。

• 简单模型：多向量双塔模型，同时预估点击率等多个目标。

• 复杂模型：三塔模型 [1] 效果好，但工程实现难度较大。

## 参考文献

1. Wang et al. COLD: towards the next generation of pre-ranking system. arXiv, 2020.# 粗精排一致性建模

• 蒸馏精排训练粗排，让粗排与精排更一致。

• 方法1：pointwise 蒸馏。

  • 设 y 是用户真实行为，设 p 是精排的预估。

  • 用 (y+p)/2 作为粗排拟合的目标。

  • 例：

    • 对于点击率目标，用户有点击（y = 1），精排预估 p = 0.6。

    • 用 (y+p)/2 = 0.8 作为粗排拟合的点击率目标。# 粗精排一致性建模

• 蒸馏精排训练粗排，让粗排与精排更一致。

• 方法1：pointwise 蒸馏。

• 方法2：pairwise 或 listwise 蒸馏。

  • 给定 k 个候选物品，按照精排预估做排序。

  • 做 learning to rank (LTR)，让粗排拟合物品的序（而非值）。

  • 例：

    • 对于物品 i 和 j，精排预估点击率为 pi > pj。

    • LTR 鼓励粗排预估点击率满足 qi > qj，否则有惩罚。

    • LTR 通常使用 pairwise logistic loss。# 粗精排一致性建模

• 蒸馏精排训练粗排，让粗排与精排更一致。

• 方法1：pointwise 蒸馏。

• 方法2：pairwise 或 listwise 蒸馏。

• 优点：粗精排一致性建模可以提升核心指标。

• 缺点：如果精排出bug，精排预估值 p 有偏，会污染粗排训练数据。# 用户行为序列建模# 用户行为序列建模架构图

向量：
[红色方块] → [红色方块] → [红色方块] → [红色方块] → [红色方块] → ... → [红色方块]
     ↑            ↑            ↑            ↑            ↑       Embedding      ↑
     
物品 ID：
[视频图标]   [视频图标]   [视频图标]   [视频图标]   [视频图标]   ...      [视频图标]

                                          ↑ 平均
                                    
                                    [红色方块]（最终输出向量）# 用户行为序列建模

• 最简单的方法是对物品向量取平均，作为一种用户特征 [1]。

• DIN [2] 使用注意力机制，对物品向量做加权平均。

• 工业界目前沿着 SIM [3] 的方向发展。先用类目等属性筛选物品，然后用 DIN 对物品向量做加权平均。

## 参考文献

1. Covington, Adams, and Sargin. Deep neural networks for YouTube recommendations. In RecSys, 2016.

2. Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.

3. Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020.# 用户行为序列建模

• 改进1：增加序列长度，让预测更准确，但是会增加计算成本和推理时间。

• 改进2：筛选的方法，比如用类目、物品向量表征聚类。

  • 离线用多模态神经网络提取物品内容特征，将物品表征为向量。

  • 离线将物品向量聚为 1000 类，每个物品有一个聚类序号。

  • 线上排序时，用户行为序列中有 n = 1,000,000 个物品。某候选物品的聚类序号是 70，对 n 个物品做筛选，只保留聚类序号为 70 的物品。n 个物品中只有数千个被保留下来。

  • 同时有好几种筛选方法，取筛选结果的并集。# 用户行为序列建模

• 改进1：增加序列长度，让预测更准确，但是会增加计算成本和推理时间。

• 改进2：筛选的方法，比如用类目、物品向量表征聚类。

• 改进3：对用户行为序列中的物品，使用 ID 以外的一些特征。

• 概括：沿着 SIM 的方向发展，让原始的序列尽量长，然后做筛选降低序列长度，最后将筛选结果输入 DIN。# 在线学习# 全量更新 vs 增量更新

基于前天的全量模型，用前天的数据，做全量更新。

前天的数据 ——→ 昨天凌晨
                   ↓ （虚线箭头）
                   ↓
                   做增量更新

[橙色圆圈] ——→ [橙色圆圈] ——→ ... ——→ [橙色圆圈]# 全量更新 vs 增量更新

基于昨天的全量模型，用昨天的数据，做全量更新。

前天的数据 ——→ 昨天凌晨 —— 昨天的数据 ——→ 今天凌晨
                   ↓                          ↓ （虚线箭头）
                   ↓                          ↓
[橙色圆圈] ——→ [橙色圆圈] ——→ ... ——→ [红色X] [橙色圆圈] ——→ [橙色圆圈] ——→ ... ——→ [橙色圆圈]

                                          做增量更新# 在线学习的资源消耗

• 既需要在凌晨做全量更新，也需要全天不间断做增量更新。

• 设在线学习需要 10,000 CPU core 的算力增量更新一个精排模型。推荐系统一共需要多少额外的算力给在线学习？

• 为了做 AB 测试，线上同时运行多个不同的模型。

• 如果线上有 m 个模型，则需要 m 套在线学习的机器。

• 线上有 m 个模型，其中 1 个是 holdout，1 个是推全的模型，m - 2 个测试的新模型。# 在线学习的资源消耗

图表说明：
- 左侧：10% 用户使用 holdout 模型
- 右侧：90% 用户分别使用不同模型：
  - 召回
  - 粗排
  - 精排 
  - 重排# 在线学习的资源消耗

图表说明（细分精排模块）：
- 左侧：10% 用户使用 holdout 模型
- 右侧：90% 用户的精排模块进一步分为：
  - 推全模型
  - 新模型#1
  - 新模型#2# 在线学习的资源消耗

• 线上有 m 个模型，其中 1 个是 holdout，1 个是推全的模型，m - 2 个测试的新模型。

• 每套在线学习的机器成本都很大，因此 m 数量很小，制约模型开发迭代的效率。

• 在线学习对指标的提升巨大，但是会制约模型开发迭代的效率。# 老汤模型# 老汤模型

• 用每天新产生的数据对模型做 1 epoch 的训练。

• 久而久之，老模型训练得非常好，很难被超过。

• 对模型做改进，重新训练，很难追上老模型......

• 问题1：如何快速判断新模型结构是否优于老模型？（不需要追上线上的老模型，只需要判断新老模型谁的结构更优。）

• 问题2：如何更快追平、超过线上的老模型？（只有几十天的数据，新模型就能追上训练上百天的老模型。）# 老汤模型

## 问题1：如何快速判断新模型结构是否优于老模型？

• 对于新、老模型结构，都随机初始化模型全连接层。

• Embedding 层可以是随机初始化，也可以是复用老模型训练好的参数。

• 用 n 天的数据训练新老模型。（从旧到新，训练 1 epoch）

• 如果新模型显著优于老模型，新模型很可能更优。

• 只是比较新老模型结构谁更好，而非真正追平老模型。# 老汤模型

## 问题2：如何更快追平线上的老模型？

• 已经得出初步结论，认为新模型很可能优于老模型。用几十天的数据训练新模型，早日追平老模型。

• 方法1：尽可能多地复用老模型训练好的 embedding 层，避免随机初始化 embedding 层。（Embedding 层是对用户、物品特点的"记忆"，比全连接层学得慢。）

• 方法2：用老模型做 teacher，蒸馏新模型。（用户真实行为是 y，老模型的预测是 p，用 (y+p)/2 作为训练新模型的目标。）# 总结：改进排序模型

• 精排模型：改进模型基座（加宽加深、特征交叉、特征工程），改进多目标预估（增加新目标、MMoE、position bias）。

• 粗排模型：三塔模型（取代多向量双塔模型），粗精排一致性建模。

• 用户行为序列建模：沿着 SIM 的方向迭代升级，加长序列长度，改进筛选物品的方法。

• 在线学习：对指标提升大，但是会降低模型迭代升级效率。

• 老汤模型制约模型迭代升级效率，需要特殊技巧。# Thank You!

http://wangshusen.github.io/# Methods for Improving Metrics: Enhancing Diversity

## 涨指标的方法：提升多样性 (Methods for Improving Metrics: Enhancing Diversity)

### Course Title and Instructor
**王树森 (Wang Shusen)**

### Course Website
**http://wangshusen.github.io/**

### Focus Area: Diversity Enhancement

This presentation focuses on methods to improve recommendation system performance through diversity enhancements across the entire recommendation pipeline.

### Key Topics Expected:

#### Diversity Strategies:
1. **Recall Diversity**: Ensuring varied candidate items from different sources
2. **Ranking Diversity**: Balancing relevance with variety in final rankings
3. **Content Diversity**: Different types, genres, and categories of items
4. **Temporal Diversity**: Mixing fresh content with established favorites
5. **Exploration vs Exploitation**: Balancing familiar preferences with discovery

#### Technical Approaches:
- **Multi-Objective Optimization**: Trading off accuracy for diversity
- **Diversity-Aware Ranking**: Algorithms that explicitly consider variety
- **Portfolio Optimization**: Mathematical approaches to diverse item selection
- **Filter Bubble Mitigation**: Preventing echo chambers and expanding user horizons
- **Serendipity Enhancement**: Introducing pleasant surprises in recommendations

#### Business Impact:
- **User Engagement**: Diverse content keeps users interested longer
- **Platform Health**: Prevents over-concentration on popular items
- **Creator Economy**: Helps surface content from diverse creators
- **Long-term Value**: Builds sustainable user satisfaction

Diversity is crucial for recommendation system health, user satisfaction, and preventing the filter bubble effect that can reduce long-term engagement.# 涨指标的方法有哪些？

• 改进召回模型，添加新的召回模型。

• 改进粗排和精排模型。

➡️ • 提升召回、粗排、精排中的多样性。

• 特殊对待新用户、低活用户等特殊人群。

• 利用关注、转发、评论这三种交互行为。# 排序的多样性# 精排多样性

• 精排阶段，结合兴趣分数和多样性分数对物品 i 排序。

  • si：兴趣分数，即融合点击率等多个预估目标。
  
  • di：多样性分数，即物品 i 与已经选中的物品的差异。
  
• 用 si + di 对物品做排序。# 精排多样性

• 精排阶段，结合兴趣分数和多样性分数对物品 i 排序。

• 常用 MMR、DPP 等方法计算多样性分数，精排使用滑动窗口，粗排不使用滑动窗口。

  • 精排决定最终的曝光，曝光页面上邻近的物品相似度应该小。所以计算精排多样性要使用滑动窗口。
  
  • 粗排要考虑整体的多样性，而非一个滑动窗口中的多样性。# 精排多样性

• 精排阶段，结合兴趣分数和多样性分数对物品 i 排序。

• 常用 MMR、DPP 等方法计算多样性分数，精排使用滑动窗口，粗排不使用滑动窗口。

• 除了多样性分数，精排还使用打散策略增加多样性。

  • 类目：当前选中物品 i，之后 5 个位置不允许跟 i 的二级类目相同。
  
  • 多模态：事先计算物品多模态内容向量表征，将全库物品聚为 1000 类；在精排阶段，如果当前选中物品 i，之后 10 个位置不允许跟 i 同属一个聚类。粗排多样性

• 粗排给5000个物品打分，选出500个物品送入精排。

• 提升粗排和精排多样性都可以提升推荐系统核心指标。

• 根据si对5000个物品排序，分数最高的200个物品送入精排。

• 对于剩余的4800个物品，对每个物品i计算兴趣分数si和多样性分数di。

• 根据si + di对剩余4800个物品排序，分数最高的300个物品送入精排。召回的多样性双塔模型：添加噪声

• 用户塔将用户特征作为输入，输出用户的向量表征；然后做ANN检索，召回向量相似度高的物品。

• 线上做召回时（在计算出用户向量之后，在做ANN检索之前），在用户向量中添加随机噪声。

• 用户的兴趣越窄（比如用户最近交互的n个物品只覆盖少数几个类目），则添加的噪声越强。

• 添加噪声使得召回的物品更多样，可以提升推荐系统核心指标。双塔模型：抽样用户行为序列

• 用户最近交互的n个物品（用户行为序列）是用户塔的输入。

• 保留最近的r个物品（r << n）。

• 从剩余的n - r个物品中随机抽样t个物品（t << n）。（可以是均匀抽样，也可以用非均匀抽样让类目平衡。）

• 将得到的r + t个物品作为用户行为序列，而不是用全部n个物品。

• 抽样用户行为序列为什么能涨指标？
  • 一方面，注入随机性，召回结果更多样化。
  • 另一方面，n可以非常大，可以利用到用户很久之前的兴趣。U2I2I：抽样用户行为序列

• U2I2I（user → item → item）中的第一个item是指用户最近交互的n个物品之一，在U2I2I中叫作种子物品。

• n个物品覆盖的类目数较少，且类目不平衡。

  • 系统共有200个类目，某用户的n个物品只覆盖15个类目。
  • 足球类目的物品有0.4n个，电视剧类目的物品有0.2n个，其余类目的物品数均少于0.05n个。U2I2I：抽样用户行为序列

• U2I2I（user → item → item）中的第一个item是指用户最近交互的n个物品之一，在U2I2I中叫作种子物品。

• n个物品覆盖的类目数较少，且类目不平衡。

• 做非均匀随机抽样，从n个物品中选出t个，让类目平衡。（想法和效果与双塔中的用户行为序列抽样相似。）

• 用抽样得到的t个物品（代替原本的n个物品）作为U2I2I的种子物品。

• 一方面，类目更平衡，多样性更好。另一方面，n可以更大，覆盖的类目更多。探索流量

• 每个用户曝光的物品中有2%是非个性化的，用作兴趣探索。

• 维护一个精选内容池，其中物品均为交互率指标高的优质物品。（内容池可以分人群，比如30~40岁男性内容池。）

• 从精选内容池中随机抽样几个物品，跳过排序，直接插入最终排序结果。

• 兴趣探索在短期内负向影响核心指标，但长期会产生正向影响。总结：提升多样性

• 精排：结合兴趣分数和多样性分数做排序；做规则打散。

• 粗排：只用兴趣分数选出部分物品；结合兴趣分数和多样性分数选出部分物品。

• 召回：往双塔模型的用户向量添加噪声；对用户行为序列做非均匀随机抽样（对双塔和U2I2I都适用）。

• 兴趣探索：保留少部分的流量给非个性化推荐。Thank You!

http://wangshusen.github.io/# Methods for Improving Metrics: Special Treatment for Special User Groups

## 涨指标的方法：特殊对待特殊人群 (Methods for Improving Metrics: Special Treatment for Special User Groups)

### Course Title and Instructor
**王树森 (Wang Shusen)**

### Course Website
**http://wangshusen.github.io/**

### Focus Area: User Segmentation and Personalization

This presentation focuses on methods to improve recommendation system performance through specialized treatment of different user segments, particularly addressing the needs of special user groups.

### Key User Groups and Challenges:

#### Special User Segments:
1. **New Users (Cold Start)**:
   - Limited interaction history
   - Unknown preferences and behavior patterns
   - Need for quick preference discovery

2. **Low-Activity Users**:
   - Sparse interaction data
   - Irregular usage patterns
   - Risk of churn due to poor recommendations

3. **High-Activity Power Users**:
   - Rich interaction history
   - Complex and evolving preferences
   - Need for sophisticated personalization

4. **Returning Users**:
   - After periods of inactivity
   - Changed preferences
   - Re-engagement challenges

### Technical Strategies Expected:

#### Cold Start Solutions:
- **Content-Based Bootstrapping**: Using item attributes for initial recommendations
- **Demographic-Based Targeting**: Leveraging user profile information
- **Popular Item Fallback**: Safe defaults for unknown preferences
- **Active Learning**: Strategic questioning to quickly learn preferences

#### Low-Activity User Optimization:
- **Simplified Models**: Less complex algorithms for sparse data
- **Transfer Learning**: Borrowing patterns from similar users
- **Engagement-First Recommendations**: Prioritizing items likely to generate interactions

#### Advanced Personalization:
- **Multi-Armed Bandits**: Exploration-exploitation for preference discovery
- **Contextual Recommendations**: Time, location, and situation-aware suggestions
- **Preference Evolution**: Tracking and adapting to changing user interests

This specialized approach ensures that recommendation systems work effectively for all user types, not just the average user.涨指标的方法有哪些？

• 改进召回模型，添加新的召回模型。

• 改进粗排和精排模型。

• 提升召回、粗排、精排中的多样性。

• 特殊对待新用户、低活用户等特殊人群。

• 利用关注、转发、评论这三种交互行为。为什么要特殊对待特殊人群？

1. 新用户、低活用户的行为很少，个性化推荐不准确。

2. 新用户、低活用户容易流失，要想办法促使他们留存。

3. 特殊用户的行为（比如点击率、交互率）不同于主流用户，基于全体用户行为训练出的模型在特殊用户人群上有偏。涨指标的方法

1. 构造特殊内容池，用于特殊用户人群的召回。

2. 使用特殊排序策略，保护特殊用户。

3. 使用特殊的排序模型，消除模型预估的偏差。构造特殊的内容池特殊内容池

• 为什么需要特殊内容池？

• 新用户、低活用户的行为很少，个性化召回不准确。（既然个性化不好，那么就保证内容质量好。）

• 针对特定人群的特点构造特殊内容池，提升用户满意度。（例如，对于喜欢留评论的中年女性，构造促评论内容池，满足这些用户的互动需求。）如何构造特殊内容池

• 方法1：根据物品获得的交互次数、交互率选择优质物品。
  • 圈定人群：只考虑特定人群，例如18~25岁一二线城市男性。
  • 构造内容池：用该人群对物品的交互次数、交互率给物品打分，选出分数最高的物品进入内容池。
  • 内容池有弱个性化的效果。
  • 内容池定期更新，加入新物品，排除交互率低和失去时效性的老物品。
  • 该内容池只对该人群生效。如何构造特殊内容池

• 方法1：根据物品获得的交互次数、交互率选择优质物品。

• 方法2：做因果推断，判断物品对人群留存率的贡献，根据贡献值选物品。特殊内容池的召回

• 通常使用双塔模型从特殊内容池中做召回。
  • 双塔模型是个性化的。
  • 对于新用户，双塔模型的个性化做不准。
  • 靠高质量内容、弱个性化做补偿。特殊内容池的召回

• 通常使用双塔模型从特殊内容池中做召回。

• 额外的训练代价？
  • 对于正常用户，不论有多少内容池，只训练一个双塔模型。
  • 对于新用户，由于历史交互记录很少，需要单独训练模型。特殊内容池的召回

• 通常使用双塔模型从特殊内容池中做召回。

• 额外的训练代价？

• 额外的推理代价？
  • 内容池定期更新，然后要更新ANN索引。
  • 线上做召回时，需要做ANN检索。
  • 特殊内容池都很小（比全量内容池小10~100倍），所以需要的额外算力不大。特殊的排序策略排除低质量物品

• 对于新用户、低活用户这样的特殊人群，业务上只关注留存，不在乎消费（总曝光量、广告收入、电商收入）。

• 对于新用户、低活用户，少出广告、甚至不出广告。

• 新发布的物品不在新用户、低活用户上做探索。
  • 物品新发布时，推荐做得不准，会损害用户体验。
  • 只在活跃的老用户上做探索，对新物品提权（boost）。
  • 不在新用户、低活用户上做探索，避免伤害用户体验。差异化的融分公式

• 新用户、低活用户的点击、交互行为不同于正常用户。

• 低活用户的人均点击量很小；没有点击就不会有进一步的交互。

• 低活用户的融分公式中，提高预估点击率的权重（相较于普通用户）。

• 保留几个曝光坑位给预估点击率最高的几个物品。
  • 例：精排从500个物品中选50个作为推荐结果，其中3个坑位给点击率最高的物品，剩余47个坑位由融分公式决定。
  • 甚至可以把点击率最高的物品排在第一，确保用户一定能看到。特殊的排序模型差异化的排序模型

• 特殊用户人群的行为不同于普通用户。新用户、低活用户的点击率、交互率偏高或偏低。

• 排序模型被主流用户主导，对特殊用户做不准预估。
  • 用全体用户数据训练出的模型，给新用户做的预估有严重偏差。
  • 如果一个APP的用90%是女性，用全体用户数据训练出的模型，对男性用户做的预估有偏差。

• 问题：对于特殊用户，如何让排序模型预估做得准？差异化的排序模型

• 方法1：大模型 + 小模型。
  • 用全体用户行为训练大模型，大模型的预估p拟合用户行为y。
  • 用特殊用户的行为训练小模型，小模型的预估q拟合大模型的残差y - p。
  • 对主流用户只用大模型做预估p。
  • 对特殊用户，结合大模型和小模型的预估p + q。差异化的排序模型

• 方法1：大模型 + 小模型。

• 方法2：融合多个experts，类似MMoE。
  • 只用一个模型，模型有多个experts，各输出一个向量。
  • 对experts的输出做加权平均。
  • 根据用户特征计算权重。
  • 以新用户为例，模型将用户的新老、活跃度等特征作为输入，输出权重，用于对experts做加权平均。差异化的排序模型

• 方法1：大模型 + 小模型。

• 方法2：融合多个experts，类似MMoE。

• 方法3：大模型预估之后，用小模型做校准。
  • 用大模型预估点击率、交互率。
  • 将用户特征、大模型预估点击率和交互率作为小模型（例如GBDT）的输入。
  • 在特殊用户人群的数据上训练小模型，小模型的输出拟合用户真实行为。错误的做法

• 每个用户人群使用一个排序模型，推荐系统同时维护多个大模型。
  • 系统有一个主模型；每个用户人群有自己的一个模型。
  • 每天凌晨，用全体用户数据更新主模型。
  • 基于训练好的主模型，在某特殊用户人群的数据上再训练1 epoch，作为该用户人群的模型。错误的做法

• 每个用户人群使用一个排序模型，推荐系统同时维护多个大模型。

• 短期可以提升指标；维护代价大，长期有害。
  • 起初，低活男性用户模型比主模型的AUC高0.2%。
  • 主模型迭代几个版本后，AUC累计提升0.5%。
  • 特殊人群模型太多，长期没有人维护和更新。
  • 如果把低活男性用户模型下线，换成主模型，在低活男性用户上的AUC反倒提升0.3%！总结：特殊对待特殊用户人群

• 召回：针对特殊用户人群，构造特殊的内容池，增加相应的召回通道。

• 排序策略：排除低质量物品，保护新用户和低活用户；特殊用户人群使用特殊的融分公式。

• 排序模型：结合大模型和小模型，小模型拟合大模型的残差；只用一个模型，模型有多个experts；大模型预估之后，用小模型做校准。Thank You!

http://wangshusen.github.io/# Methods for Improving Metrics: Leveraging Interactive Behaviors

## 涨指标的方法：利用交互行为 (Methods for Improving Metrics: Leveraging Interactive Behaviors)

### Course Title and Instructor
**王树森 (Wang Shusen)**

### Course Website
**http://wangshusen.github.io/**

### Focus Area: Multi-Modal Interaction Behavior Analysis

This presentation focuses on methods to improve recommendation system performance by effectively leveraging various types of user interaction behaviors beyond basic clicks and views.

### Key Interaction Behaviors:

#### Primary Interaction Types:
1. **关注 (Follow/Subscribe)**:
   - Strong positive signal indicating user interest
   - Long-term engagement indicator
   - Creator-user relationship establishment

2. **转发 (Share/Retweet)**:
   - High-value engagement indicating content resonance
   - Social amplification behavior
   - Quality and relevance signal

3. **评论 (Comment)**:
   - Deep engagement with content
   - Opinion and sentiment expression
   - Community participation indicator

### Technical Implementation Strategies:

#### Multi-Signal Integration:
- **Weighted Feedback**: Different interaction types with varying importance weights
- **Implicit vs Explicit Signals**: Balancing direct actions with inferred preferences
- **Temporal Patterns**: Understanding interaction timing and frequency
- **Cross-Platform Behavior**: Integrating behaviors across different interfaces

#### Advanced Modeling Approaches:
- **Multi-Task Learning**: Simultaneous prediction of different interaction types
- **Behavioral Sequence Modeling**: Understanding interaction progression patterns
- **Social Signal Processing**: Leveraging network effects and viral behaviors
- **Sentiment Analysis**: Extracting emotional context from comments and shares

#### Business Applications:
- **Content Quality Assessment**: Using engagement depth as quality indicators
- **Creator Recommendation**: Suggesting users to follow based on interaction patterns  
- **Viral Content Prediction**: Identifying content likely to be shared
- **Community Building**: Fostering discussion and interaction through recommendations

This comprehensive approach to interaction behavior analysis enables more nuanced and effective recommendation systems that understand the full spectrum of user engagement.涨指标的方法有哪些？

• 改进召回模型，添加新的召回模型。

• 改进粗排和精排模型。

• 提升召回、粗排、精排中的多样性。

• 特殊对待新用户、低活用户等特殊人群。

• 利用关注、转发、评论这三种交互行为。用户的交互行为

• 交互行为：点赞、收藏、转发、关注、评论……

• 推荐系统如何利用交互行为？

• 最简单的方法：将模型预估的交互率用于排序。
  • 模型将交互行为当做预估的目标。
  • 将预估的点击率、交互率做融合，作为排序的依据。

• 交互行为有没有其他用途？关注关注量对留存的价值

• 对于一位用户，他关注的作者越多，则平台对他的吸引力越强。

• 用户留存率（r）与他关注的作者数量（f）正相关。关注量对留存的价值

• 对于一位用户，他关注的作者越多，则平台对他的吸引力越强。

• 用户留存率（r）与他关注的作者数量（f）正相关。

• 如果某用户的f较小，则推荐系统要促使该用户关注更多作者。

[图表显示留存率(r)随关注作者数量(f)增加而上升的曲线]关注量对留存的价值

• 如何利用关注关系提升用户留存？

• 方法1：用排序策略提升关注量。
  • 对于用户u，模型预估候选物品i的关注率为pi。
  • 设用户u已经关注了f个作者。
  • 我们定义单调递减函数w(f)，用户已经关注的作者越多，则w(f)越小。
  • 在排序融分公式中添加w(f)·pi，用于促关注。（如果f小且pi大，则w(f)·pi给物品i带来很大加分。）关注量对留存的价值

• 如何利用关注关系提升用户留存？

• 方法1：用排序策略提升关注量。

• 方法2：构造促关注内容池和召回通道。
  • 这个内容池中物品的关注率高，可以促关注。
  • 如果用户关注的作者数f较小，则对该用户使用该内容池。
  • 召回配额可以固定，也可以与f负相关。粉丝数对促发布的价值

• UGC平台将作者发布量、发布率作为核心指标，希望作者多发布。

• 作者发布的物品被平台推送给用户，会产生点赞、评论、关注等交互。

• 交互（尤其是关注、评论）可以提升作者发布积极性。

• 作者的粉丝数越少，则每增加一个粉丝对发布积极性的提升越大。粉丝数对促发布的价值

• 用排序策略帮助低粉新作者涨粉。

• 某作者a的粉丝数（被关注数）为fa。

• 作者a发布的物品i可能被推荐给用户u，模型预估关注率为pui。

• 我们定义单调递减函数w(fa)作为权重；作者a的粉丝越多，则w(fa)越小。

• 在排序融分公式中添加w(fa)·pui，帮助低粉作者涨粉。隐式关注关系

• 召回通道U2A2I：user → author → item。

• 显式关注关系：用户u关注了作者a，将a发布的物品推荐给u。（点击率、交互率指标通常高于其他召回通道。）

• 隐式关注关系：用户u喜欢看作者a发布的物品，但是u没有关注a。

• 隐式关注的作者数量远大于显式关注。挖掘隐式关注关系，构造U2A2I召回通道，可以提升推荐系统核心指标。转发（分享）促转发（分享回流）

• A平台用户将物品转发到B平台，可以为A吸引站外流量。

• 推荐系统做促转发（也叫分享回流）可以提升DAU和消费指标。促转发（分享回流）

• A平台用户将物品转发到B平台，可以为A吸引站外流量。

• 推荐系统做促转发（也叫分享回流）可以提升DAU和消费指标。

• 简单提升转发次数是否有效呢？
  • 模型预估转发率为p，融分公式中有一项w·p，让转发率大的物品更容易获得曝光机会。
  • 增大权重w可以促转发，吸引站外流量，但是会负面影响点击率和其他交互率。KOL建模

• 目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。

• 什么样的用户的转发可以吸引大量站外流量？
  • 答案：其他平台的Key Opinion Leader (KOL)。KOL建模

• 目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。

• 什么样的用户的转发可以吸引大量站外流量？
  • 答案：其他平台的Key Opinion Leader (KOL)。
  • 用户u是我们站内的KOL，但他不是其他平台的KOL，他的转发价值大吗？
  • 用户v在我们站内没有粉丝，但他是其他平台的KOL，他的转发价值大吗？KOL建模

• 目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。

• 什么样的用户的转发可以吸引大量站外流量？其他平台的Key Opinion Leader (KOL)！

• 如何判断本平台的用户是不是其他平台的KOL？

• 该用户历史上的转发能带来多少站外流量。促转发的策略

• 目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。

• 识别出的站外KOL之后，该如何用于排序和召回？

• 方法1：排序融分公式中添加额外的一项ku · pui。
  • ku：如果用户u是站外KOL，则ku大。
  • pui：为用户u推荐物品i，模型预估的转发率。
• 如果u是站外KOL，则多给他曝光他可能转发的物品。促转发的策略

• 目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。

• 识别出的站外KOL之后，该如何用于排序和召回？

• 方法1：排序融分公式中添加额外的一项ku · pui。

• 方法2：构造促转发内容池和召回通道，对站外KOL生效。评论评论促发布

• UGC平台将作者发布量、发布率作为核心指标，希望作者多发布。

• 关注、评论等交互可以提升作者发布积极性。

• 如果新发布物品尚未获得很多评论，则给预估评论率提权，让物品尽快获得评论。

• 排序融分公式中添加额外一项wi · pi。
  • wi：权重，与物品i已有的评论数量负相关。
  • pi：为用户推荐物品i，模型预估的评论率。评论的其他价值

• 有的用户喜欢留评论，喜欢跟作者、评论区用户互动。
  • 给这样的用户添加促评论的内容池，让他们更多机会参与讨论。
  • 有利于提升这些用户的留存。

• 有的用户常留高质量评论（评论的点赞量高）。
  • 高质量评论对作者、其他用户的留存有贡献。（作者、其他用户觉得这样的评论有趣或有帮助。）
  • 用排序和召回策略鼓励这些用户多留评论。总结：利用交互行为

• 关注：留存价值（让新用户关注更多作者，提升新用户留存）；发布价值（帮助新作者获得更多粉丝，提升作者发布积极性）；利用隐式关注关系做召回。

• 转发：判断哪些用户是站外的KOL，利用他们转发的价值，吸引站外的流量。

• 评论：发布价值（促使新物品获得评论，提升作者发布积极性）；留存价值（给喜欢讨论的用户创造更多留评机会）；鼓励高质量评论的用户多留评论。Thank You!

http://wangshusen.github.io/