Multi-Task Learning - Weight Assignment

This slide shows the weight assignment phase of the multi-task learning architecture:

**Structure:**
- Three intermediate representations: x₁, x₂, x₃ (purple vectors)
- Left output: [p₁, p₂, p₃] with label 权重 (Weights)
- Right output: [q₁, q₂, q₃] with label 权重 (Weights)

**Key Components:**
- The intermediate representations (x₁, x₂, x₃) are generated by the neural networks from the previous layer
- These representations are used to compute weights for both output tasks
- The weights (p₁, p₂, p₃) and (q₁, q₂, q₃) determine the importance of each representation for their respective tasks

**Architecture Significance:**
- This demonstrates how multi-task learning assigns different importance weights to shared representations
- Each task (left and right outputs) can focus on different aspects of the learned features
- The weight assignment allows for task-specific attention to different parts of the shared representation